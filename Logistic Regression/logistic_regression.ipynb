{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression_from_scratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/MachineLearning/blob/master/Logistic%20Regression/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d29hm6cwYWuY"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "**ABSTRACT**\n",
        "\n",
        "This is a hands-on practice of implementing the Logistic Regression Algorithm on Churn Modelling Dataset.\n",
        "\n",
        "1. Load the Churn Modelling Dataset and pre-process the dataset with LabelEncoder, OneHotEncoder and StandardScaler.\n",
        "2. Implement Logistic Regression Algorithm from scratch by defining the Sigmoid Function, Cross-Entropy Function and Gradient Descent using Python, run the gradient descent using Scipy fmin_cg to calculate the $\\theta$. Then make predictions and evaluate the results with Accuracy Score and Confusion Matrix.\n",
        "3. As a comparation, run the LogisticRegression Model from sklearn, compare the results with the above implementation from scratch. The conclusion is the same Accuracy Score is achieved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qqzWioayYmzg"
      },
      "source": [
        "## 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BeHy__8qYEkQ",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXQRk7A3YqLe"
      },
      "source": [
        "### 1.1 Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fKLcJXOVYinO",
        "colab": {}
      },
      "source": [
        "datafile = 'https://raw.githubusercontent.com/jchen8000/MachineLearning/master/Classification/data/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(datafile)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D2ImbpCCYtvt",
        "outputId": "37dcc0d7-565d-4a44-d080-b1582c63cc7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
              "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
              "1          2    15647311      Hill  ...               1       112542.58      0\n",
              "2          3    15619304      Onio  ...               0       113931.57      1\n",
              "3          4    15701354      Boni  ...               0        93826.63      0\n",
              "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XieTbrpKZkPH",
        "outputId": "e28885d4-845d-4f29-f693-b611a62490a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r1tDX_zFZo2H"
      },
      "source": [
        "### 1.2 Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YvA5EuVwZt4C",
        "outputId": "cb8b16b4-2dfc-402b-a823-ccfb5c1511e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
        "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder.fit_transform(X).toarray()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QHn9grSVaY9x"
      },
      "source": [
        "### 1.3 Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WCMDl-4SacTo",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H_Nga4sJszbb"
      },
      "source": [
        "### 1.4 Add Bias vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5JkJtm3rrgiK",
        "outputId": "4c84fbde-9779-4333-9941-25cf62e7316c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Before bias vector added:\", X.shape)\n",
        "m = X.shape[0]\n",
        "X = np.hstack((np.ones((m,1)), X))\n",
        "print(\"After add bias vector\", X.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before bias vector added: (10000, 12)\n",
            "After add bias vector (10000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NByg1h3laQW3"
      },
      "source": [
        "### 1.5 Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6OBd_kgaUKq",
        "outputId": "86c22c1e-d679-405a-d94c-ce36e0956472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "print(\"X_train: \", X_train.shape)\n",
        "print(\"X_test: \", X_test.shape)\n",
        "print(\"y_train: \", y_train.shape)\n",
        "print(\"y_test: \", y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:  (8000, 13)\n",
            "X_test:  (2000, 13)\n",
            "y_train:  (8000,)\n",
            "y_test:  (2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2T-CbucLakB2"
      },
      "source": [
        "## 2. Logistic Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eNvpHT6gbQZp"
      },
      "source": [
        "### 2.1 Sigmoid function, Cost Function and Gradient Descent\n",
        "\n",
        "\n",
        "\n",
        "**2.1.1 Sigmoid function**\n",
        "\n",
        "> # $h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{- \\theta^{T}x} }  $\n",
        "\n",
        "> The sigmoid function is having a characteristic \"S\"-shaped curve or ***sigmoid curve***, as  below\n",
        "\n",
        "> ![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e8NmS7Owegph",
        "outputId": "d40dae12-f286-490d-f663-fa9ab07e1c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "axis_x = np.arange(-10,10,.1)\n",
        "plt.plot(axis_x,sigmoid(axis_x))\n",
        "plt.title(\"Sigmoid Function.\")\n",
        "plt.grid(True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neKoyOESecLQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2iQFwek1SO3J"
      },
      "source": [
        "**2.1.2 Regularized Cross Entropy Cost Function**\n",
        "\n",
        "> The Regularized Cross Entropy Function is defined as following:\n",
        "\n",
        ">## $J(\\theta) = \\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}[-y^{(i)}log(h_{\\theta}(x^{(i)})) - (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GPtz7S5XSnXk"
      },
      "source": [
        "**2.1.3 Regularized Gradient Descent for Logistic Regression**\n",
        "\n",
        ">  for j = 0:\n",
        "\n",
        ">> ## $\\displaystyle\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
        "\n",
        "> for j >= 1:\n",
        "\n",
        ">> ## $\\displaystyle\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = \\Big(\\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}\\Big) + \\frac{\\lambda}{m} \\theta_{j} $\n",
        "\n",
        "> The following code implements the above math expression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XoVJk17Eb_93",
        "colab": {}
      },
      "source": [
        "def costFunction(theta_t, X_t, y_t, lambda_t):\n",
        "    m = len(y_t)\n",
        "    J = (-1/m) * (y_t.T @ np.log(sigmoid(X_t @ theta_t)) + (1 - y_t.T) @ np.log(1 - sigmoid(X_t @ theta_t)))\n",
        "    reg = (lambda_t/(2*m)) * (theta_t[1:].T @ theta_t[1:])\n",
        "    J = J + reg\n",
        "    return J\n",
        "  \n",
        "\n",
        "def GradientDescent(theta, X, y, lambda_t):\n",
        "    m = len(y)\n",
        "    grad = np.zeros([m,1])\n",
        "    grad = (1/m) * X.T @ (sigmoid(X @ theta) - y)\n",
        "    grad[1:] = grad[1:] + (lambda_t / m) * theta[1:]\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MqG4Ah2zcHig"
      },
      "source": [
        "### 2.2 Learning parameters using fmin_tnc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JG10VBaMbPPX",
        "outputId": "942ff3bd-5689-497b-f8cd-ad32ff8df962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(m, n) = X_train.shape\n",
        "#y_train = y_train[:, np.newaxis]\n",
        "theta = np.zeros((n,1))\n",
        "lmbda = 1\n",
        "J = costFunction(theta, X_train, y_train, lmbda)\n",
        "print(\"The initial cost: \", J)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The initial cost:  [[0.69314718]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odNCxS_ZcGCF",
        "outputId": "779c2411-4fc2-4a3d-c7d0-108fe63fb345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import scipy.optimize as opt  \n",
        "iteration = 50\n",
        "\n",
        "\n",
        "theta = opt.fmin_cg(f = costFunction, x0 = theta.flatten(),  fprime = GradientDescent, args = (X_train, y_train.flatten(), lmbda), maxiter = iteration)\n",
        "\n",
        "\n",
        "print(\"theta = \\n\", theta) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.429069\n",
            "         Iterations: 8\n",
            "         Function evaluations: 19\n",
            "         Gradient evaluations: 19\n",
            "$\theta$ =  [-1.63685723 -0.14136409  0.23636897 -0.07364662 -0.07208391 -0.26918615\n",
            "  0.75457167 -0.08137458  0.15306213 -0.085331   -0.02370001 -0.51819785\n",
            "  0.02916427]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LdK2lI9JfQi0",
        "outputId": "3ef5751a-bdac-4ddc-d91f-7a3232c34324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "J = costFunction(theta, X_train, y_train, lmbda)\n",
        "print(\"The final cost: \", J)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The final cost:  0.42906933690751703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FH97YJZbc0oQ"
      },
      "source": [
        "### 2.3 Evaluating logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "InT1gTAldikC",
        "outputId": "453e5981-4dde-4c1e-a5eb-a46bdb577f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = sigmoid(np.dot(X_test, theta))\n",
        "y_pred = (y_pred >= 0.5)\n",
        "y_pred.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rmPpTUPrdDSs",
        "outputId": "923abfb7-50ff-4c18-8e05-e0c146811a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1526   69]\n",
            " [ 309   96]]\n",
            "\n",
            "Accuracy Score: 0.811\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89      1595\n",
            "           1       0.58      0.24      0.34       405\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.71      0.60      0.61      2000\n",
            "weighted avg       0.78      0.81      0.78      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PROHEutKfFCk"
      },
      "source": [
        "## 3. Logistic Regression using Scikit Learn Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8_9Rw0fggIFN"
      },
      "source": [
        "### 3.1 Remove the bias vector which is used by previous method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ja1AudAKfeOM",
        "colab": {}
      },
      "source": [
        "X_train = X_train[:, 1:]\n",
        "X_test = X_test[:, 1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uV90IZcRgUBS"
      },
      "source": [
        "### 3.2 Train the Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbZny5H9faY7",
        "outputId": "f7bb8c67-b14c-4818-e908-cabfe6bd6fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(random_state=0, solver='lbfgs')\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gdRg_p1zgiOb"
      },
      "source": [
        "### 3.3 Evaluate the Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzPwVoVafsBP",
        "outputId": "3907f774-8903-40d6-9144-7a5133102213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1526   69]\n",
            " [ 309   96]]\n",
            "\n",
            "Accuracy Score: 0.811\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89      1595\n",
            "           1       0.58      0.24      0.34       405\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.71      0.60      0.61      2000\n",
            "weighted avg       0.78      0.81      0.78      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HGC09gtNhCr_"
      },
      "source": [
        "## 4. Conclusion\n",
        "\n",
        "**The Logistic Regression build from scratch has the same results with the Scikit Learn Logistic Regression Model.**"
      ]
    }
  ]
}