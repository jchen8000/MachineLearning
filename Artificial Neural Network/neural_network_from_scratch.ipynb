{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "2MwVkDk443U9"
      },
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cG6XbGfF6mqc"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P6Fzfikz43VE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nk0zkAdMG0vZ"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Importing the dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d13XAkG76vho",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for some reasons, the data file on github has some problems when reading\n",
        "#datafile = 'https://github.com/jchen8000/MachineLearning/blob/master/Classification/data/Churn_Modelling.csv'\n",
        "\n",
        "#Found the same data file from internet\n",
        "datafile = 'https://floobits.com/calvinlow18/ANN/raw/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(datafile)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v8JcxTYLF3qe",
        "outputId": "bb65fbb5-a46c-4665-864e-df362db4218c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
              "0          1    15634602  Hargrave          619    France  Female   42   \n",
              "1          2    15647311      Hill          608     Spain  Female   41   \n",
              "2          3    15619304      Onio          502    France  Female   42   \n",
              "3          4    15701354      Boni          699    France  Female   39   \n",
              "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
              "\n",
              "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0       2       0.00              1          1               1   \n",
              "1       1   83807.86              1          0               1   \n",
              "2       8  159660.80              3          1               0   \n",
              "3       1       0.00              2          0               0   \n",
              "4       2  125510.82              1          1               1   \n",
              "\n",
              "   EstimatedSalary  Exited  \n",
              "0        101348.88       1  \n",
              "1        112542.58       0  \n",
              "2        113931.57       1  \n",
              "3         93826.63       0  \n",
              "4         79084.10       0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "98O3WxVX7x3M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "#y shape looks like (m,), make it looks like (m,1)\n",
        "y = y[:,np.newaxis]                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Jc5Qt_WtG-tQ"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Encoding categorical data\n",
        "\n",
        "Encode the country name (string) to 0, 1, 2 etc. \n",
        "Encode female/male (string) to 0, 1\n",
        "\n",
        "Also need One Hot Encoding, see [Label Encoder vs. One Hot Encoder](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HNT14OUTGA0o",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-3vj5ylJJhnb",
        "outputId": "d5c4da58-1dd8-44c1-b4f1-87d3bafd95f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder.fit_transform(X).toarray()\n",
        "X = X[:, 1:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_V7uylCl43VT"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Splitting the dataset into the Training set and Test set\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZC-jcbxGDsg0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "O4IeRW4ANln_"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 Feature Scaling"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S-GiB9So43VX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vH7w4qLR43Vs",
        "outputId": "da2f7832-5886-4b4e-ad9a-35b7a58b8181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print( X_train.shape )\n",
        "print( X_test.shape )\n",
        "print( y_train.shape )\n",
        "print( y_test.shape )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 11)\n",
            "(2000, 11)\n",
            "(8000, 1)\n",
            "(2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kFQoOEm5O6Df"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Build a Neural Network from scratch\n",
        "\n",
        "![Neural Network Model](https://cdn-images-1.medium.com/max/800/1*l78dvvJFf0cOJnXTJglR7A.png)"
      ]
    },
    {
      "metadata": {
        "id": "ZyhY0YbXFH0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Neural Network Cost Function\n",
        "\n",
        "> ## $ \\min_\\Theta J(\\Theta)=-\\frac{\\mathrm{1} }{m} \\sum_{i=1}^{m}  \\sum_{k=1}^{K}\\left[ y_k^{(i)} log((h_\\Theta(x^{(i)}))_k) + (1 - y_k^{(i)}) log (1 - (h_\\Theta(x^{(i)}))_k) \\right]  + \\frac{\\mathrm{\\lambda}}{2m}  \\sum_{l=1}^{L-1} \\sum_{i=1}^{S_l}\\sum_{j=1}^{S_l+1}( \\Theta_{ji}^{(l)})^2$\n",
        "\n",
        "> Where $ h_\\Theta(x)  \\in  \\mathbb{R}^K, (h_\\Theta(x))_i = i^{th} output  $\n",
        "\n",
        "> $ L = $ total no. of layers in neural network\n",
        "\n",
        "> $ S_l = $ no. of units (not couning bias unit ) in layer $ l $\n",
        "\n",
        "> ### Think of $ J(\\Theta) \\approx ( h_\\Theta(x^{(i)}) - y^{(i)} ) ^2 $"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Bk-Na74oPVeu"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Sigmoid Gredient\n",
        "\n",
        "> ## $\\frac{\\mathrm{d} }{\\mathrm{d} z}g(z) = g(z)(1-g(z)) $\n",
        "\n",
        "> where\n",
        "\n",
        "> ## $ g(z) = sigmoid(z) = \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6ATuXdxt43Wd"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 Backpropagation\n",
        "\n",
        "> ## $  \\delta^{(3)}_j = a_j^{(3)} - y_j $,  ( total number of layers $ L = 3 $ )\n",
        "\n",
        "> ## $  \\delta^{(2)} = ( \\Theta^{(2)} )^T  \\delta^{(3)} .* g'(z^{(2)}) $\n",
        "\n",
        "> ## $  \\delta^{(1)} = ( \\Theta^{(1)} )^T  \\delta^{(2)} .* g'(z^{(1)}) $"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N1Re9DpR1Qk_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self, inputSize, hiddenSize, outputSize, lmbda):\n",
        "  #parameters\n",
        "    self.inputSize = inputSize\n",
        "    self.outputSize = outputSize\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.lmbda = lmbda\n",
        "    \n",
        "  #weights\n",
        "    epsilon = 0.2\n",
        "    self.theta1 = np.random.randn(self.inputSize, self.hiddenSize)  * 2 * epsilon - epsilon\n",
        "    self.theta2 = np.random.randn(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    #self.theta1 = np.random.rand(self.inputSize, self.hiddenSize) * 2 * epsilon - epsilon\n",
        "    #self.theta2 = np.random.rand(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    \n",
        "  #history\n",
        "    self.loss_history =  [] \n",
        "\n",
        "  def forward(self, X):\n",
        "    #forward propagation through our network\n",
        "    self.z = np.dot(X, self.theta1) # dot product of X (input) and first set of 3x2 weights\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = np.dot(self.z2, self.theta2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "    o = self.sigmoid(self.z3) # final activation function\n",
        "    return o\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    # activation function\n",
        "    return 1/(1+np.exp(-s))\n",
        "\n",
        "  def sigmoidPrime(self, s):\n",
        "    #derivative of sigmoid\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    # backward propagate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.theta2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.theta1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.theta2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "    \n",
        "  def cost(self, X, y ):\n",
        "    m = len(y)\n",
        "    y_output = self.forward(X)\n",
        "    \n",
        "    c1 = np.multiply(y, np.log(y_output))\n",
        "    c2 = np.multiply(1-y, np.log(1-y_output))\n",
        "    c = np.sum(c1 + c2)\n",
        "    \n",
        "    r1 = np.sum(np.sum(np.power(self.theta1,2), axis = 1))\n",
        "    r2 = np.sum(np.sum(np.power(self.theta2,2), axis = 1))\n",
        "    \n",
        "    return np.sum(c / (-m)) + (r1 + r2) * self.lmbda / (2*m)\n",
        "\n",
        "  \n",
        "  def loss(self, X, y):\n",
        "    return np.mean(np.square(y - self.forward(X)))\n",
        "\n",
        "  def train(self, X, y, epoch):\n",
        "    for i in range(epoch):\n",
        "      o = self.forward(X)\n",
        "      self.backward(X, y, o)\n",
        "      self.loss_history.append(self.loss(X,y))\n",
        "      print(\"epoch:[\", i, \"], loss: \", str(self.loss(X,y))  )\n",
        "\n",
        "  def predict(self, X):\n",
        "    return self.forward(X)\n",
        "  \n",
        "  \n",
        "  def get_loss_histroy(self):\n",
        "    return self.loss_history\n",
        "\n",
        "  def get_weight(self):\n",
        "    return self.theta1, self.theta2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1MjhrdvPyAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot the convergence of the cost function\n",
        "def plotConvergence(cost_history, iterations):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(len(cost_history)),cost_history,'bo')\n",
        "    plt.grid(True)\n",
        "    plt.title(\"Convergence of Cost Function\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
        "    dummy = plt.ylim([min(cost_history)-0.2*(max(cost_history)-min(cost_history)), max(cost_history)+0.2*(max(cost_history)-min(cost_history))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4suFNvfWj2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        },
        "outputId": "b2849aa6-bafb-4cca-8834-7f4a2aa62215"
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network( 11, 3, 1, 1 )\n",
        "#t1, t2 = NN.get_weight()\n",
        "iterations = 500\n",
        "NN.train(X_train, y_train, iterations)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:[ 0 ], loss:  0.2039996869311051\n",
            "epoch:[ 1 ], loss:  0.20399931304994132\n",
            "epoch:[ 2 ], loss:  0.20399762068048347\n",
            "epoch:[ 3 ], loss:  0.20398170472707608\n",
            "epoch:[ 4 ], loss:  0.20377329570428285\n",
            "epoch:[ 5 ], loss:  0.19592353206045066\n",
            "epoch:[ 6 ], loss:  0.16406014130396654\n",
            "epoch:[ 7 ], loss:  0.16194606613690143\n",
            "epoch:[ 8 ], loss:  0.15505095602097696\n",
            "epoch:[ 9 ], loss:  0.15112017513154793\n",
            "epoch:[ 10 ], loss:  0.14924464748822072\n",
            "epoch:[ 11 ], loss:  0.149438097114816\n",
            "epoch:[ 12 ], loss:  0.1491761213576754\n",
            "epoch:[ 13 ], loss:  0.14895717925482124\n",
            "epoch:[ 14 ], loss:  0.14880343212819655\n",
            "epoch:[ 15 ], loss:  0.1477356662621358\n",
            "epoch:[ 16 ], loss:  0.14757541322188178\n",
            "epoch:[ 17 ], loss:  0.14911015713770967\n",
            "epoch:[ 18 ], loss:  0.14753317645737202\n",
            "epoch:[ 19 ], loss:  0.14797205252477313\n",
            "epoch:[ 20 ], loss:  0.1485399112469073\n",
            "epoch:[ 21 ], loss:  0.14748636307977836\n",
            "epoch:[ 22 ], loss:  0.1478678312335476\n",
            "epoch:[ 23 ], loss:  0.14739499165677872\n",
            "epoch:[ 24 ], loss:  0.14770936852610042\n",
            "epoch:[ 25 ], loss:  0.14705377347963994\n",
            "epoch:[ 26 ], loss:  0.14737183120407207\n",
            "epoch:[ 27 ], loss:  0.14826344721638163\n",
            "epoch:[ 28 ], loss:  0.1479392486706862\n",
            "epoch:[ 29 ], loss:  0.1470954006308252\n",
            "epoch:[ 30 ], loss:  0.14711254029913384\n",
            "epoch:[ 31 ], loss:  0.14765708802434693\n",
            "epoch:[ 32 ], loss:  0.1476851471080167\n",
            "epoch:[ 33 ], loss:  0.1472438319105506\n",
            "epoch:[ 34 ], loss:  0.14745827451888635\n",
            "epoch:[ 35 ], loss:  0.14693317627194927\n",
            "epoch:[ 36 ], loss:  0.14691998545050697\n",
            "epoch:[ 37 ], loss:  0.14770255528874043\n",
            "epoch:[ 38 ], loss:  0.14714203417279406\n",
            "epoch:[ 39 ], loss:  0.14601041087809674\n",
            "epoch:[ 40 ], loss:  0.147700677592097\n",
            "epoch:[ 41 ], loss:  0.14665417079840812\n",
            "epoch:[ 42 ], loss:  0.14769816458733032\n",
            "epoch:[ 43 ], loss:  0.14683823628080528\n",
            "epoch:[ 44 ], loss:  0.1465224021131589\n",
            "epoch:[ 45 ], loss:  0.14648010156842037\n",
            "epoch:[ 46 ], loss:  0.1474412992561168\n",
            "epoch:[ 47 ], loss:  0.14799208112036066\n",
            "epoch:[ 48 ], loss:  0.14728783289778902\n",
            "epoch:[ 49 ], loss:  0.14782327279842708\n",
            "epoch:[ 50 ], loss:  0.14653313526561307\n",
            "epoch:[ 51 ], loss:  0.14738017048368415\n",
            "epoch:[ 52 ], loss:  0.14768638431485914\n",
            "epoch:[ 53 ], loss:  0.14755348819643296\n",
            "epoch:[ 54 ], loss:  0.14656233241421374\n",
            "epoch:[ 55 ], loss:  0.1464804883933052\n",
            "epoch:[ 56 ], loss:  0.14780659765144089\n",
            "epoch:[ 57 ], loss:  0.1474254441570766\n",
            "epoch:[ 58 ], loss:  0.1466061277525834\n",
            "epoch:[ 59 ], loss:  0.14632348431823344\n",
            "epoch:[ 60 ], loss:  0.14679260596405155\n",
            "epoch:[ 61 ], loss:  0.14667148049238304\n",
            "epoch:[ 62 ], loss:  0.14649513672251468\n",
            "epoch:[ 63 ], loss:  0.14637904888734785\n",
            "epoch:[ 64 ], loss:  0.14732453641653365\n",
            "epoch:[ 65 ], loss:  0.14674462675333663\n",
            "epoch:[ 66 ], loss:  0.14646251694728754\n",
            "epoch:[ 67 ], loss:  0.14647160584581104\n",
            "epoch:[ 68 ], loss:  0.14622705555796378\n",
            "epoch:[ 69 ], loss:  0.1465664351453143\n",
            "epoch:[ 70 ], loss:  0.1461796449943759\n",
            "epoch:[ 71 ], loss:  0.14583626942800493\n",
            "epoch:[ 72 ], loss:  0.14678418519315534\n",
            "epoch:[ 73 ], loss:  0.14637594305425009\n",
            "epoch:[ 74 ], loss:  0.14606848285570176\n",
            "epoch:[ 75 ], loss:  0.14578490139397887\n",
            "epoch:[ 76 ], loss:  0.14608588101250694\n",
            "epoch:[ 77 ], loss:  0.1459514230247569\n",
            "epoch:[ 78 ], loss:  0.1456004794890119\n",
            "epoch:[ 79 ], loss:  0.14574776676659856\n",
            "epoch:[ 80 ], loss:  0.14629213861253496\n",
            "epoch:[ 81 ], loss:  0.14626553394916716\n",
            "epoch:[ 82 ], loss:  0.14580457293197446\n",
            "epoch:[ 83 ], loss:  0.14611042679110284\n",
            "epoch:[ 84 ], loss:  0.14537026747249013\n",
            "epoch:[ 85 ], loss:  0.1456989674474404\n",
            "epoch:[ 86 ], loss:  0.14562398044351238\n",
            "epoch:[ 87 ], loss:  0.14606580180899378\n",
            "epoch:[ 88 ], loss:  0.14604317206422252\n",
            "epoch:[ 89 ], loss:  0.14607607223321747\n",
            "epoch:[ 90 ], loss:  0.1463848914715916\n",
            "epoch:[ 91 ], loss:  0.1460750340836783\n",
            "epoch:[ 92 ], loss:  0.14561640727644554\n",
            "epoch:[ 93 ], loss:  0.145573145660642\n",
            "epoch:[ 94 ], loss:  0.14591477814907694\n",
            "epoch:[ 95 ], loss:  0.14569998292757014\n",
            "epoch:[ 96 ], loss:  0.14518518700940813\n",
            "epoch:[ 97 ], loss:  0.14594192894187524\n",
            "epoch:[ 98 ], loss:  0.1455602479040335\n",
            "epoch:[ 99 ], loss:  0.14562276910383468\n",
            "epoch:[ 100 ], loss:  0.14550571429387654\n",
            "epoch:[ 101 ], loss:  0.14500122067247906\n",
            "epoch:[ 102 ], loss:  0.145413146130736\n",
            "epoch:[ 103 ], loss:  0.14567730332118925\n",
            "epoch:[ 104 ], loss:  0.1455211294830693\n",
            "epoch:[ 105 ], loss:  0.14532277840079522\n",
            "epoch:[ 106 ], loss:  0.14511577843686144\n",
            "epoch:[ 107 ], loss:  0.14545059991134343\n",
            "epoch:[ 108 ], loss:  0.14607210379864036\n",
            "epoch:[ 109 ], loss:  0.14586982099268433\n",
            "epoch:[ 110 ], loss:  0.14559945480535827\n",
            "epoch:[ 111 ], loss:  0.14550456929675107\n",
            "epoch:[ 112 ], loss:  0.14589168023699375\n",
            "epoch:[ 113 ], loss:  0.14574093357093446\n",
            "epoch:[ 114 ], loss:  0.14545769397165884\n",
            "epoch:[ 115 ], loss:  0.1457079308601955\n",
            "epoch:[ 116 ], loss:  0.14528482628621764\n",
            "epoch:[ 117 ], loss:  0.14486615952836637\n",
            "epoch:[ 118 ], loss:  0.14520114273149687\n",
            "epoch:[ 119 ], loss:  0.1457278817605272\n",
            "epoch:[ 120 ], loss:  0.14585678412555145\n",
            "epoch:[ 121 ], loss:  0.14650227761711582\n",
            "epoch:[ 122 ], loss:  0.1465873607213653\n",
            "epoch:[ 123 ], loss:  0.14532961700114588\n",
            "epoch:[ 124 ], loss:  0.14542528559570575\n",
            "epoch:[ 125 ], loss:  0.146695460369307\n",
            "epoch:[ 126 ], loss:  0.14557648906132553\n",
            "epoch:[ 127 ], loss:  0.14549326941475185\n",
            "epoch:[ 128 ], loss:  0.14553115951594567\n",
            "epoch:[ 129 ], loss:  0.1458200999347412\n",
            "epoch:[ 130 ], loss:  0.14568650607681602\n",
            "epoch:[ 131 ], loss:  0.1452317114089922\n",
            "epoch:[ 132 ], loss:  0.14523194054916666\n",
            "epoch:[ 133 ], loss:  0.14520026083208148\n",
            "epoch:[ 134 ], loss:  0.14479365245627743\n",
            "epoch:[ 135 ], loss:  0.145516039157666\n",
            "epoch:[ 136 ], loss:  0.14557444174679401\n",
            "epoch:[ 137 ], loss:  0.14507328012293697\n",
            "epoch:[ 138 ], loss:  0.14504415948009647\n",
            "epoch:[ 139 ], loss:  0.14485645134581682\n",
            "epoch:[ 140 ], loss:  0.14513889176608072\n",
            "epoch:[ 141 ], loss:  0.14579665411086187\n",
            "epoch:[ 142 ], loss:  0.1451982104745745\n",
            "epoch:[ 143 ], loss:  0.14567837237059933\n",
            "epoch:[ 144 ], loss:  0.14537083391053998\n",
            "epoch:[ 145 ], loss:  0.14610557402177074\n",
            "epoch:[ 146 ], loss:  0.14577966535896578\n",
            "epoch:[ 147 ], loss:  0.14563374027533588\n",
            "epoch:[ 148 ], loss:  0.14526920787108447\n",
            "epoch:[ 149 ], loss:  0.14539255178960314\n",
            "epoch:[ 150 ], loss:  0.14518785905517476\n",
            "epoch:[ 151 ], loss:  0.1449254239654918\n",
            "epoch:[ 152 ], loss:  0.1451118895948721\n",
            "epoch:[ 153 ], loss:  0.14516576089691\n",
            "epoch:[ 154 ], loss:  0.14562097021966686\n",
            "epoch:[ 155 ], loss:  0.14551362209010887\n",
            "epoch:[ 156 ], loss:  0.14553255347331367\n",
            "epoch:[ 157 ], loss:  0.14496355778674425\n",
            "epoch:[ 158 ], loss:  0.14588769645294078\n",
            "epoch:[ 159 ], loss:  0.14519332459042517\n",
            "epoch:[ 160 ], loss:  0.14557468589010708\n",
            "epoch:[ 161 ], loss:  0.14550423959326594\n",
            "epoch:[ 162 ], loss:  0.14483593915278778\n",
            "epoch:[ 163 ], loss:  0.14507048613635065\n",
            "epoch:[ 164 ], loss:  0.14519606114247224\n",
            "epoch:[ 165 ], loss:  0.1452173209917746\n",
            "epoch:[ 166 ], loss:  0.1448933694995564\n",
            "epoch:[ 167 ], loss:  0.14466367514835235\n",
            "epoch:[ 168 ], loss:  0.14497309917899392\n",
            "epoch:[ 169 ], loss:  0.14585222612391358\n",
            "epoch:[ 170 ], loss:  0.1449714999934542\n",
            "epoch:[ 171 ], loss:  0.14536621148633452\n",
            "epoch:[ 172 ], loss:  0.1457676102483881\n",
            "epoch:[ 173 ], loss:  0.14497641537470604\n",
            "epoch:[ 174 ], loss:  0.14506266464809364\n",
            "epoch:[ 175 ], loss:  0.14555686141948992\n",
            "epoch:[ 176 ], loss:  0.14551399692445127\n",
            "epoch:[ 177 ], loss:  0.14487498930659246\n",
            "epoch:[ 178 ], loss:  0.14506939620256928\n",
            "epoch:[ 179 ], loss:  0.14538824038211628\n",
            "epoch:[ 180 ], loss:  0.14513114278126266\n",
            "epoch:[ 181 ], loss:  0.14508022647497149\n",
            "epoch:[ 182 ], loss:  0.14510974079188968\n",
            "epoch:[ 183 ], loss:  0.14533042386540437\n",
            "epoch:[ 184 ], loss:  0.145079188873439\n",
            "epoch:[ 185 ], loss:  0.14563057999175746\n",
            "epoch:[ 186 ], loss:  0.14480772577624698\n",
            "epoch:[ 187 ], loss:  0.1452356952816944\n",
            "epoch:[ 188 ], loss:  0.14479739156916185\n",
            "epoch:[ 189 ], loss:  0.14503296077669153\n",
            "epoch:[ 190 ], loss:  0.14497866909483714\n",
            "epoch:[ 191 ], loss:  0.14490712601860223\n",
            "epoch:[ 192 ], loss:  0.14492940213295258\n",
            "epoch:[ 193 ], loss:  0.14466368289148593\n",
            "epoch:[ 194 ], loss:  0.14489375368088578\n",
            "epoch:[ 195 ], loss:  0.14528559015853224\n",
            "epoch:[ 196 ], loss:  0.14499841071234265\n",
            "epoch:[ 197 ], loss:  0.1454368993226578\n",
            "epoch:[ 198 ], loss:  0.14508590305854363\n",
            "epoch:[ 199 ], loss:  0.14490599292891015\n",
            "epoch:[ 200 ], loss:  0.1445652310819213\n",
            "epoch:[ 201 ], loss:  0.14558306874415308\n",
            "epoch:[ 202 ], loss:  0.1457569799915628\n",
            "epoch:[ 203 ], loss:  0.14491871158295794\n",
            "epoch:[ 204 ], loss:  0.14488908550328597\n",
            "epoch:[ 205 ], loss:  0.1447430109844534\n",
            "epoch:[ 206 ], loss:  0.14535171294794957\n",
            "epoch:[ 207 ], loss:  0.14501395370703282\n",
            "epoch:[ 208 ], loss:  0.1452050056520241\n",
            "epoch:[ 209 ], loss:  0.14532572962737403\n",
            "epoch:[ 210 ], loss:  0.14528392961941813\n",
            "epoch:[ 211 ], loss:  0.14608790366744764\n",
            "epoch:[ 212 ], loss:  0.1453364384986369\n",
            "epoch:[ 213 ], loss:  0.14501827072663445\n",
            "epoch:[ 214 ], loss:  0.1447929858103243\n",
            "epoch:[ 215 ], loss:  0.14509693174602942\n",
            "epoch:[ 216 ], loss:  0.14601777893350762\n",
            "epoch:[ 217 ], loss:  0.1447640208971619\n",
            "epoch:[ 218 ], loss:  0.14479090724072166\n",
            "epoch:[ 219 ], loss:  0.14488380915939422\n",
            "epoch:[ 220 ], loss:  0.1451412524394121\n",
            "epoch:[ 221 ], loss:  0.1448857418041962\n",
            "epoch:[ 222 ], loss:  0.14530508211223947\n",
            "epoch:[ 223 ], loss:  0.14489366256872022\n",
            "epoch:[ 224 ], loss:  0.14467080853463005\n",
            "epoch:[ 225 ], loss:  0.14544449671222834\n",
            "epoch:[ 226 ], loss:  0.14482099454903202\n",
            "epoch:[ 227 ], loss:  0.1454267070907284\n",
            "epoch:[ 228 ], loss:  0.14554527854546917\n",
            "epoch:[ 229 ], loss:  0.14539165295956089\n",
            "epoch:[ 230 ], loss:  0.14522454939132357\n",
            "epoch:[ 231 ], loss:  0.14500929664037637\n",
            "epoch:[ 232 ], loss:  0.14483502352158148\n",
            "epoch:[ 233 ], loss:  0.1450085145248755\n",
            "epoch:[ 234 ], loss:  0.14529120803207057\n",
            "epoch:[ 235 ], loss:  0.14571580655256577\n",
            "epoch:[ 236 ], loss:  0.14508710830789362\n",
            "epoch:[ 237 ], loss:  0.1446389896125185\n",
            "epoch:[ 238 ], loss:  0.14498990063265263\n",
            "epoch:[ 239 ], loss:  0.1452233636915698\n",
            "epoch:[ 240 ], loss:  0.14505717845300825\n",
            "epoch:[ 241 ], loss:  0.14527958002288954\n",
            "epoch:[ 242 ], loss:  0.14579570686008758\n",
            "epoch:[ 243 ], loss:  0.1454155682852879\n",
            "epoch:[ 244 ], loss:  0.1451521159710623\n",
            "epoch:[ 245 ], loss:  0.14512149732052343\n",
            "epoch:[ 246 ], loss:  0.14516344442535115\n",
            "epoch:[ 247 ], loss:  0.14509954818763923\n",
            "epoch:[ 248 ], loss:  0.14496243571120945\n",
            "epoch:[ 249 ], loss:  0.1447596163158152\n",
            "epoch:[ 250 ], loss:  0.14523498381246608\n",
            "epoch:[ 251 ], loss:  0.1459721780471735\n",
            "epoch:[ 252 ], loss:  0.14516530060083127\n",
            "epoch:[ 253 ], loss:  0.1452841877283241\n",
            "epoch:[ 254 ], loss:  0.14502322834402623\n",
            "epoch:[ 255 ], loss:  0.14470827765392824\n",
            "epoch:[ 256 ], loss:  0.14476954441702836\n",
            "epoch:[ 257 ], loss:  0.1449831023719297\n",
            "epoch:[ 258 ], loss:  0.1451973728495898\n",
            "epoch:[ 259 ], loss:  0.14581995512619517\n",
            "epoch:[ 260 ], loss:  0.14496738792748878\n",
            "epoch:[ 261 ], loss:  0.14474057154322614\n",
            "epoch:[ 262 ], loss:  0.14474080527640115\n",
            "epoch:[ 263 ], loss:  0.14485341258796547\n",
            "epoch:[ 264 ], loss:  0.14487505209317203\n",
            "epoch:[ 265 ], loss:  0.1449518588505826\n",
            "epoch:[ 266 ], loss:  0.14492623223112064\n",
            "epoch:[ 267 ], loss:  0.1450795106543614\n",
            "epoch:[ 268 ], loss:  0.14481813217721556\n",
            "epoch:[ 269 ], loss:  0.14495518920135408\n",
            "epoch:[ 270 ], loss:  0.14494437723934236\n",
            "epoch:[ 271 ], loss:  0.14499456417958448\n",
            "epoch:[ 272 ], loss:  0.14511011488601047\n",
            "epoch:[ 273 ], loss:  0.1454397314309641\n",
            "epoch:[ 274 ], loss:  0.14488709010297954\n",
            "epoch:[ 275 ], loss:  0.1448193870203559\n",
            "epoch:[ 276 ], loss:  0.1448088676603123\n",
            "epoch:[ 277 ], loss:  0.1452981897933471\n",
            "epoch:[ 278 ], loss:  0.1447201701339053\n",
            "epoch:[ 279 ], loss:  0.14487445301548516\n",
            "epoch:[ 280 ], loss:  0.14504048415677806\n",
            "epoch:[ 281 ], loss:  0.1454501277275467\n",
            "epoch:[ 282 ], loss:  0.1446442433930452\n",
            "epoch:[ 283 ], loss:  0.14515284461986466\n",
            "epoch:[ 284 ], loss:  0.14502677022401805\n",
            "epoch:[ 285 ], loss:  0.14476296889455154\n",
            "epoch:[ 286 ], loss:  0.14457150847556\n",
            "epoch:[ 287 ], loss:  0.14471165083995285\n",
            "epoch:[ 288 ], loss:  0.14457952874182592\n",
            "epoch:[ 289 ], loss:  0.14487125579611485\n",
            "epoch:[ 290 ], loss:  0.14506851800649254\n",
            "epoch:[ 291 ], loss:  0.14498635955604056\n",
            "epoch:[ 292 ], loss:  0.14505977526723793\n",
            "epoch:[ 293 ], loss:  0.14495209403091508\n",
            "epoch:[ 294 ], loss:  0.14505169825812375\n",
            "epoch:[ 295 ], loss:  0.14533715027686175\n",
            "epoch:[ 296 ], loss:  0.14490695446901566\n",
            "epoch:[ 297 ], loss:  0.14489969007768205\n",
            "epoch:[ 298 ], loss:  0.14513701710585872\n",
            "epoch:[ 299 ], loss:  0.1450372683282705\n",
            "epoch:[ 300 ], loss:  0.1446447555648294\n",
            "epoch:[ 301 ], loss:  0.14479826625547038\n",
            "epoch:[ 302 ], loss:  0.14528943867920338\n",
            "epoch:[ 303 ], loss:  0.14496405230804452\n",
            "epoch:[ 304 ], loss:  0.14454050806428598\n",
            "epoch:[ 305 ], loss:  0.14482413631789778\n",
            "epoch:[ 306 ], loss:  0.14459180793386942\n",
            "epoch:[ 307 ], loss:  0.14447570697746046\n",
            "epoch:[ 308 ], loss:  0.14545468957245394\n",
            "epoch:[ 309 ], loss:  0.14522062501983946\n",
            "epoch:[ 310 ], loss:  0.14444656156080687\n",
            "epoch:[ 311 ], loss:  0.14471378797080545\n",
            "epoch:[ 312 ], loss:  0.14521660466444325\n",
            "epoch:[ 313 ], loss:  0.14531930937245557\n",
            "epoch:[ 314 ], loss:  0.14494822162317597\n",
            "epoch:[ 315 ], loss:  0.14478534965113785\n",
            "epoch:[ 316 ], loss:  0.14507126963946979\n",
            "epoch:[ 317 ], loss:  0.14581089955182397\n",
            "epoch:[ 318 ], loss:  0.1450894375079214\n",
            "epoch:[ 319 ], loss:  0.14464552242675283\n",
            "epoch:[ 320 ], loss:  0.14502112417704266\n",
            "epoch:[ 321 ], loss:  0.14514646958672917\n",
            "epoch:[ 322 ], loss:  0.14530918027190223\n",
            "epoch:[ 323 ], loss:  0.14471296426299438\n",
            "epoch:[ 324 ], loss:  0.14527846052722262\n",
            "epoch:[ 325 ], loss:  0.14517907074019548\n",
            "epoch:[ 326 ], loss:  0.14516770819082428\n",
            "epoch:[ 327 ], loss:  0.14490197187290563\n",
            "epoch:[ 328 ], loss:  0.14444002104179818\n",
            "epoch:[ 329 ], loss:  0.14483677770091316\n",
            "epoch:[ 330 ], loss:  0.1451462548959691\n",
            "epoch:[ 331 ], loss:  0.14542257692637478\n",
            "epoch:[ 332 ], loss:  0.14537123174740407\n",
            "epoch:[ 333 ], loss:  0.14510864148077365\n",
            "epoch:[ 334 ], loss:  0.14457534115158818\n",
            "epoch:[ 335 ], loss:  0.14483368737709706\n",
            "epoch:[ 336 ], loss:  0.14520844507724667\n",
            "epoch:[ 337 ], loss:  0.14501924127794746\n",
            "epoch:[ 338 ], loss:  0.1449567009711269\n",
            "epoch:[ 339 ], loss:  0.14480822378020042\n",
            "epoch:[ 340 ], loss:  0.1449329324871744\n",
            "epoch:[ 341 ], loss:  0.14477796294629144\n",
            "epoch:[ 342 ], loss:  0.14503156897115826\n",
            "epoch:[ 343 ], loss:  0.1450983229784607\n",
            "epoch:[ 344 ], loss:  0.14485377779001882\n",
            "epoch:[ 345 ], loss:  0.14502468846487482\n",
            "epoch:[ 346 ], loss:  0.14527840618148108\n",
            "epoch:[ 347 ], loss:  0.14477421715964692\n",
            "epoch:[ 348 ], loss:  0.14462193643064516\n",
            "epoch:[ 349 ], loss:  0.1453057179143266\n",
            "epoch:[ 350 ], loss:  0.14492703522221398\n",
            "epoch:[ 351 ], loss:  0.1446649107516997\n",
            "epoch:[ 352 ], loss:  0.1448723943403571\n",
            "epoch:[ 353 ], loss:  0.14477152785581257\n",
            "epoch:[ 354 ], loss:  0.14492638456906493\n",
            "epoch:[ 355 ], loss:  0.14467219427441846\n",
            "epoch:[ 356 ], loss:  0.1445875485560183\n",
            "epoch:[ 357 ], loss:  0.14511651527624655\n",
            "epoch:[ 358 ], loss:  0.14477432171596058\n",
            "epoch:[ 359 ], loss:  0.14496302859072\n",
            "epoch:[ 360 ], loss:  0.1451128066066264\n",
            "epoch:[ 361 ], loss:  0.1449815331204943\n",
            "epoch:[ 362 ], loss:  0.14481738997412966\n",
            "epoch:[ 363 ], loss:  0.14495665317598094\n",
            "epoch:[ 364 ], loss:  0.14482559916839916\n",
            "epoch:[ 365 ], loss:  0.1450829671538437\n",
            "epoch:[ 366 ], loss:  0.14496220702583038\n",
            "epoch:[ 367 ], loss:  0.14499262706770225\n",
            "epoch:[ 368 ], loss:  0.14477164328680678\n",
            "epoch:[ 369 ], loss:  0.14481829429937562\n",
            "epoch:[ 370 ], loss:  0.14492188800569777\n",
            "epoch:[ 371 ], loss:  0.1449566309500982\n",
            "epoch:[ 372 ], loss:  0.1448410230781528\n",
            "epoch:[ 373 ], loss:  0.1449184493679777\n",
            "epoch:[ 374 ], loss:  0.1446066873805278\n",
            "epoch:[ 375 ], loss:  0.14484524086814082\n",
            "epoch:[ 376 ], loss:  0.1454633740153695\n",
            "epoch:[ 377 ], loss:  0.14464995689841023\n",
            "epoch:[ 378 ], loss:  0.14451248281634946\n",
            "epoch:[ 379 ], loss:  0.14496666719314552\n",
            "epoch:[ 380 ], loss:  0.14480227442022378\n",
            "epoch:[ 381 ], loss:  0.1448662946540458\n",
            "epoch:[ 382 ], loss:  0.1451167235498055\n",
            "epoch:[ 383 ], loss:  0.14503660884274433\n",
            "epoch:[ 384 ], loss:  0.14467699389950348\n",
            "epoch:[ 385 ], loss:  0.14474252447299948\n",
            "epoch:[ 386 ], loss:  0.14497486491981557\n",
            "epoch:[ 387 ], loss:  0.1448024767740236\n",
            "epoch:[ 388 ], loss:  0.14487005490000757\n",
            "epoch:[ 389 ], loss:  0.14477531751859105\n",
            "epoch:[ 390 ], loss:  0.14491219604716585\n",
            "epoch:[ 391 ], loss:  0.14468039530138616\n",
            "epoch:[ 392 ], loss:  0.14491897903743714\n",
            "epoch:[ 393 ], loss:  0.145519709842241\n",
            "epoch:[ 394 ], loss:  0.14529567476288544\n",
            "epoch:[ 395 ], loss:  0.1448403192262449\n",
            "epoch:[ 396 ], loss:  0.14485489169313978\n",
            "epoch:[ 397 ], loss:  0.14460565539719775\n",
            "epoch:[ 398 ], loss:  0.1451617049909753\n",
            "epoch:[ 399 ], loss:  0.144849233696783\n",
            "epoch:[ 400 ], loss:  0.1447675754220689\n",
            "epoch:[ 401 ], loss:  0.14513215285472697\n",
            "epoch:[ 402 ], loss:  0.14524008060006632\n",
            "epoch:[ 403 ], loss:  0.14494659924695685\n",
            "epoch:[ 404 ], loss:  0.1447285745890977\n",
            "epoch:[ 405 ], loss:  0.1449330720356873\n",
            "epoch:[ 406 ], loss:  0.14509621776427092\n",
            "epoch:[ 407 ], loss:  0.1454196511567452\n",
            "epoch:[ 408 ], loss:  0.14470627590538032\n",
            "epoch:[ 409 ], loss:  0.14491521853536254\n",
            "epoch:[ 410 ], loss:  0.14528201080424982\n",
            "epoch:[ 411 ], loss:  0.14535847559569812\n",
            "epoch:[ 412 ], loss:  0.14489985154512408\n",
            "epoch:[ 413 ], loss:  0.14490639611820477\n",
            "epoch:[ 414 ], loss:  0.14534827790585922\n",
            "epoch:[ 415 ], loss:  0.14509476030449428\n",
            "epoch:[ 416 ], loss:  0.14481697570531818\n",
            "epoch:[ 417 ], loss:  0.144916921222204\n",
            "epoch:[ 418 ], loss:  0.14518791161978994\n",
            "epoch:[ 419 ], loss:  0.14507829779715317\n",
            "epoch:[ 420 ], loss:  0.1448155692678792\n",
            "epoch:[ 421 ], loss:  0.14494324865902236\n",
            "epoch:[ 422 ], loss:  0.14491558754441838\n",
            "epoch:[ 423 ], loss:  0.14487153181387435\n",
            "epoch:[ 424 ], loss:  0.14503208562508843\n",
            "epoch:[ 425 ], loss:  0.14515326388597014\n",
            "epoch:[ 426 ], loss:  0.14481146545370285\n",
            "epoch:[ 427 ], loss:  0.1448373796119258\n",
            "epoch:[ 428 ], loss:  0.14570267685049362\n",
            "epoch:[ 429 ], loss:  0.14582909146484158\n",
            "epoch:[ 430 ], loss:  0.14495388080933896\n",
            "epoch:[ 431 ], loss:  0.1445494215619953\n",
            "epoch:[ 432 ], loss:  0.14492934101144636\n",
            "epoch:[ 433 ], loss:  0.14495696044648138\n",
            "epoch:[ 434 ], loss:  0.14531818209856737\n",
            "epoch:[ 435 ], loss:  0.1448156346556214\n",
            "epoch:[ 436 ], loss:  0.144939198392264\n",
            "epoch:[ 437 ], loss:  0.14488379821096495\n",
            "epoch:[ 438 ], loss:  0.14481522247908193\n",
            "epoch:[ 439 ], loss:  0.14468378551344777\n",
            "epoch:[ 440 ], loss:  0.1446401151481794\n",
            "epoch:[ 441 ], loss:  0.1446967596599013\n",
            "epoch:[ 442 ], loss:  0.14443890082731667\n",
            "epoch:[ 443 ], loss:  0.1446519408621161\n",
            "epoch:[ 444 ], loss:  0.14481408091361195\n",
            "epoch:[ 445 ], loss:  0.14510604080498546\n",
            "epoch:[ 446 ], loss:  0.14495930426460324\n",
            "epoch:[ 447 ], loss:  0.14454232957457225\n",
            "epoch:[ 448 ], loss:  0.14462378395723693\n",
            "epoch:[ 449 ], loss:  0.14463245587577034\n",
            "epoch:[ 450 ], loss:  0.14538020101446927\n",
            "epoch:[ 451 ], loss:  0.1455077341959322\n",
            "epoch:[ 452 ], loss:  0.1454298915221296\n",
            "epoch:[ 453 ], loss:  0.14472444640964555\n",
            "epoch:[ 454 ], loss:  0.14487162736016015\n",
            "epoch:[ 455 ], loss:  0.14476925535729357\n",
            "epoch:[ 456 ], loss:  0.14480096839314435\n",
            "epoch:[ 457 ], loss:  0.14497346057834218\n",
            "epoch:[ 458 ], loss:  0.14491575861789716\n",
            "epoch:[ 459 ], loss:  0.1448540220416877\n",
            "epoch:[ 460 ], loss:  0.14465662809995575\n",
            "epoch:[ 461 ], loss:  0.14492191834735949\n",
            "epoch:[ 462 ], loss:  0.14496936604609698\n",
            "epoch:[ 463 ], loss:  0.14451443330126545\n",
            "epoch:[ 464 ], loss:  0.14463295563517414\n",
            "epoch:[ 465 ], loss:  0.14452277771455058\n",
            "epoch:[ 466 ], loss:  0.1448097927035494\n",
            "epoch:[ 467 ], loss:  0.14480059486975835\n",
            "epoch:[ 468 ], loss:  0.1449148585305716\n",
            "epoch:[ 469 ], loss:  0.14532830177463046\n",
            "epoch:[ 470 ], loss:  0.1450753511473772\n",
            "epoch:[ 471 ], loss:  0.14466907031216542\n",
            "epoch:[ 472 ], loss:  0.14466565345089819\n",
            "epoch:[ 473 ], loss:  0.14507692419822693\n",
            "epoch:[ 474 ], loss:  0.14505639283304866\n",
            "epoch:[ 475 ], loss:  0.14487235199605514\n",
            "epoch:[ 476 ], loss:  0.14514520552495272\n",
            "epoch:[ 477 ], loss:  0.1449374134122154\n",
            "epoch:[ 478 ], loss:  0.14472624470485743\n",
            "epoch:[ 479 ], loss:  0.14469252285752005\n",
            "epoch:[ 480 ], loss:  0.14499636726840232\n",
            "epoch:[ 481 ], loss:  0.14505289306262995\n",
            "epoch:[ 482 ], loss:  0.14441858953941272\n",
            "epoch:[ 483 ], loss:  0.14455604692292584\n",
            "epoch:[ 484 ], loss:  0.14515062706515783\n",
            "epoch:[ 485 ], loss:  0.14513828943141305\n",
            "epoch:[ 486 ], loss:  0.14548774331975287\n",
            "epoch:[ 487 ], loss:  0.14478695165866398\n",
            "epoch:[ 488 ], loss:  0.14496297941573943\n",
            "epoch:[ 489 ], loss:  0.14504147962239886\n",
            "epoch:[ 490 ], loss:  0.14515677671001234\n",
            "epoch:[ 491 ], loss:  0.1448497075115078\n",
            "epoch:[ 492 ], loss:  0.1448057650026276\n",
            "epoch:[ 493 ], loss:  0.1447345604399821\n",
            "epoch:[ 494 ], loss:  0.14465894019195316\n",
            "epoch:[ 495 ], loss:  0.14458338817583233\n",
            "epoch:[ 496 ], loss:  0.14499299212569355\n",
            "epoch:[ 497 ], loss:  0.14487297416653508\n",
            "epoch:[ 498 ], loss:  0.14452502915665655\n",
            "epoch:[ 499 ], loss:  0.14462318102093147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-8hMDyzQZqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "34e54d75-ee57-43fc-bf52-dd5efd62aebc"
      },
      "cell_type": "code",
      "source": [
        "plotConvergence(NN.get_loss_histroy(),iterations )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAGCCAYAAAChJrSAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIABCGUAAAAGIJQBAAAY4P8B7ToOgxyryusAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f6ROChOed9pC",
        "colab_type": "code",
        "outputId": "845f5f2f-ad31-406a-d745-d4e33413c7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = NN.predict(X_test)\n",
        "y_pred = (y_pred >= 0.5)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1505   90]\n",
            " [ 274  131]]\n",
            "\n",
            "Accuracy Score: 0.818\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89      1595\n",
            "           1       0.59      0.32      0.42       405\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      2000\n",
            "   macro avg       0.72      0.63      0.66      2000\n",
            "weighted avg       0.79      0.82      0.80      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}