{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2MwVkDk443U9"
      },
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "***ABSTRACT***\n",
        "\n",
        "***In this hands-on practice we build a self-defined Neural Network from scratch by defining the parameters $\\Theta$, the sigmoid function, derivative of sigmoid, cross-entropy  function, and performing all the calculations for Forward Propagation and Backpropagtion to train the neural network. We also collect the cost history data during the calculation and finally plot the cost convergence diagram to show how the process is going. We use the Churn_Modelling Dataset for the training of our neural network.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cG6XbGfF6mqc"
      },
      "source": [
        "## 1. Churn_Modelling Dataset\n",
        "\n",
        "The Churn Modelling dataset contains customers information of a bank with a flag that s/he exits from the bank within 6 months. We will build an ANN to learn from the dataset and predict if a customer will leave the bank or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nk0zkAdMG0vZ"
      },
      "source": [
        "### 1.1 Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d13XAkG76vho",
        "outputId": "3c4883fe-c80c-4893-d034-d1ed021af7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "datafile = 'https://floobits.com/calvinlow18/ANN/raw/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(datafile)\n",
        "dataset.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
              "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
              "1          2    15647311      Hill  ...               1       112542.58      0\n",
              "2          3    15619304      Onio  ...               0       113931.57      1\n",
              "3          4    15701354      Boni  ...               0        93826.63      0\n",
              "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98O3WxVX7x3M",
        "colab": {}
      },
      "source": [
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "#y shape looks like (m,), make it looks like (m,1)\n",
        "y = y[:,np.newaxis]    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jc5Qt_WtG-tQ"
      },
      "source": [
        "### 1.2 Encoding categorical data and Feature Scaling\n",
        "\n",
        "Encode the country name (string)  and female/male (string) as One Hot Encoding.\n",
        "Standard scaler other numeric data\n",
        "\n",
        "Also need One Hot Encoding, see [Label Encoder vs. One Hot Encoder](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXyBqQElpgCi",
        "colab_type": "code",
        "outputId": "c0aa0c89-5f29-4962-fd04-f6608b082d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "\n",
        "preprocess = make_column_transformer(\n",
        "    (OneHotEncoder(),[1,2]),\n",
        "    (StandardScaler(),[0,3,4,5,6,7,8,9])\n",
        "    #(MinMaxScaler(feature_range=(0, 1)),[0,3,4,5,6,7,8,9])\n",
        ")\n",
        "\n",
        "X = preprocess.fit_transform(X)\n",
        "\n",
        "m = X.shape[0]\n",
        "X = np.hstack((np.ones((m,1)), X))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_V7uylCl43VT"
      },
      "source": [
        "### 1.3 Splitting the dataset into the Training set and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZC-jcbxGDsg0",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vH7w4qLR43Vs",
        "outputId": "57ec5f49-29d6-41df-a223-d428bebc08a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print( X_train.shape )\n",
        "print( X_test.shape )\n",
        "print( y_train.shape )\n",
        "print( y_test.shape )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 14)\n",
            "(2000, 14)\n",
            "(8000, 1)\n",
            "(2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFQoOEm5O6Df"
      },
      "source": [
        "## 2. Build a Neural Network from scratch\n",
        "\n",
        "![Neural Network Model](https://raw.githubusercontent.com/jchen8000/MachineLearning/master/images/NeuralNetwork.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyhY0YbXFH0A",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Cross-Entropy Cost Function\n",
        "\n",
        "> ## $ \\min_\\Theta J(\\Theta)=-\\frac{\\mathrm{1} }{m} \\sum_{i=1}^{m}  \\sum_{k=1}^{K}\\left[ y_k^{(i)} log((h_\\Theta(x^{(i)}))_k) + (1 - y_k^{(i)}) log (1 - (h_\\Theta(x^{(i)}))_k) \\right]  + \\frac{\\mathrm{\\lambda}}{2m}  \\sum_{l=1}^{L-1} \\sum_{i=1}^{S_l}\\sum_{j=1}^{S_l+1}( \\Theta_{ji}^{(l)})^2$\n",
        "\n",
        "> Where $ h_\\Theta(x)  \\in  \\mathbb{R}^K, (h_\\Theta(x))_i = i^{th} output  $\n",
        "\n",
        "> $ L = $ total no. of layers in neural network\n",
        "\n",
        "> $ S_l = $ no. of units (not couning bias unit ) in layer $ l $\n",
        "\n",
        "> ### Think of $ J(\\Theta) \\approx ( h_\\Theta(x^{(i)}) - y^{(i)} ) ^2 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bk-Na74oPVeu"
      },
      "source": [
        "### 2.2 Sigmoid Function and Derivative of Sigmoid\n",
        "\n",
        "*  **Sigmoid Function:**\n",
        "> ## $ g(z) = sigmoid(z) = \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $\n",
        "\n",
        "\n",
        "*  **Derivative of Sigmoid Function:**\n",
        "> ## $\\frac{\\mathrm{d} }{\\mathrm{d} z}g(z) = g(z)(1-g(z)) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ATuXdxt43Wd"
      },
      "source": [
        "### 2.3 Backpropagation\n",
        "\n",
        "> ## $  \\delta^{(3)}_j = a_j^{(3)} - y_j $,  ( total number of layers $ L = 3 $ )\n",
        "\n",
        "> ## $  \\delta^{(2)} = ( \\Theta^{(2)} )^T  \\delta^{(3)} .* g'(z^{(2)}) $\n",
        "\n",
        "> ## $  \\delta^{(1)} = ( \\Theta^{(1)} )^T  \\delta^{(2)} .* g'(z^{(1)}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1Re9DpR1Qk_",
        "colab": {}
      },
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self, inputSize, hiddenSize, outputSize, lmbda):\n",
        "  #parameters\n",
        "    self.inputSize = inputSize\n",
        "    self.outputSize = outputSize\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.lmbda = lmbda\n",
        "    \n",
        "  #weights\n",
        "    epsilon = 0.2\n",
        "    self.theta1 = np.random.randn(self.inputSize, self.hiddenSize)  * 2 * epsilon - epsilon\n",
        "    self.theta2 = np.random.randn(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    #self.theta1 = np.random.rand(self.inputSize, self.hiddenSize) * 2 * epsilon - epsilon\n",
        "    #self.theta2 = np.random.rand(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    \n",
        "  #history\n",
        "    self.loss_history =  [] \n",
        "    self.cost_history =  [] \n",
        "\n",
        "  def forward(self, X):\n",
        "    #forward propagation through our network\n",
        "    self.z = np.dot(X, self.theta1) # dot product of X (input) and first set of 3x2 weights\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = np.dot(self.z2, self.theta2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "    o = self.sigmoid(self.z3) # final activation function\n",
        "    return o\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    # activation function\n",
        "    return 1/(1+np.exp(-s))\n",
        "\n",
        "  def sigmoidDerivative(self, s):\n",
        "    #derivative of sigmoid\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    # backward propagate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidDerivative(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.theta2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidDerivative(self.z2) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.theta1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.theta2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "    \n",
        "  def cost(self, X, y ):\n",
        "    m = len(y)\n",
        "    y_output = self.forward(X)\n",
        "    \n",
        "    c1 = np.multiply(y, np.log(y_output))\n",
        "    c2 = np.multiply(1-y, np.log(1-y_output))\n",
        "    c = np.sum(c1 + c2)\n",
        "    \n",
        "    r1 = np.sum(np.sum(np.power(self.theta1,2), axis = 1))\n",
        "    r2 = np.sum(np.sum(np.power(self.theta2,2), axis = 1))\n",
        "    \n",
        "    return np.sum(c / (-m)) + (r1 + r2) * self.lmbda / (2*m)\n",
        "\n",
        "  \n",
        "  def loss(self, X, y):\n",
        "    return np.mean(np.square(y - self.forward(X)))\n",
        "\n",
        "  def train(self, X, y, epoch):\n",
        "    for i in range(epoch):\n",
        "      o = self.forward(X)\n",
        "      self.backward(X, y, o)\n",
        "      self.loss_history.append(self.loss(X,y))\n",
        "      self.cost_history.append(self.cost(X,y))\n",
        "      print(\"epoch:[\", i, \"], cost: \", str(self.cost(X,y))  )\n",
        "\n",
        "  def predict(self, X):\n",
        "    return self.forward(X)\n",
        "  \n",
        "  \n",
        "  def get_cost_histroy(self):\n",
        "    return self.cost_history\n",
        "  \n",
        "  def get_loss_histroy(self):\n",
        "    return self.loss_history\n",
        "\n",
        "  def get_weight(self):\n",
        "    return self.theta1, self.theta2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1MjhrdvPyAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the convergence of the cost function\n",
        "def plotConvergence(cost_history, iterations):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(len(cost_history)),cost_history,'bo')\n",
        "    plt.grid(True)\n",
        "    plt.title(\"Convergence of Cost Function\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
        "    dummy = plt.ylim([min(cost_history)-0.2*(max(cost_history)-min(cost_history)), max(cost_history)+0.2*(max(cost_history)-min(cost_history))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4suFNvfWj2A",
        "colab_type": "code",
        "outputId": "2e2d69f9-5786-4a14-d93f-a474d28929f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        }
      },
      "source": [
        "NN = Neural_Network( inputSize=14, \n",
        "                     hiddenSize=3, \n",
        "                     outputSize=1, \n",
        "                     lmbda=1 )\n",
        "\n",
        "\n",
        "iterations = 500\n",
        "NN.train(X_train, y_train, iterations)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:[ 0 ], cost:  63.940161140904564\n",
            "epoch:[ 1 ], cost:  55.93949374659857\n",
            "epoch:[ 2 ], cost:  55.08739698680884\n",
            "epoch:[ 3 ], cost:  54.805348166389805\n",
            "epoch:[ 4 ], cost:  53.83122566315066\n",
            "epoch:[ 5 ], cost:  51.620583606707164\n",
            "epoch:[ 6 ], cost:  49.29434003359889\n",
            "epoch:[ 7 ], cost:  46.50756535977918\n",
            "epoch:[ 8 ], cost:  45.80607169951079\n",
            "epoch:[ 9 ], cost:  42.82180559521364\n",
            "epoch:[ 10 ], cost:  40.95669874493696\n",
            "epoch:[ 11 ], cost:  40.50161369111902\n",
            "epoch:[ 12 ], cost:  39.295106645001006\n",
            "epoch:[ 13 ], cost:  39.796575851396675\n",
            "epoch:[ 14 ], cost:  38.61105041681028\n",
            "epoch:[ 15 ], cost:  38.58100706605994\n",
            "epoch:[ 16 ], cost:  36.36960372187005\n",
            "epoch:[ 17 ], cost:  36.40401101375056\n",
            "epoch:[ 18 ], cost:  35.80401181246204\n",
            "epoch:[ 19 ], cost:  35.8666563660595\n",
            "epoch:[ 20 ], cost:  35.76799149290531\n",
            "epoch:[ 21 ], cost:  35.96168759513426\n",
            "epoch:[ 22 ], cost:  35.48226044303727\n",
            "epoch:[ 23 ], cost:  35.857342055703576\n",
            "epoch:[ 24 ], cost:  35.83319649330391\n",
            "epoch:[ 25 ], cost:  36.18874333669373\n",
            "epoch:[ 26 ], cost:  36.146721030535886\n",
            "epoch:[ 27 ], cost:  35.84441836359632\n",
            "epoch:[ 28 ], cost:  36.14389830815717\n",
            "epoch:[ 29 ], cost:  35.69295064792696\n",
            "epoch:[ 30 ], cost:  34.88906442173902\n",
            "epoch:[ 31 ], cost:  34.728565783492876\n",
            "epoch:[ 32 ], cost:  34.339975664687046\n",
            "epoch:[ 33 ], cost:  33.943492038266555\n",
            "epoch:[ 34 ], cost:  34.472066573485876\n",
            "epoch:[ 35 ], cost:  32.518776508389976\n",
            "epoch:[ 36 ], cost:  33.38427745036035\n",
            "epoch:[ 37 ], cost:  32.17330716116473\n",
            "epoch:[ 38 ], cost:  32.949867974510894\n",
            "epoch:[ 39 ], cost:  31.306895436704938\n",
            "epoch:[ 40 ], cost:  31.800180743033376\n",
            "epoch:[ 41 ], cost:  31.839057151175467\n",
            "epoch:[ 42 ], cost:  30.693586611284843\n",
            "epoch:[ 43 ], cost:  30.786265926630143\n",
            "epoch:[ 44 ], cost:  30.949665886515206\n",
            "epoch:[ 45 ], cost:  30.128402667661383\n",
            "epoch:[ 46 ], cost:  29.94184200748058\n",
            "epoch:[ 47 ], cost:  29.697095987451775\n",
            "epoch:[ 48 ], cost:  29.725061353248304\n",
            "epoch:[ 49 ], cost:  29.254171458899346\n",
            "epoch:[ 50 ], cost:  29.608020937130764\n",
            "epoch:[ 51 ], cost:  29.16293057446555\n",
            "epoch:[ 52 ], cost:  29.105726350410837\n",
            "epoch:[ 53 ], cost:  28.2952548874111\n",
            "epoch:[ 54 ], cost:  28.717442871771244\n",
            "epoch:[ 55 ], cost:  28.376667952855907\n",
            "epoch:[ 56 ], cost:  27.5169905870934\n",
            "epoch:[ 57 ], cost:  28.39282239173283\n",
            "epoch:[ 58 ], cost:  27.624794924873573\n",
            "epoch:[ 59 ], cost:  28.091296126167467\n",
            "epoch:[ 60 ], cost:  27.405505334982937\n",
            "epoch:[ 61 ], cost:  27.63570122934893\n",
            "epoch:[ 62 ], cost:  27.010998147632357\n",
            "epoch:[ 63 ], cost:  27.909385649919372\n",
            "epoch:[ 64 ], cost:  27.16200746177018\n",
            "epoch:[ 65 ], cost:  26.869267706065685\n",
            "epoch:[ 66 ], cost:  27.23065390349577\n",
            "epoch:[ 67 ], cost:  26.538357244206843\n",
            "epoch:[ 68 ], cost:  26.77909739532064\n",
            "epoch:[ 69 ], cost:  27.070912056788046\n",
            "epoch:[ 70 ], cost:  26.23705455162727\n",
            "epoch:[ 71 ], cost:  27.449175951250066\n",
            "epoch:[ 72 ], cost:  25.841174918020354\n",
            "epoch:[ 73 ], cost:  27.219089592449215\n",
            "epoch:[ 74 ], cost:  26.46672134644409\n",
            "epoch:[ 75 ], cost:  26.396357606440713\n",
            "epoch:[ 76 ], cost:  26.639686100567467\n",
            "epoch:[ 77 ], cost:  26.19304509906264\n",
            "epoch:[ 78 ], cost:  26.50683018621974\n",
            "epoch:[ 79 ], cost:  26.071727637709742\n",
            "epoch:[ 80 ], cost:  25.869386494192597\n",
            "epoch:[ 81 ], cost:  26.548863627600753\n",
            "epoch:[ 82 ], cost:  25.855726988965365\n",
            "epoch:[ 83 ], cost:  26.002902937709823\n",
            "epoch:[ 84 ], cost:  25.850961635528503\n",
            "epoch:[ 85 ], cost:  25.82803540652642\n",
            "epoch:[ 86 ], cost:  25.774459277334678\n",
            "epoch:[ 87 ], cost:  26.08992778974481\n",
            "epoch:[ 88 ], cost:  25.53933474100704\n",
            "epoch:[ 89 ], cost:  25.756379010750273\n",
            "epoch:[ 90 ], cost:  25.446361224848502\n",
            "epoch:[ 91 ], cost:  26.146582945390676\n",
            "epoch:[ 92 ], cost:  24.93869944667079\n",
            "epoch:[ 93 ], cost:  25.606406515045563\n",
            "epoch:[ 94 ], cost:  25.226501859575965\n",
            "epoch:[ 95 ], cost:  25.562002799388708\n",
            "epoch:[ 96 ], cost:  25.303501108195547\n",
            "epoch:[ 97 ], cost:  25.212187789804627\n",
            "epoch:[ 98 ], cost:  25.56979020031388\n",
            "epoch:[ 99 ], cost:  25.38502791555667\n",
            "epoch:[ 100 ], cost:  25.344735894870375\n",
            "epoch:[ 101 ], cost:  25.114917143155463\n",
            "epoch:[ 102 ], cost:  25.153375980921787\n",
            "epoch:[ 103 ], cost:  24.93514199834901\n",
            "epoch:[ 104 ], cost:  24.944366713514416\n",
            "epoch:[ 105 ], cost:  24.831898765833834\n",
            "epoch:[ 106 ], cost:  25.119087471026283\n",
            "epoch:[ 107 ], cost:  25.204125969473036\n",
            "epoch:[ 108 ], cost:  24.432465174397144\n",
            "epoch:[ 109 ], cost:  24.78625666444122\n",
            "epoch:[ 110 ], cost:  24.838189296048863\n",
            "epoch:[ 111 ], cost:  24.52116829351\n",
            "epoch:[ 112 ], cost:  25.040935804971625\n",
            "epoch:[ 113 ], cost:  23.89507443766198\n",
            "epoch:[ 114 ], cost:  24.483616548338944\n",
            "epoch:[ 115 ], cost:  23.891378682613006\n",
            "epoch:[ 116 ], cost:  24.01963059097922\n",
            "epoch:[ 117 ], cost:  23.90567914227022\n",
            "epoch:[ 118 ], cost:  23.461572818577256\n",
            "epoch:[ 119 ], cost:  23.77511887122344\n",
            "epoch:[ 120 ], cost:  23.55697578345198\n",
            "epoch:[ 121 ], cost:  23.850218391120695\n",
            "epoch:[ 122 ], cost:  23.127522946387476\n",
            "epoch:[ 123 ], cost:  24.453682093619296\n",
            "epoch:[ 124 ], cost:  22.445430085335587\n",
            "epoch:[ 125 ], cost:  23.690924614977636\n",
            "epoch:[ 126 ], cost:  22.90830075136933\n",
            "epoch:[ 127 ], cost:  23.587449855320216\n",
            "epoch:[ 128 ], cost:  23.222356553030576\n",
            "epoch:[ 129 ], cost:  23.439788747413907\n",
            "epoch:[ 130 ], cost:  23.055428637850966\n",
            "epoch:[ 131 ], cost:  22.612406249101838\n",
            "epoch:[ 132 ], cost:  23.48430906100792\n",
            "epoch:[ 133 ], cost:  22.858220157615733\n",
            "epoch:[ 134 ], cost:  23.778891998870588\n",
            "epoch:[ 135 ], cost:  22.975092257691607\n",
            "epoch:[ 136 ], cost:  23.768592868687545\n",
            "epoch:[ 137 ], cost:  22.48869050974793\n",
            "epoch:[ 138 ], cost:  23.499083578067463\n",
            "epoch:[ 139 ], cost:  23.164198594152346\n",
            "epoch:[ 140 ], cost:  23.015804578972034\n",
            "epoch:[ 141 ], cost:  22.951852534018354\n",
            "epoch:[ 142 ], cost:  23.86204130202186\n",
            "epoch:[ 143 ], cost:  22.650220992544526\n",
            "epoch:[ 144 ], cost:  23.378534309039217\n",
            "epoch:[ 145 ], cost:  22.892936821928053\n",
            "epoch:[ 146 ], cost:  23.206821528467415\n",
            "epoch:[ 147 ], cost:  22.896073515195592\n",
            "epoch:[ 148 ], cost:  23.337717996589696\n",
            "epoch:[ 149 ], cost:  22.454896811568805\n",
            "epoch:[ 150 ], cost:  23.460632608809856\n",
            "epoch:[ 151 ], cost:  22.72306416994735\n",
            "epoch:[ 152 ], cost:  23.494287614023925\n",
            "epoch:[ 153 ], cost:  22.297885794242518\n",
            "epoch:[ 154 ], cost:  23.13792302167252\n",
            "epoch:[ 155 ], cost:  23.24338896205477\n",
            "epoch:[ 156 ], cost:  22.912171416171976\n",
            "epoch:[ 157 ], cost:  22.95602829047199\n",
            "epoch:[ 158 ], cost:  22.832724823568565\n",
            "epoch:[ 159 ], cost:  23.01070209692757\n",
            "epoch:[ 160 ], cost:  22.563800618296344\n",
            "epoch:[ 161 ], cost:  23.125545309080884\n",
            "epoch:[ 162 ], cost:  23.125893093840325\n",
            "epoch:[ 163 ], cost:  23.023213317145785\n",
            "epoch:[ 164 ], cost:  23.325941709074815\n",
            "epoch:[ 165 ], cost:  22.960928578643067\n",
            "epoch:[ 166 ], cost:  23.085969605967133\n",
            "epoch:[ 167 ], cost:  23.482362867379187\n",
            "epoch:[ 168 ], cost:  22.52992292167074\n",
            "epoch:[ 169 ], cost:  23.033746038704066\n",
            "epoch:[ 170 ], cost:  23.04508025215282\n",
            "epoch:[ 171 ], cost:  22.868056062731814\n",
            "epoch:[ 172 ], cost:  22.828728224288483\n",
            "epoch:[ 173 ], cost:  23.474442309491394\n",
            "epoch:[ 174 ], cost:  22.76531284027089\n",
            "epoch:[ 175 ], cost:  22.88569627037399\n",
            "epoch:[ 176 ], cost:  22.988988526090484\n",
            "epoch:[ 177 ], cost:  22.78058221273619\n",
            "epoch:[ 178 ], cost:  22.96392070012105\n",
            "epoch:[ 179 ], cost:  23.092774427722137\n",
            "epoch:[ 180 ], cost:  22.88363949205069\n",
            "epoch:[ 181 ], cost:  22.95932806520722\n",
            "epoch:[ 182 ], cost:  22.942217530992547\n",
            "epoch:[ 183 ], cost:  23.166148099432938\n",
            "epoch:[ 184 ], cost:  22.806675304075128\n",
            "epoch:[ 185 ], cost:  22.856167136687013\n",
            "epoch:[ 186 ], cost:  22.925982732110967\n",
            "epoch:[ 187 ], cost:  23.145753423016355\n",
            "epoch:[ 188 ], cost:  22.807081849570856\n",
            "epoch:[ 189 ], cost:  23.04304354671457\n",
            "epoch:[ 190 ], cost:  22.760305676209846\n",
            "epoch:[ 191 ], cost:  22.802630315416373\n",
            "epoch:[ 192 ], cost:  23.248242160684242\n",
            "epoch:[ 193 ], cost:  22.933014365056625\n",
            "epoch:[ 194 ], cost:  23.153070904984368\n",
            "epoch:[ 195 ], cost:  22.87350877434271\n",
            "epoch:[ 196 ], cost:  23.301732459168647\n",
            "epoch:[ 197 ], cost:  22.922450480970515\n",
            "epoch:[ 198 ], cost:  23.260511390368162\n",
            "epoch:[ 199 ], cost:  22.78535289589291\n",
            "epoch:[ 200 ], cost:  22.698836580982324\n",
            "epoch:[ 201 ], cost:  23.098171189391646\n",
            "epoch:[ 202 ], cost:  22.559204382205916\n",
            "epoch:[ 203 ], cost:  22.852500570006846\n",
            "epoch:[ 204 ], cost:  22.865149477152634\n",
            "epoch:[ 205 ], cost:  22.598309673511203\n",
            "epoch:[ 206 ], cost:  22.926017272353253\n",
            "epoch:[ 207 ], cost:  22.42382846804523\n",
            "epoch:[ 208 ], cost:  22.94351658424135\n",
            "epoch:[ 209 ], cost:  22.471160825090465\n",
            "epoch:[ 210 ], cost:  22.677967825465306\n",
            "epoch:[ 211 ], cost:  22.629877229019453\n",
            "epoch:[ 212 ], cost:  22.75767043494469\n",
            "epoch:[ 213 ], cost:  22.71035221454092\n",
            "epoch:[ 214 ], cost:  22.59868975503696\n",
            "epoch:[ 215 ], cost:  22.59536420049805\n",
            "epoch:[ 216 ], cost:  22.60526982463093\n",
            "epoch:[ 217 ], cost:  23.19180105588958\n",
            "epoch:[ 218 ], cost:  22.691293066402054\n",
            "epoch:[ 219 ], cost:  22.832406977622632\n",
            "epoch:[ 220 ], cost:  22.675523568889812\n",
            "epoch:[ 221 ], cost:  22.377110166708977\n",
            "epoch:[ 222 ], cost:  22.893948880091912\n",
            "epoch:[ 223 ], cost:  22.694078459385405\n",
            "epoch:[ 224 ], cost:  22.636625886063598\n",
            "epoch:[ 225 ], cost:  22.56676337311216\n",
            "epoch:[ 226 ], cost:  22.71940442151965\n",
            "epoch:[ 227 ], cost:  22.388155937336663\n",
            "epoch:[ 228 ], cost:  22.599069360780383\n",
            "epoch:[ 229 ], cost:  22.178710223355658\n",
            "epoch:[ 230 ], cost:  22.53870414685541\n",
            "epoch:[ 231 ], cost:  22.45991092676641\n",
            "epoch:[ 232 ], cost:  22.193769571531483\n",
            "epoch:[ 233 ], cost:  22.3568614973223\n",
            "epoch:[ 234 ], cost:  22.33208610138508\n",
            "epoch:[ 235 ], cost:  21.86197251427359\n",
            "epoch:[ 236 ], cost:  22.63174043040688\n",
            "epoch:[ 237 ], cost:  21.88857590152264\n",
            "epoch:[ 238 ], cost:  22.48036002348449\n",
            "epoch:[ 239 ], cost:  21.93859872539165\n",
            "epoch:[ 240 ], cost:  22.35199293690872\n",
            "epoch:[ 241 ], cost:  21.89767577110474\n",
            "epoch:[ 242 ], cost:  22.434799492036454\n",
            "epoch:[ 243 ], cost:  21.430687600491733\n",
            "epoch:[ 244 ], cost:  22.36150700684211\n",
            "epoch:[ 245 ], cost:  21.7727441847848\n",
            "epoch:[ 246 ], cost:  22.23605072417859\n",
            "epoch:[ 247 ], cost:  21.660236585612815\n",
            "epoch:[ 248 ], cost:  21.938481391883776\n",
            "epoch:[ 249 ], cost:  21.737109557704017\n",
            "epoch:[ 250 ], cost:  21.8245374326835\n",
            "epoch:[ 251 ], cost:  21.445558310650178\n",
            "epoch:[ 252 ], cost:  21.495910715580035\n",
            "epoch:[ 253 ], cost:  21.669775484848742\n",
            "epoch:[ 254 ], cost:  21.141954196524313\n",
            "epoch:[ 255 ], cost:  21.661045279722202\n",
            "epoch:[ 256 ], cost:  21.23999263079451\n",
            "epoch:[ 257 ], cost:  21.505766937813945\n",
            "epoch:[ 258 ], cost:  21.328172712208985\n",
            "epoch:[ 259 ], cost:  21.385013647447806\n",
            "epoch:[ 260 ], cost:  21.47998513783856\n",
            "epoch:[ 261 ], cost:  21.36191464898686\n",
            "epoch:[ 262 ], cost:  21.077640602452057\n",
            "epoch:[ 263 ], cost:  21.32587070797691\n",
            "epoch:[ 264 ], cost:  21.225121483116677\n",
            "epoch:[ 265 ], cost:  21.086368077629633\n",
            "epoch:[ 266 ], cost:  21.036335385388764\n",
            "epoch:[ 267 ], cost:  21.32996350572598\n",
            "epoch:[ 268 ], cost:  21.109240810683765\n",
            "epoch:[ 269 ], cost:  20.94859941965072\n",
            "epoch:[ 270 ], cost:  21.17396417576414\n",
            "epoch:[ 271 ], cost:  20.8855055209664\n",
            "epoch:[ 272 ], cost:  21.170832407651332\n",
            "epoch:[ 273 ], cost:  20.48967017826975\n",
            "epoch:[ 274 ], cost:  20.855115954834094\n",
            "epoch:[ 275 ], cost:  20.353142079847586\n",
            "epoch:[ 276 ], cost:  20.482829349674553\n",
            "epoch:[ 277 ], cost:  20.598688027802474\n",
            "epoch:[ 278 ], cost:  20.50828772532599\n",
            "epoch:[ 279 ], cost:  20.736616427631375\n",
            "epoch:[ 280 ], cost:  20.16812297014362\n",
            "epoch:[ 281 ], cost:  20.600187585086438\n",
            "epoch:[ 282 ], cost:  20.471530379548074\n",
            "epoch:[ 283 ], cost:  20.10420048798227\n",
            "epoch:[ 284 ], cost:  20.356373604215726\n",
            "epoch:[ 285 ], cost:  20.425587690207774\n",
            "epoch:[ 286 ], cost:  20.206419936609578\n",
            "epoch:[ 287 ], cost:  20.397987256783956\n",
            "epoch:[ 288 ], cost:  20.25159465442374\n",
            "epoch:[ 289 ], cost:  20.334901189405997\n",
            "epoch:[ 290 ], cost:  20.065593247558517\n",
            "epoch:[ 291 ], cost:  20.442987697216545\n",
            "epoch:[ 292 ], cost:  19.738336274970663\n",
            "epoch:[ 293 ], cost:  20.348931379284394\n",
            "epoch:[ 294 ], cost:  19.856030538249794\n",
            "epoch:[ 295 ], cost:  20.30499870932585\n",
            "epoch:[ 296 ], cost:  20.054728978746684\n",
            "epoch:[ 297 ], cost:  19.810727182486062\n",
            "epoch:[ 298 ], cost:  20.233873593718634\n",
            "epoch:[ 299 ], cost:  19.74635678239531\n",
            "epoch:[ 300 ], cost:  20.290713803668318\n",
            "epoch:[ 301 ], cost:  20.02841053444629\n",
            "epoch:[ 302 ], cost:  20.141741761290703\n",
            "epoch:[ 303 ], cost:  19.98491922981434\n",
            "epoch:[ 304 ], cost:  19.918974841094844\n",
            "epoch:[ 305 ], cost:  20.00087870774145\n",
            "epoch:[ 306 ], cost:  20.041498090183893\n",
            "epoch:[ 307 ], cost:  20.08096724382603\n",
            "epoch:[ 308 ], cost:  19.919151884126098\n",
            "epoch:[ 309 ], cost:  19.737648019790207\n",
            "epoch:[ 310 ], cost:  19.79223714029164\n",
            "epoch:[ 311 ], cost:  20.13119850890102\n",
            "epoch:[ 312 ], cost:  19.903395841187695\n",
            "epoch:[ 313 ], cost:  20.086820457804713\n",
            "epoch:[ 314 ], cost:  19.532224708736926\n",
            "epoch:[ 315 ], cost:  20.369482123182856\n",
            "epoch:[ 316 ], cost:  19.303634595996126\n",
            "epoch:[ 317 ], cost:  19.88705166741919\n",
            "epoch:[ 318 ], cost:  20.08565007279243\n",
            "epoch:[ 319 ], cost:  19.912571556465277\n",
            "epoch:[ 320 ], cost:  19.932871514019176\n",
            "epoch:[ 321 ], cost:  20.049700990299083\n",
            "epoch:[ 322 ], cost:  19.807280614064847\n",
            "epoch:[ 323 ], cost:  19.93198183306437\n",
            "epoch:[ 324 ], cost:  19.89364574773101\n",
            "epoch:[ 325 ], cost:  19.81794646325113\n",
            "epoch:[ 326 ], cost:  19.78554601478641\n",
            "epoch:[ 327 ], cost:  19.76932521680323\n",
            "epoch:[ 328 ], cost:  20.026436202084753\n",
            "epoch:[ 329 ], cost:  19.812138141773453\n",
            "epoch:[ 330 ], cost:  20.031919564086607\n",
            "epoch:[ 331 ], cost:  19.53621957380347\n",
            "epoch:[ 332 ], cost:  19.954569299491745\n",
            "epoch:[ 333 ], cost:  19.59657263005856\n",
            "epoch:[ 334 ], cost:  19.936125343645557\n",
            "epoch:[ 335 ], cost:  19.601823121941997\n",
            "epoch:[ 336 ], cost:  19.80843619136555\n",
            "epoch:[ 337 ], cost:  19.912599929294554\n",
            "epoch:[ 338 ], cost:  20.037015663489672\n",
            "epoch:[ 339 ], cost:  19.31308149891133\n",
            "epoch:[ 340 ], cost:  20.21561100507014\n",
            "epoch:[ 341 ], cost:  19.680893428846655\n",
            "epoch:[ 342 ], cost:  19.746694456379252\n",
            "epoch:[ 343 ], cost:  19.6870663377149\n",
            "epoch:[ 344 ], cost:  20.099170906185883\n",
            "epoch:[ 345 ], cost:  19.507971891168708\n",
            "epoch:[ 346 ], cost:  19.650626954229757\n",
            "epoch:[ 347 ], cost:  19.82342143775076\n",
            "epoch:[ 348 ], cost:  19.646005644612085\n",
            "epoch:[ 349 ], cost:  19.674840109403785\n",
            "epoch:[ 350 ], cost:  19.782839985582736\n",
            "epoch:[ 351 ], cost:  19.58152207594746\n",
            "epoch:[ 352 ], cost:  19.624627710634815\n",
            "epoch:[ 353 ], cost:  19.456193447283464\n",
            "epoch:[ 354 ], cost:  19.878084977297327\n",
            "epoch:[ 355 ], cost:  19.697525658524533\n",
            "epoch:[ 356 ], cost:  19.3522242639741\n",
            "epoch:[ 357 ], cost:  19.600443434581045\n",
            "epoch:[ 358 ], cost:  19.78047869453463\n",
            "epoch:[ 359 ], cost:  19.431647725148068\n",
            "epoch:[ 360 ], cost:  20.10743085188406\n",
            "epoch:[ 361 ], cost:  19.37324145975364\n",
            "epoch:[ 362 ], cost:  19.94920119440008\n",
            "epoch:[ 363 ], cost:  19.301847747172754\n",
            "epoch:[ 364 ], cost:  19.90891984228679\n",
            "epoch:[ 365 ], cost:  19.557252990337535\n",
            "epoch:[ 366 ], cost:  20.227745935306714\n",
            "epoch:[ 367 ], cost:  19.47968238131395\n",
            "epoch:[ 368 ], cost:  19.897845981338502\n",
            "epoch:[ 369 ], cost:  19.781752911510207\n",
            "epoch:[ 370 ], cost:  19.61919330183059\n",
            "epoch:[ 371 ], cost:  19.625201554450804\n",
            "epoch:[ 372 ], cost:  19.803569631002265\n",
            "epoch:[ 373 ], cost:  19.611629037560697\n",
            "epoch:[ 374 ], cost:  19.590268804776343\n",
            "epoch:[ 375 ], cost:  19.710636700033014\n",
            "epoch:[ 376 ], cost:  19.512838351565193\n",
            "epoch:[ 377 ], cost:  19.528744964495424\n",
            "epoch:[ 378 ], cost:  19.26178820382359\n",
            "epoch:[ 379 ], cost:  19.962124238611505\n",
            "epoch:[ 380 ], cost:  19.504941236443084\n",
            "epoch:[ 381 ], cost:  20.19394426497435\n",
            "epoch:[ 382 ], cost:  19.479462785860186\n",
            "epoch:[ 383 ], cost:  19.52147844601646\n",
            "epoch:[ 384 ], cost:  19.588795249088168\n",
            "epoch:[ 385 ], cost:  19.662840015932336\n",
            "epoch:[ 386 ], cost:  19.42915951240423\n",
            "epoch:[ 387 ], cost:  19.737358748345475\n",
            "epoch:[ 388 ], cost:  19.479436446585684\n",
            "epoch:[ 389 ], cost:  19.860628891062834\n",
            "epoch:[ 390 ], cost:  19.517470641519164\n",
            "epoch:[ 391 ], cost:  20.045776267383363\n",
            "epoch:[ 392 ], cost:  19.339026832977964\n",
            "epoch:[ 393 ], cost:  19.887894834216993\n",
            "epoch:[ 394 ], cost:  19.577032695644494\n",
            "epoch:[ 395 ], cost:  19.952308401289475\n",
            "epoch:[ 396 ], cost:  19.47591343924205\n",
            "epoch:[ 397 ], cost:  19.813986065722005\n",
            "epoch:[ 398 ], cost:  19.479896428767084\n",
            "epoch:[ 399 ], cost:  19.82721010160675\n",
            "epoch:[ 400 ], cost:  19.721031340490573\n",
            "epoch:[ 401 ], cost:  19.340348409652062\n",
            "epoch:[ 402 ], cost:  19.875524249982263\n",
            "epoch:[ 403 ], cost:  19.779171884107672\n",
            "epoch:[ 404 ], cost:  19.761610359628257\n",
            "epoch:[ 405 ], cost:  19.83101574062812\n",
            "epoch:[ 406 ], cost:  19.6659361674339\n",
            "epoch:[ 407 ], cost:  19.670201869215806\n",
            "epoch:[ 408 ], cost:  19.586520751613083\n",
            "epoch:[ 409 ], cost:  19.932578151008606\n",
            "epoch:[ 410 ], cost:  19.349447097444383\n",
            "epoch:[ 411 ], cost:  20.059461559506232\n",
            "epoch:[ 412 ], cost:  19.321341564144355\n",
            "epoch:[ 413 ], cost:  20.07911518939678\n",
            "epoch:[ 414 ], cost:  19.666282714231873\n",
            "epoch:[ 415 ], cost:  20.000544810482186\n",
            "epoch:[ 416 ], cost:  19.77750608666034\n",
            "epoch:[ 417 ], cost:  20.18102745960765\n",
            "epoch:[ 418 ], cost:  19.474253803273676\n",
            "epoch:[ 419 ], cost:  19.97606831117052\n",
            "epoch:[ 420 ], cost:  19.38406793583942\n",
            "epoch:[ 421 ], cost:  19.867098820864438\n",
            "epoch:[ 422 ], cost:  20.033980584688923\n",
            "epoch:[ 423 ], cost:  19.8771058248452\n",
            "epoch:[ 424 ], cost:  19.556139339225492\n",
            "epoch:[ 425 ], cost:  19.888753294665445\n",
            "epoch:[ 426 ], cost:  19.85734336605256\n",
            "epoch:[ 427 ], cost:  19.726965371018025\n",
            "epoch:[ 428 ], cost:  20.06634241360966\n",
            "epoch:[ 429 ], cost:  19.849655209668878\n",
            "epoch:[ 430 ], cost:  20.026296203236335\n",
            "epoch:[ 431 ], cost:  19.69306915929925\n",
            "epoch:[ 432 ], cost:  19.83506734470119\n",
            "epoch:[ 433 ], cost:  20.061513358887762\n",
            "epoch:[ 434 ], cost:  19.58535627079931\n",
            "epoch:[ 435 ], cost:  19.959616329525627\n",
            "epoch:[ 436 ], cost:  19.91343013194294\n",
            "epoch:[ 437 ], cost:  19.93167367265874\n",
            "epoch:[ 438 ], cost:  19.44423283042441\n",
            "epoch:[ 439 ], cost:  19.978771957579802\n",
            "epoch:[ 440 ], cost:  20.059926201807226\n",
            "epoch:[ 441 ], cost:  19.681774451303312\n",
            "epoch:[ 442 ], cost:  20.3253552618323\n",
            "epoch:[ 443 ], cost:  19.67924263331208\n",
            "epoch:[ 444 ], cost:  20.142282258051342\n",
            "epoch:[ 445 ], cost:  19.820368696785838\n",
            "epoch:[ 446 ], cost:  19.787231736056828\n",
            "epoch:[ 447 ], cost:  19.922328810502673\n",
            "epoch:[ 448 ], cost:  19.964473962388222\n",
            "epoch:[ 449 ], cost:  19.611489997200724\n",
            "epoch:[ 450 ], cost:  19.98265867755256\n",
            "epoch:[ 451 ], cost:  19.740952327172558\n",
            "epoch:[ 452 ], cost:  19.82570156733237\n",
            "epoch:[ 453 ], cost:  20.031298604873008\n",
            "epoch:[ 454 ], cost:  19.622442845505304\n",
            "epoch:[ 455 ], cost:  20.090764821057924\n",
            "epoch:[ 456 ], cost:  19.50434659055362\n",
            "epoch:[ 457 ], cost:  19.938277120521654\n",
            "epoch:[ 458 ], cost:  19.817853335326713\n",
            "epoch:[ 459 ], cost:  19.712197168294235\n",
            "epoch:[ 460 ], cost:  19.697364730223455\n",
            "epoch:[ 461 ], cost:  19.844523296930788\n",
            "epoch:[ 462 ], cost:  19.610925142677896\n",
            "epoch:[ 463 ], cost:  19.943814082908855\n",
            "epoch:[ 464 ], cost:  19.573290004120462\n",
            "epoch:[ 465 ], cost:  19.973746466905407\n",
            "epoch:[ 466 ], cost:  19.917763663597526\n",
            "epoch:[ 467 ], cost:  19.797209112485305\n",
            "epoch:[ 468 ], cost:  19.534446361150366\n",
            "epoch:[ 469 ], cost:  19.898533943451678\n",
            "epoch:[ 470 ], cost:  19.973315889133332\n",
            "epoch:[ 471 ], cost:  19.68275257566948\n",
            "epoch:[ 472 ], cost:  19.761091371142477\n",
            "epoch:[ 473 ], cost:  19.916820602968365\n",
            "epoch:[ 474 ], cost:  19.834296558563395\n",
            "epoch:[ 475 ], cost:  19.694789796488504\n",
            "epoch:[ 476 ], cost:  19.513301571826894\n",
            "epoch:[ 477 ], cost:  19.971157108430354\n",
            "epoch:[ 478 ], cost:  19.590552231072532\n",
            "epoch:[ 479 ], cost:  20.074858500672768\n",
            "epoch:[ 480 ], cost:  19.432030359375382\n",
            "epoch:[ 481 ], cost:  20.014278355703727\n",
            "epoch:[ 482 ], cost:  19.940399480591502\n",
            "epoch:[ 483 ], cost:  19.716350643724958\n",
            "epoch:[ 484 ], cost:  19.773865700450944\n",
            "epoch:[ 485 ], cost:  19.75705663396345\n",
            "epoch:[ 486 ], cost:  19.911365109117227\n",
            "epoch:[ 487 ], cost:  19.854077959644965\n",
            "epoch:[ 488 ], cost:  19.927893947588657\n",
            "epoch:[ 489 ], cost:  19.749517516238473\n",
            "epoch:[ 490 ], cost:  20.012299196511687\n",
            "epoch:[ 491 ], cost:  19.84673547720107\n",
            "epoch:[ 492 ], cost:  19.8826456709113\n",
            "epoch:[ 493 ], cost:  19.57305090711725\n",
            "epoch:[ 494 ], cost:  20.285220385306133\n",
            "epoch:[ 495 ], cost:  19.728099701625574\n",
            "epoch:[ 496 ], cost:  20.22488046532292\n",
            "epoch:[ 497 ], cost:  19.410499985439905\n",
            "epoch:[ 498 ], cost:  20.263182877769268\n",
            "epoch:[ 499 ], cost:  19.832738508513685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-8hMDyzQZqm",
        "colab_type": "code",
        "outputId": "4c197cbc-b46f-4124-a135-403c721b8ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plotConvergence(NN.get_cost_histroy(),iterations )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGhjCJEmSamAIkyRJqsH/D4JJ9BJMAszXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ROChOed9pC",
        "colab_type": "code",
        "outputId": "70ab8a12-d778-4c1e-812e-775134a8054e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = NN.predict(X_test)\n",
        "y_pred = (y_pred >= 0.5)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1553   42]\n",
            " [ 278  127]]\n",
            "\n",
            "Accuracy Score: 0.84\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.97      0.91      1595\n",
            "           1       0.75      0.31      0.44       405\n",
            "\n",
            "   micro avg       0.84      0.84      0.84      2000\n",
            "   macro avg       0.80      0.64      0.67      2000\n",
            "weighted avg       0.83      0.84      0.81      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}