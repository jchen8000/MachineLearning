{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2MwVkDk443U9"
      },
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "***ABSTRACT***\n",
        "\n",
        "***In this hands-on practice we build a self-defined Neural Network from scratch by defining the parameters $\\Theta$, the sigmoid function, derivative of sigmoid, cross-entropy  function, and performing all the calculations for Forward Propagation and Backpropagtion to train the neural network. We also collect the cost history data during the calculation and finally plot the cost convergence diagram to show how the process is going. We use the Churn_Modelling Dataset for the training of our neural network.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cG6XbGfF6mqc"
      },
      "source": [
        "## 1. Churn_Modelling Dataset\n",
        "\n",
        "The Churn Modelling dataset contains customers information of a bank with a flag that s/he exits from the bank within 6 months. We will build an ANN to learn from the dataset and predict if a customer will leave the bank or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nk0zkAdMG0vZ"
      },
      "source": [
        "### 1.1 Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d13XAkG76vho",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "50ec66b6-dd62-4d0e-f5a4-2382c0c4dd5f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "datafile = 'https://floobits.com/calvinlow18/ANN/raw/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(datafile)\n",
        "dataset.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
              "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
              "1          2    15647311      Hill  ...               1       112542.58      0\n",
              "2          3    15619304      Onio  ...               0       113931.57      1\n",
              "3          4    15701354      Boni  ...               0        93826.63      0\n",
              "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98O3WxVX7x3M",
        "colab": {}
      },
      "source": [
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "#y shape looks like (m,), make it looks like (m,1)\n",
        "y = y[:,np.newaxis]                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jc5Qt_WtG-tQ"
      },
      "source": [
        "### 1.2 Encoding categorical data and Feature Scaling\n",
        "\n",
        "Encode the country name (string)  and female/male (string) as One Hot Encoding.\n",
        "Standard scaler other numeric data\n",
        "\n",
        "Also need One Hot Encoding, see [Label Encoder vs. One Hot Encoder](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXyBqQElpgCi",
        "colab_type": "code",
        "outputId": "13a50807-8b54-49fe-c404-d090d0a238a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "\n",
        "preprocess = make_column_transformer(\n",
        "    (OneHotEncoder(),[1,2]),\n",
        "    (StandardScaler(),[0,3,4,5,6,7,8,9])\n",
        ")\n",
        "\n",
        "X = preprocess.fit_transform(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_V7uylCl43VT"
      },
      "source": [
        "### 1.3 Splitting the dataset into the Training set and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZC-jcbxGDsg0",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vH7w4qLR43Vs",
        "outputId": "7833b2e8-5334-4c2b-90d5-45e9e178208e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print( X_train.shape )\n",
        "print( X_test.shape )\n",
        "print( y_train.shape )\n",
        "print( y_test.shape )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 13)\n",
            "(2000, 13)\n",
            "(8000, 1)\n",
            "(2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFQoOEm5O6Df"
      },
      "source": [
        "## 2. Build a Neural Network from scratch\n",
        "\n",
        "![Neural Network Model](https://raw.githubusercontent.com/jchen8000/MachineLearning/master/images/NeuralNetwork.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyhY0YbXFH0A",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Cross-Entropy Cost Function\n",
        "\n",
        "> ## $ \\min_\\Theta J(\\Theta)=-\\frac{\\mathrm{1} }{m} \\sum_{i=1}^{m}  \\sum_{k=1}^{K}\\left[ y_k^{(i)} log((h_\\Theta(x^{(i)}))_k) + (1 - y_k^{(i)}) log (1 - (h_\\Theta(x^{(i)}))_k) \\right]  + \\frac{\\mathrm{\\lambda}}{2m}  \\sum_{l=1}^{L-1} \\sum_{i=1}^{S_l}\\sum_{j=1}^{S_l+1}( \\Theta_{ji}^{(l)})^2$\n",
        "\n",
        "> Where $ h_\\Theta(x)  \\in  \\mathbb{R}^K, (h_\\Theta(x))_i = i^{th} output  $\n",
        "\n",
        "> $ L = $ total no. of layers in neural network\n",
        "\n",
        "> $ S_l = $ no. of units (not couning bias unit ) in layer $ l $\n",
        "\n",
        "> ### Think of $ J(\\Theta) \\approx ( h_\\Theta(x^{(i)}) - y^{(i)} ) ^2 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bk-Na74oPVeu"
      },
      "source": [
        "### 2.2 Sigmoid Function and Derivative of Sigmoid\n",
        "\n",
        "*  **Sigmoid Function:**\n",
        "> ## $ g(z) = sigmoid(z) = \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $\n",
        "\n",
        "\n",
        "*  **Derivative of Sigmoid Function:**\n",
        "> ## $\\frac{\\mathrm{d} }{\\mathrm{d} z}g(z) = g(z)(1-g(z)) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ATuXdxt43Wd"
      },
      "source": [
        "### 2.3 Backpropagation\n",
        "\n",
        "> ## $  \\delta^{(3)}_j = a_j^{(3)} - y_j $,  ( total number of layers $ L = 3 $ )\n",
        "\n",
        "> ## $  \\delta^{(2)} = ( \\Theta^{(2)} )^T  \\delta^{(3)} .* g'(z^{(2)}) $\n",
        "\n",
        "> ## $  \\delta^{(1)} = ( \\Theta^{(1)} )^T  \\delta^{(2)} .* g'(z^{(1)}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1Re9DpR1Qk_",
        "colab": {}
      },
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self, inputSize, hiddenSize, outputSize, lmbda):\n",
        "  #parameters\n",
        "    self.inputSize = inputSize\n",
        "    self.outputSize = outputSize\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.lmbda = lmbda\n",
        "    \n",
        "  #weights\n",
        "    epsilon = 0.1\n",
        "    self.theta1 = np.random.randn(self.inputSize, self.hiddenSize)  * 2 * epsilon - epsilon\n",
        "    self.theta2 = np.random.randn(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    #self.theta1 = np.random.rand(self.inputSize, self.hiddenSize) * 2 * epsilon - epsilon\n",
        "    #self.theta2 = np.random.rand(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    \n",
        "  #history\n",
        "    self.loss_history =  [] \n",
        "    self.cost_history =  [] \n",
        "\n",
        "  def forward(self, X):\n",
        "    #forward propagation through our network\n",
        "    self.z = np.dot(X, self.theta1) # dot product of X (input) and first set of 3x2 weights\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = np.dot(self.z2, self.theta2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "    o = self.sigmoid(self.z3) # final activation function\n",
        "    return o\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    # activation function\n",
        "    return 1/(1+np.exp(-s))\n",
        "\n",
        "  def sigmoidDerivative(self, s):\n",
        "    #derivative of sigmoid\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    # backward propagate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidDerivative(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.theta2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidDerivative(self.z2) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.theta1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.theta2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "    \n",
        "  def cost(self, X, y ):\n",
        "    m = len(y)\n",
        "    y_output = self.forward(X)\n",
        "    \n",
        "    c1 = np.multiply(y, np.log(y_output))\n",
        "    c2 = np.multiply(1-y, np.log(1-y_output))\n",
        "    c = np.sum(c1 + c2)\n",
        "    \n",
        "    r1 = np.sum(np.sum(np.power(self.theta1,2), axis = 1))\n",
        "    r2 = np.sum(np.sum(np.power(self.theta2,2), axis = 1))\n",
        "    \n",
        "    return np.sum(c / (-m)) + (r1 + r2) * self.lmbda / (2*m)\n",
        "\n",
        "  \n",
        "  def loss(self, X, y):\n",
        "    return np.mean(np.square(y - self.forward(X)))\n",
        "\n",
        "  def train(self, X, y, epoch):\n",
        "    for i in range(epoch):\n",
        "      o = self.forward(X)\n",
        "      self.backward(X, y, o)\n",
        "      self.loss_history.append(self.loss(X,y))\n",
        "      self.cost_history.append(self.cost(X,y))\n",
        "      print(\"epoch:[\", i, \"], cost: \", str(self.cost(X,y))  )\n",
        "\n",
        "  def predict(self, X):\n",
        "    return self.forward(X)\n",
        "  \n",
        "  \n",
        "  def get_cost_histroy(self):\n",
        "    return self.cost_history\n",
        "  \n",
        "  def get_loss_histroy(self):\n",
        "    return self.loss_history\n",
        "\n",
        "  def get_weight(self):\n",
        "    return self.theta1, self.theta2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1MjhrdvPyAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the convergence of the cost function\n",
        "def plotConvergence(cost_history, iterations):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(len(cost_history)),cost_history,'bo')\n",
        "    plt.grid(True)\n",
        "    plt.title(\"Convergence of Cost Function\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
        "    dummy = plt.ylim([min(cost_history)-0.2*(max(cost_history)-min(cost_history)), max(cost_history)+0.2*(max(cost_history)-min(cost_history))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4suFNvfWj2A",
        "colab_type": "code",
        "outputId": "ebe63fad-3f43-41ee-a555-fa5cc75c92f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        }
      },
      "source": [
        "NN = Neural_Network( inputSize=13, \n",
        "                     hiddenSize=3, \n",
        "                     outputSize=1, \n",
        "                     lmbda=0.8 )\n",
        "\n",
        "\n",
        "iterations = 500\n",
        "NN.train(X_train, y_train, iterations)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:[ 0 ], cost:  64.5030813774565\n",
            "epoch:[ 1 ], cost:  52.872646316531\n",
            "epoch:[ 2 ], cost:  50.12117955117704\n",
            "epoch:[ 3 ], cost:  47.64042050500599\n",
            "epoch:[ 4 ], cost:  47.09713905538252\n",
            "epoch:[ 5 ], cost:  45.724543384071644\n",
            "epoch:[ 6 ], cost:  45.56045563529271\n",
            "epoch:[ 7 ], cost:  44.319512080186215\n",
            "epoch:[ 8 ], cost:  42.95773421367766\n",
            "epoch:[ 9 ], cost:  43.484230918805295\n",
            "epoch:[ 10 ], cost:  41.15896839064573\n",
            "epoch:[ 11 ], cost:  41.86507230045493\n",
            "epoch:[ 12 ], cost:  41.31027107575758\n",
            "epoch:[ 13 ], cost:  41.88017051903359\n",
            "epoch:[ 14 ], cost:  40.403733046763854\n",
            "epoch:[ 15 ], cost:  41.016924271827506\n",
            "epoch:[ 16 ], cost:  40.379762840632495\n",
            "epoch:[ 17 ], cost:  40.62058921611463\n",
            "epoch:[ 18 ], cost:  40.04984046398417\n",
            "epoch:[ 19 ], cost:  40.99849517133039\n",
            "epoch:[ 20 ], cost:  39.29030297870875\n",
            "epoch:[ 21 ], cost:  39.74100595918799\n",
            "epoch:[ 22 ], cost:  39.139289358159644\n",
            "epoch:[ 23 ], cost:  39.14923400318915\n",
            "epoch:[ 24 ], cost:  37.89830132845059\n",
            "epoch:[ 25 ], cost:  38.20066348319751\n",
            "epoch:[ 26 ], cost:  37.7838407181733\n",
            "epoch:[ 27 ], cost:  37.650483440869195\n",
            "epoch:[ 28 ], cost:  38.00710238592269\n",
            "epoch:[ 29 ], cost:  37.085778309467266\n",
            "epoch:[ 30 ], cost:  37.46962345073372\n",
            "epoch:[ 31 ], cost:  36.60763659142293\n",
            "epoch:[ 32 ], cost:  37.1530964977854\n",
            "epoch:[ 33 ], cost:  36.376715510759645\n",
            "epoch:[ 34 ], cost:  35.94204476380914\n",
            "epoch:[ 35 ], cost:  36.565917839037695\n",
            "epoch:[ 36 ], cost:  35.82450119176862\n",
            "epoch:[ 37 ], cost:  36.375957112069884\n",
            "epoch:[ 38 ], cost:  35.564966478636215\n",
            "epoch:[ 39 ], cost:  35.56824840382069\n",
            "epoch:[ 40 ], cost:  36.084181987885316\n",
            "epoch:[ 41 ], cost:  35.56111560010257\n",
            "epoch:[ 42 ], cost:  35.18674571504729\n",
            "epoch:[ 43 ], cost:  35.01838703421672\n",
            "epoch:[ 44 ], cost:  35.36785536415465\n",
            "epoch:[ 45 ], cost:  34.746424360824115\n",
            "epoch:[ 46 ], cost:  34.59303755178348\n",
            "epoch:[ 47 ], cost:  34.59748831251338\n",
            "epoch:[ 48 ], cost:  34.11460804551279\n",
            "epoch:[ 49 ], cost:  34.14382390459046\n",
            "epoch:[ 50 ], cost:  33.48107980261162\n",
            "epoch:[ 51 ], cost:  33.58526650166838\n",
            "epoch:[ 52 ], cost:  34.51621455960057\n",
            "epoch:[ 53 ], cost:  32.89495841037845\n",
            "epoch:[ 54 ], cost:  33.95820945396065\n",
            "epoch:[ 55 ], cost:  33.08537877068483\n",
            "epoch:[ 56 ], cost:  33.417902417484704\n",
            "epoch:[ 57 ], cost:  33.05068632406752\n",
            "epoch:[ 58 ], cost:  33.38570173339055\n",
            "epoch:[ 59 ], cost:  32.945640862903716\n",
            "epoch:[ 60 ], cost:  32.83053204480781\n",
            "epoch:[ 61 ], cost:  33.02922187305106\n",
            "epoch:[ 62 ], cost:  32.39082458591129\n",
            "epoch:[ 63 ], cost:  32.67596809459358\n",
            "epoch:[ 64 ], cost:  32.17757005401887\n",
            "epoch:[ 65 ], cost:  32.67777773542888\n",
            "epoch:[ 66 ], cost:  31.54767658374751\n",
            "epoch:[ 67 ], cost:  32.257435961887516\n",
            "epoch:[ 68 ], cost:  32.030961506797766\n",
            "epoch:[ 69 ], cost:  31.808797426853822\n",
            "epoch:[ 70 ], cost:  31.81260014667593\n",
            "epoch:[ 71 ], cost:  31.741575874102054\n",
            "epoch:[ 72 ], cost:  31.990898405168664\n",
            "epoch:[ 73 ], cost:  31.680824074547942\n",
            "epoch:[ 74 ], cost:  31.813092647376912\n",
            "epoch:[ 75 ], cost:  31.585994701917176\n",
            "epoch:[ 76 ], cost:  31.86078657098332\n",
            "epoch:[ 77 ], cost:  31.689218121914383\n",
            "epoch:[ 78 ], cost:  31.959012621816473\n",
            "epoch:[ 79 ], cost:  31.8517182077634\n",
            "epoch:[ 80 ], cost:  31.654576622236988\n",
            "epoch:[ 81 ], cost:  31.40618634560869\n",
            "epoch:[ 82 ], cost:  31.668681805246358\n",
            "epoch:[ 83 ], cost:  31.232472449570643\n",
            "epoch:[ 84 ], cost:  31.341627937783183\n",
            "epoch:[ 85 ], cost:  31.02823999981966\n",
            "epoch:[ 86 ], cost:  31.44315970125682\n",
            "epoch:[ 87 ], cost:  30.97165635929266\n",
            "epoch:[ 88 ], cost:  31.07728186573371\n",
            "epoch:[ 89 ], cost:  30.505158275276138\n",
            "epoch:[ 90 ], cost:  31.124680733236396\n",
            "epoch:[ 91 ], cost:  30.64030729461909\n",
            "epoch:[ 92 ], cost:  31.0525107329149\n",
            "epoch:[ 93 ], cost:  29.735223337594846\n",
            "epoch:[ 94 ], cost:  31.146070982690624\n",
            "epoch:[ 95 ], cost:  30.695344428066896\n",
            "epoch:[ 96 ], cost:  30.217626409147773\n",
            "epoch:[ 97 ], cost:  30.922499461432245\n",
            "epoch:[ 98 ], cost:  29.61357435615773\n",
            "epoch:[ 99 ], cost:  30.540590554142973\n",
            "epoch:[ 100 ], cost:  30.162940626228124\n",
            "epoch:[ 101 ], cost:  30.15033014233886\n",
            "epoch:[ 102 ], cost:  30.229181523630245\n",
            "epoch:[ 103 ], cost:  29.85619441008094\n",
            "epoch:[ 104 ], cost:  29.6735094680186\n",
            "epoch:[ 105 ], cost:  30.290737259102492\n",
            "epoch:[ 106 ], cost:  29.770157326256125\n",
            "epoch:[ 107 ], cost:  30.14454862441172\n",
            "epoch:[ 108 ], cost:  29.717182761574236\n",
            "epoch:[ 109 ], cost:  30.30550498656035\n",
            "epoch:[ 110 ], cost:  29.373624479310678\n",
            "epoch:[ 111 ], cost:  30.193960259205173\n",
            "epoch:[ 112 ], cost:  29.65581616632111\n",
            "epoch:[ 113 ], cost:  30.119178919939593\n",
            "epoch:[ 114 ], cost:  29.365634761235647\n",
            "epoch:[ 115 ], cost:  29.80165999467321\n",
            "epoch:[ 116 ], cost:  30.119491152903308\n",
            "epoch:[ 117 ], cost:  29.80813138971996\n",
            "epoch:[ 118 ], cost:  29.44752496806263\n",
            "epoch:[ 119 ], cost:  29.298935352503396\n",
            "epoch:[ 120 ], cost:  29.977069968352875\n",
            "epoch:[ 121 ], cost:  29.772723783596795\n",
            "epoch:[ 122 ], cost:  29.774845507468534\n",
            "epoch:[ 123 ], cost:  29.708691993722244\n",
            "epoch:[ 124 ], cost:  29.766104546144888\n",
            "epoch:[ 125 ], cost:  29.593162872688595\n",
            "epoch:[ 126 ], cost:  29.81399422844585\n",
            "epoch:[ 127 ], cost:  28.97841778114396\n",
            "epoch:[ 128 ], cost:  29.421431163621836\n",
            "epoch:[ 129 ], cost:  29.01299308663387\n",
            "epoch:[ 130 ], cost:  29.244666544592285\n",
            "epoch:[ 131 ], cost:  29.211324612504875\n",
            "epoch:[ 132 ], cost:  29.00552512144684\n",
            "epoch:[ 133 ], cost:  29.513073925101338\n",
            "epoch:[ 134 ], cost:  28.48021822373538\n",
            "epoch:[ 135 ], cost:  29.23167958719191\n",
            "epoch:[ 136 ], cost:  29.224508202795654\n",
            "epoch:[ 137 ], cost:  29.46029873128863\n",
            "epoch:[ 138 ], cost:  29.44059753520839\n",
            "epoch:[ 139 ], cost:  28.795965262998415\n",
            "epoch:[ 140 ], cost:  29.30547015556439\n",
            "epoch:[ 141 ], cost:  28.863876030268223\n",
            "epoch:[ 142 ], cost:  29.046606813786553\n",
            "epoch:[ 143 ], cost:  28.35101711552452\n",
            "epoch:[ 144 ], cost:  29.111187411775116\n",
            "epoch:[ 145 ], cost:  27.8770364180618\n",
            "epoch:[ 146 ], cost:  28.670171567071307\n",
            "epoch:[ 147 ], cost:  28.75723377162396\n",
            "epoch:[ 148 ], cost:  28.342973940524917\n",
            "epoch:[ 149 ], cost:  28.551018599798088\n",
            "epoch:[ 150 ], cost:  28.533743070443798\n",
            "epoch:[ 151 ], cost:  28.297524140831282\n",
            "epoch:[ 152 ], cost:  28.437855907132082\n",
            "epoch:[ 153 ], cost:  28.683750504231913\n",
            "epoch:[ 154 ], cost:  28.270509914050823\n",
            "epoch:[ 155 ], cost:  28.626511213574485\n",
            "epoch:[ 156 ], cost:  28.55420646623093\n",
            "epoch:[ 157 ], cost:  29.058141205113436\n",
            "epoch:[ 158 ], cost:  28.376511787881455\n",
            "epoch:[ 159 ], cost:  28.80651394991188\n",
            "epoch:[ 160 ], cost:  28.15509052268951\n",
            "epoch:[ 161 ], cost:  28.976847525611706\n",
            "epoch:[ 162 ], cost:  28.759251251324805\n",
            "epoch:[ 163 ], cost:  28.284498057394377\n",
            "epoch:[ 164 ], cost:  28.60651371241753\n",
            "epoch:[ 165 ], cost:  28.80292191061335\n",
            "epoch:[ 166 ], cost:  28.631083614550825\n",
            "epoch:[ 167 ], cost:  28.537277543081327\n",
            "epoch:[ 168 ], cost:  28.503098693358062\n",
            "epoch:[ 169 ], cost:  28.644269996392193\n",
            "epoch:[ 170 ], cost:  28.40971051918491\n",
            "epoch:[ 171 ], cost:  28.655995964333442\n",
            "epoch:[ 172 ], cost:  27.945822049145022\n",
            "epoch:[ 173 ], cost:  28.744557133444452\n",
            "epoch:[ 174 ], cost:  28.164970773597638\n",
            "epoch:[ 175 ], cost:  28.381955952154673\n",
            "epoch:[ 176 ], cost:  28.610255051486984\n",
            "epoch:[ 177 ], cost:  27.94300705250282\n",
            "epoch:[ 178 ], cost:  28.63235272801109\n",
            "epoch:[ 179 ], cost:  28.584608613677933\n",
            "epoch:[ 180 ], cost:  28.34392365154232\n",
            "epoch:[ 181 ], cost:  28.8281601866444\n",
            "epoch:[ 182 ], cost:  28.161077441116014\n",
            "epoch:[ 183 ], cost:  28.76323806733135\n",
            "epoch:[ 184 ], cost:  28.25350820379915\n",
            "epoch:[ 185 ], cost:  28.68764271752522\n",
            "epoch:[ 186 ], cost:  27.922979195429967\n",
            "epoch:[ 187 ], cost:  28.63547905481201\n",
            "epoch:[ 188 ], cost:  28.049028706260785\n",
            "epoch:[ 189 ], cost:  28.357744129930524\n",
            "epoch:[ 190 ], cost:  28.107225726151476\n",
            "epoch:[ 191 ], cost:  28.214450013121585\n",
            "epoch:[ 192 ], cost:  28.219047976088042\n",
            "epoch:[ 193 ], cost:  28.143513826606352\n",
            "epoch:[ 194 ], cost:  28.052151228230127\n",
            "epoch:[ 195 ], cost:  27.923005635647417\n",
            "epoch:[ 196 ], cost:  27.93543250760368\n",
            "epoch:[ 197 ], cost:  28.08219835592321\n",
            "epoch:[ 198 ], cost:  28.091790471689848\n",
            "epoch:[ 199 ], cost:  27.806048212276902\n",
            "epoch:[ 200 ], cost:  28.55260760324782\n",
            "epoch:[ 201 ], cost:  27.88786331073095\n",
            "epoch:[ 202 ], cost:  28.15340014430958\n",
            "epoch:[ 203 ], cost:  27.558779648255484\n",
            "epoch:[ 204 ], cost:  28.045546897189947\n",
            "epoch:[ 205 ], cost:  27.63445833503337\n",
            "epoch:[ 206 ], cost:  28.382996579264997\n",
            "epoch:[ 207 ], cost:  27.340602831959288\n",
            "epoch:[ 208 ], cost:  27.984022653645066\n",
            "epoch:[ 209 ], cost:  27.845549640505368\n",
            "epoch:[ 210 ], cost:  28.04560238212523\n",
            "epoch:[ 211 ], cost:  28.024820956214814\n",
            "epoch:[ 212 ], cost:  28.108059609448212\n",
            "epoch:[ 213 ], cost:  27.92034415903135\n",
            "epoch:[ 214 ], cost:  27.752384296128128\n",
            "epoch:[ 215 ], cost:  27.798873324124038\n",
            "epoch:[ 216 ], cost:  27.940286560962335\n",
            "epoch:[ 217 ], cost:  27.788121294287308\n",
            "epoch:[ 218 ], cost:  28.191233655506796\n",
            "epoch:[ 219 ], cost:  27.5543254074367\n",
            "epoch:[ 220 ], cost:  27.801817643411635\n",
            "epoch:[ 221 ], cost:  27.978661568484497\n",
            "epoch:[ 222 ], cost:  27.935603151343642\n",
            "epoch:[ 223 ], cost:  28.103086451853848\n",
            "epoch:[ 224 ], cost:  28.034051061030237\n",
            "epoch:[ 225 ], cost:  27.756513650275966\n",
            "epoch:[ 226 ], cost:  28.148393082006063\n",
            "epoch:[ 227 ], cost:  27.680371782510576\n",
            "epoch:[ 228 ], cost:  28.31414963599304\n",
            "epoch:[ 229 ], cost:  27.587802219952053\n",
            "epoch:[ 230 ], cost:  28.24013490137403\n",
            "epoch:[ 231 ], cost:  27.769136206231195\n",
            "epoch:[ 232 ], cost:  28.32335687908931\n",
            "epoch:[ 233 ], cost:  27.32902711959074\n",
            "epoch:[ 234 ], cost:  27.997672674057927\n",
            "epoch:[ 235 ], cost:  27.748968242087916\n",
            "epoch:[ 236 ], cost:  28.264070235109948\n",
            "epoch:[ 237 ], cost:  27.237907414461894\n",
            "epoch:[ 238 ], cost:  27.950801992797626\n",
            "epoch:[ 239 ], cost:  27.719796307540978\n",
            "epoch:[ 240 ], cost:  27.75139732714151\n",
            "epoch:[ 241 ], cost:  27.93935316069165\n",
            "epoch:[ 242 ], cost:  27.709410973677613\n",
            "epoch:[ 243 ], cost:  28.047238667651175\n",
            "epoch:[ 244 ], cost:  27.832418118499483\n",
            "epoch:[ 245 ], cost:  27.56918249035629\n",
            "epoch:[ 246 ], cost:  28.054456687618025\n",
            "epoch:[ 247 ], cost:  27.911529056628574\n",
            "epoch:[ 248 ], cost:  28.016117453437044\n",
            "epoch:[ 249 ], cost:  27.654965812119777\n",
            "epoch:[ 250 ], cost:  28.553162869748355\n",
            "epoch:[ 251 ], cost:  27.071486165804316\n",
            "epoch:[ 252 ], cost:  27.70857870600371\n",
            "epoch:[ 253 ], cost:  27.813772455406344\n",
            "epoch:[ 254 ], cost:  28.0279105006894\n",
            "epoch:[ 255 ], cost:  28.010583098272015\n",
            "epoch:[ 256 ], cost:  27.67641351794981\n",
            "epoch:[ 257 ], cost:  27.68713282011222\n",
            "epoch:[ 258 ], cost:  28.23661031931115\n",
            "epoch:[ 259 ], cost:  27.430408099221907\n",
            "epoch:[ 260 ], cost:  28.000579813199945\n",
            "epoch:[ 261 ], cost:  27.906438570562784\n",
            "epoch:[ 262 ], cost:  28.177906343856968\n",
            "epoch:[ 263 ], cost:  27.42185684651388\n",
            "epoch:[ 264 ], cost:  28.313533084142332\n",
            "epoch:[ 265 ], cost:  27.576156524069283\n",
            "epoch:[ 266 ], cost:  27.891299887139212\n",
            "epoch:[ 267 ], cost:  27.724376478134236\n",
            "epoch:[ 268 ], cost:  28.039930365235808\n",
            "epoch:[ 269 ], cost:  27.7760762748487\n",
            "epoch:[ 270 ], cost:  27.81574840436551\n",
            "epoch:[ 271 ], cost:  27.82031688596853\n",
            "epoch:[ 272 ], cost:  28.195652374570685\n",
            "epoch:[ 273 ], cost:  27.625796874409243\n",
            "epoch:[ 274 ], cost:  28.302158762094933\n",
            "epoch:[ 275 ], cost:  27.392861256916852\n",
            "epoch:[ 276 ], cost:  28.026324523073633\n",
            "epoch:[ 277 ], cost:  27.848957440266865\n",
            "epoch:[ 278 ], cost:  28.116994529934313\n",
            "epoch:[ 279 ], cost:  27.87361751963219\n",
            "epoch:[ 280 ], cost:  27.98904137560847\n",
            "epoch:[ 281 ], cost:  28.18217434930333\n",
            "epoch:[ 282 ], cost:  27.667309620156438\n",
            "epoch:[ 283 ], cost:  27.844186866760328\n",
            "epoch:[ 284 ], cost:  28.10128873574376\n",
            "epoch:[ 285 ], cost:  27.895040442001996\n",
            "epoch:[ 286 ], cost:  27.997321286100117\n",
            "epoch:[ 287 ], cost:  27.90672218789331\n",
            "epoch:[ 288 ], cost:  28.311891486419064\n",
            "epoch:[ 289 ], cost:  27.665588000221604\n",
            "epoch:[ 290 ], cost:  27.697839920109303\n",
            "epoch:[ 291 ], cost:  28.111396375713298\n",
            "epoch:[ 292 ], cost:  28.012936649388717\n",
            "epoch:[ 293 ], cost:  28.275017226271427\n",
            "epoch:[ 294 ], cost:  27.6103592503242\n",
            "epoch:[ 295 ], cost:  28.057380667800686\n",
            "epoch:[ 296 ], cost:  28.09559127217444\n",
            "epoch:[ 297 ], cost:  27.88240132713125\n",
            "epoch:[ 298 ], cost:  28.182864636161334\n",
            "epoch:[ 299 ], cost:  27.81646332445458\n",
            "epoch:[ 300 ], cost:  27.943109189786316\n",
            "epoch:[ 301 ], cost:  28.16549146882936\n",
            "epoch:[ 302 ], cost:  28.348214353837832\n",
            "epoch:[ 303 ], cost:  27.73979245397325\n",
            "epoch:[ 304 ], cost:  28.079821722653698\n",
            "epoch:[ 305 ], cost:  28.29577805174829\n",
            "epoch:[ 306 ], cost:  27.830880994623477\n",
            "epoch:[ 307 ], cost:  28.2861839033843\n",
            "epoch:[ 308 ], cost:  27.713900898478197\n",
            "epoch:[ 309 ], cost:  28.08937418369368\n",
            "epoch:[ 310 ], cost:  27.84725615441995\n",
            "epoch:[ 311 ], cost:  28.10726384767593\n",
            "epoch:[ 312 ], cost:  28.136893025940804\n",
            "epoch:[ 313 ], cost:  28.100208282925596\n",
            "epoch:[ 314 ], cost:  27.879944779925317\n",
            "epoch:[ 315 ], cost:  28.14027856667674\n",
            "epoch:[ 316 ], cost:  28.047116531793296\n",
            "epoch:[ 317 ], cost:  28.063094498261634\n",
            "epoch:[ 318 ], cost:  28.280748699703032\n",
            "epoch:[ 319 ], cost:  27.99340357722645\n",
            "epoch:[ 320 ], cost:  27.838119550437348\n",
            "epoch:[ 321 ], cost:  28.106304595088773\n",
            "epoch:[ 322 ], cost:  27.973535904952705\n",
            "epoch:[ 323 ], cost:  28.449011314690253\n",
            "epoch:[ 324 ], cost:  27.76789499252557\n",
            "epoch:[ 325 ], cost:  28.146076348384803\n",
            "epoch:[ 326 ], cost:  28.18131705550623\n",
            "epoch:[ 327 ], cost:  28.10088993859428\n",
            "epoch:[ 328 ], cost:  28.268373781563184\n",
            "epoch:[ 329 ], cost:  28.070590724704807\n",
            "epoch:[ 330 ], cost:  28.284530595323794\n",
            "epoch:[ 331 ], cost:  27.927881894664075\n",
            "epoch:[ 332 ], cost:  28.083318766530308\n",
            "epoch:[ 333 ], cost:  27.99950208157197\n",
            "epoch:[ 334 ], cost:  28.128172846904512\n",
            "epoch:[ 335 ], cost:  28.122367035745405\n",
            "epoch:[ 336 ], cost:  28.323519760649084\n",
            "epoch:[ 337 ], cost:  27.862397057776462\n",
            "epoch:[ 338 ], cost:  28.358660226392914\n",
            "epoch:[ 339 ], cost:  27.915275873248035\n",
            "epoch:[ 340 ], cost:  28.41704771551948\n",
            "epoch:[ 341 ], cost:  27.741852480550403\n",
            "epoch:[ 342 ], cost:  28.195658462251064\n",
            "epoch:[ 343 ], cost:  28.079097479301293\n",
            "epoch:[ 344 ], cost:  28.486407052997464\n",
            "epoch:[ 345 ], cost:  27.931852652074454\n",
            "epoch:[ 346 ], cost:  28.40242749947681\n",
            "epoch:[ 347 ], cost:  27.89827318288334\n",
            "epoch:[ 348 ], cost:  28.288226488323744\n",
            "epoch:[ 349 ], cost:  28.134025015029387\n",
            "epoch:[ 350 ], cost:  28.181096984359918\n",
            "epoch:[ 351 ], cost:  28.284905777060874\n",
            "epoch:[ 352 ], cost:  27.99922991010286\n",
            "epoch:[ 353 ], cost:  27.996980684088694\n",
            "epoch:[ 354 ], cost:  28.34226064855761\n",
            "epoch:[ 355 ], cost:  28.143438190397134\n",
            "epoch:[ 356 ], cost:  28.480305269997835\n",
            "epoch:[ 357 ], cost:  28.015993799957787\n",
            "epoch:[ 358 ], cost:  28.541627783396386\n",
            "epoch:[ 359 ], cost:  27.85400392748121\n",
            "epoch:[ 360 ], cost:  28.332961607485586\n",
            "epoch:[ 361 ], cost:  28.121500189951888\n",
            "epoch:[ 362 ], cost:  28.529404979163484\n",
            "epoch:[ 363 ], cost:  28.286993665207508\n",
            "epoch:[ 364 ], cost:  28.198573432728896\n",
            "epoch:[ 365 ], cost:  28.409799169026368\n",
            "epoch:[ 366 ], cost:  28.13184304096657\n",
            "epoch:[ 367 ], cost:  27.799219123564495\n",
            "epoch:[ 368 ], cost:  28.198352611110735\n",
            "epoch:[ 369 ], cost:  28.318279422773124\n",
            "epoch:[ 370 ], cost:  28.370807525186947\n",
            "epoch:[ 371 ], cost:  28.129001945193846\n",
            "epoch:[ 372 ], cost:  28.31928603166171\n",
            "epoch:[ 373 ], cost:  28.18851616817604\n",
            "epoch:[ 374 ], cost:  28.26225251826036\n",
            "epoch:[ 375 ], cost:  28.200795472608924\n",
            "epoch:[ 376 ], cost:  28.191566949968564\n",
            "epoch:[ 377 ], cost:  28.27079148346826\n",
            "epoch:[ 378 ], cost:  28.324495697428695\n",
            "epoch:[ 379 ], cost:  28.140812729848\n",
            "epoch:[ 380 ], cost:  28.2665496524878\n",
            "epoch:[ 381 ], cost:  28.399010665984427\n",
            "epoch:[ 382 ], cost:  28.311213148390458\n",
            "epoch:[ 383 ], cost:  28.125408474018283\n",
            "epoch:[ 384 ], cost:  28.162568417709487\n",
            "epoch:[ 385 ], cost:  28.16908250637595\n",
            "epoch:[ 386 ], cost:  28.201149266442265\n",
            "epoch:[ 387 ], cost:  28.038987771697258\n",
            "epoch:[ 388 ], cost:  28.255622234768744\n",
            "epoch:[ 389 ], cost:  28.548793120528376\n",
            "epoch:[ 390 ], cost:  28.069123443613904\n",
            "epoch:[ 391 ], cost:  28.55349969931754\n",
            "epoch:[ 392 ], cost:  27.989140748998185\n",
            "epoch:[ 393 ], cost:  28.393513911663316\n",
            "epoch:[ 394 ], cost:  28.231152802503598\n",
            "epoch:[ 395 ], cost:  28.25712833033942\n",
            "epoch:[ 396 ], cost:  28.200249307326754\n",
            "epoch:[ 397 ], cost:  28.0596520225585\n",
            "epoch:[ 398 ], cost:  28.29374400541097\n",
            "epoch:[ 399 ], cost:  28.35081234626295\n",
            "epoch:[ 400 ], cost:  28.21417854495416\n",
            "epoch:[ 401 ], cost:  28.417722076164956\n",
            "epoch:[ 402 ], cost:  28.343302998230087\n",
            "epoch:[ 403 ], cost:  28.232562681761323\n",
            "epoch:[ 404 ], cost:  28.368614003884662\n",
            "epoch:[ 405 ], cost:  28.266086581426066\n",
            "epoch:[ 406 ], cost:  28.27285549005361\n",
            "epoch:[ 407 ], cost:  28.14153817798593\n",
            "epoch:[ 408 ], cost:  28.3273833101658\n",
            "epoch:[ 409 ], cost:  28.345266018171046\n",
            "epoch:[ 410 ], cost:  28.308133857668874\n",
            "epoch:[ 411 ], cost:  28.274384121732528\n",
            "epoch:[ 412 ], cost:  28.378181959025156\n",
            "epoch:[ 413 ], cost:  27.980671341188433\n",
            "epoch:[ 414 ], cost:  28.327576863562065\n",
            "epoch:[ 415 ], cost:  28.153743124008322\n",
            "epoch:[ 416 ], cost:  28.211289923332874\n",
            "epoch:[ 417 ], cost:  28.12648138828872\n",
            "epoch:[ 418 ], cost:  28.346059501939344\n",
            "epoch:[ 419 ], cost:  28.100381281449366\n",
            "epoch:[ 420 ], cost:  28.24463042983277\n",
            "epoch:[ 421 ], cost:  27.972729183153547\n",
            "epoch:[ 422 ], cost:  28.057898103914045\n",
            "epoch:[ 423 ], cost:  28.236711956352558\n",
            "epoch:[ 424 ], cost:  28.093838595288123\n",
            "epoch:[ 425 ], cost:  28.082344773763996\n",
            "epoch:[ 426 ], cost:  28.486180772872288\n",
            "epoch:[ 427 ], cost:  27.753204449982356\n",
            "epoch:[ 428 ], cost:  28.06058548505202\n",
            "epoch:[ 429 ], cost:  28.149914330859865\n",
            "epoch:[ 430 ], cost:  28.26275437344468\n",
            "epoch:[ 431 ], cost:  28.239368536314387\n",
            "epoch:[ 432 ], cost:  28.298317242737372\n",
            "epoch:[ 433 ], cost:  28.27347062475797\n",
            "epoch:[ 434 ], cost:  28.19719341310851\n",
            "epoch:[ 435 ], cost:  28.11517225934125\n",
            "epoch:[ 436 ], cost:  28.191733942156155\n",
            "epoch:[ 437 ], cost:  28.25101421267268\n",
            "epoch:[ 438 ], cost:  28.044714982089676\n",
            "epoch:[ 439 ], cost:  28.16346436758639\n",
            "epoch:[ 440 ], cost:  28.32931165430245\n",
            "epoch:[ 441 ], cost:  28.29859667296891\n",
            "epoch:[ 442 ], cost:  28.32913517847782\n",
            "epoch:[ 443 ], cost:  28.211041181130135\n",
            "epoch:[ 444 ], cost:  28.06447877649047\n",
            "epoch:[ 445 ], cost:  28.030147437457423\n",
            "epoch:[ 446 ], cost:  28.323926238826704\n",
            "epoch:[ 447 ], cost:  28.22895286882306\n",
            "epoch:[ 448 ], cost:  28.299993983889163\n",
            "epoch:[ 449 ], cost:  28.34132392134017\n",
            "epoch:[ 450 ], cost:  27.93432250048116\n",
            "epoch:[ 451 ], cost:  27.967056688588208\n",
            "epoch:[ 452 ], cost:  28.336908332220403\n",
            "epoch:[ 453 ], cost:  28.057592001838955\n",
            "epoch:[ 454 ], cost:  28.445208570615286\n",
            "epoch:[ 455 ], cost:  27.99626247407221\n",
            "epoch:[ 456 ], cost:  28.261884911097773\n",
            "epoch:[ 457 ], cost:  28.13810457479758\n",
            "epoch:[ 458 ], cost:  28.179681090832005\n",
            "epoch:[ 459 ], cost:  28.36746500079513\n",
            "epoch:[ 460 ], cost:  28.16735829783059\n",
            "epoch:[ 461 ], cost:  28.334059886329506\n",
            "epoch:[ 462 ], cost:  28.22627532805279\n",
            "epoch:[ 463 ], cost:  28.3792913153276\n",
            "epoch:[ 464 ], cost:  28.109343498609775\n",
            "epoch:[ 465 ], cost:  28.344887140297054\n",
            "epoch:[ 466 ], cost:  27.977339075820517\n",
            "epoch:[ 467 ], cost:  28.255051335868686\n",
            "epoch:[ 468 ], cost:  28.237500296537167\n",
            "epoch:[ 469 ], cost:  28.310789949264183\n",
            "epoch:[ 470 ], cost:  28.07585415476661\n",
            "epoch:[ 471 ], cost:  28.530094153867715\n",
            "epoch:[ 472 ], cost:  27.962578656221694\n",
            "epoch:[ 473 ], cost:  28.22966928697065\n",
            "epoch:[ 474 ], cost:  28.43912925535836\n",
            "epoch:[ 475 ], cost:  28.19066076686998\n",
            "epoch:[ 476 ], cost:  28.5138986110684\n",
            "epoch:[ 477 ], cost:  27.991092667143466\n",
            "epoch:[ 478 ], cost:  28.416830934659743\n",
            "epoch:[ 479 ], cost:  28.22235757113658\n",
            "epoch:[ 480 ], cost:  28.402935513648444\n",
            "epoch:[ 481 ], cost:  28.043139972736057\n",
            "epoch:[ 482 ], cost:  28.4307881252385\n",
            "epoch:[ 483 ], cost:  28.064724719769607\n",
            "epoch:[ 484 ], cost:  28.669694350701867\n",
            "epoch:[ 485 ], cost:  27.94782421090328\n",
            "epoch:[ 486 ], cost:  28.227882111019294\n",
            "epoch:[ 487 ], cost:  28.455797659031347\n",
            "epoch:[ 488 ], cost:  28.442408805732743\n",
            "epoch:[ 489 ], cost:  28.465271207248325\n",
            "epoch:[ 490 ], cost:  28.259620641299872\n",
            "epoch:[ 491 ], cost:  28.431908861679233\n",
            "epoch:[ 492 ], cost:  28.324536038261247\n",
            "epoch:[ 493 ], cost:  28.560090124953348\n",
            "epoch:[ 494 ], cost:  28.128987966276842\n",
            "epoch:[ 495 ], cost:  28.705817306916266\n",
            "epoch:[ 496 ], cost:  28.079555112795312\n",
            "epoch:[ 497 ], cost:  28.724525392561127\n",
            "epoch:[ 498 ], cost:  27.956469363361375\n",
            "epoch:[ 499 ], cost:  28.41488596996358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-8hMDyzQZqm",
        "colab_type": "code",
        "outputId": "86d29add-9d08-4baf-c380-9531a3a58d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plotConvergence(NN.get_cost_histroy(),iterations )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCJMkSaqBIUySJKkGhjBJkqQa/H/qzO/Jjm0AeQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ROChOed9pC",
        "colab_type": "code",
        "outputId": "4a941439-6296-4c4b-bb36-e9c13774fbc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = NN.predict(X_test)\n",
        "y_pred = (y_pred >= 0.5)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1565   30]\n",
            " [ 309   96]]\n",
            "\n",
            "Accuracy Score: 0.8305\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.98      0.90      1595\n",
            "           1       0.76      0.24      0.36       405\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      2000\n",
            "   macro avg       0.80      0.61      0.63      2000\n",
            "weighted avg       0.82      0.83      0.79      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}