{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "2MwVkDk443U9"
      },
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cG6XbGfF6mqc"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P6Fzfikz43VE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nk0zkAdMG0vZ"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Importing the dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d13XAkG76vho",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for some reasons, the data file on github has some problems when reading\n",
        "#datafile = 'https://github.com/jchen8000/MachineLearning/blob/master/Classification/data/Churn_Modelling.csv'\n",
        "\n",
        "#Found the same data file from internet\n",
        "datafile = 'https://floobits.com/calvinlow18/ANN/raw/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(datafile)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v8JcxTYLF3qe",
        "outputId": "bb65fbb5-a46c-4665-864e-df362db4218c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
              "0          1    15634602  Hargrave          619    France  Female   42   \n",
              "1          2    15647311      Hill          608     Spain  Female   41   \n",
              "2          3    15619304      Onio          502    France  Female   42   \n",
              "3          4    15701354      Boni          699    France  Female   39   \n",
              "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
              "\n",
              "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0       2       0.00              1          1               1   \n",
              "1       1   83807.86              1          0               1   \n",
              "2       8  159660.80              3          1               0   \n",
              "3       1       0.00              2          0               0   \n",
              "4       2  125510.82              1          1               1   \n",
              "\n",
              "   EstimatedSalary  Exited  \n",
              "0        101348.88       1  \n",
              "1        112542.58       0  \n",
              "2        113931.57       1  \n",
              "3         93826.63       0  \n",
              "4         79084.10       0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "98O3WxVX7x3M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "#y shape looks like (m,), make it looks like (m,1)\n",
        "y = y[:,np.newaxis]                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Jc5Qt_WtG-tQ"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Encoding categorical data and Feature Scaling\n",
        "\n",
        "Encode the country name (string)  and female/male (string) as One Hot Encoding.\n",
        "Standard scaler other numeric data\n",
        "\n",
        "Also need One Hot Encoding, see [Label Encoder vs. One Hot Encoder](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
      ]
    },
    {
      "metadata": {
        "id": "RXyBqQElpgCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7012e9e7-94c4-4ee6-942b-e8f2b12f3d34"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "\n",
        "preprocess = make_column_transformer(\n",
        "    (OneHotEncoder(),[1,2]),\n",
        "    (StandardScaler(),[0,3,4,5,6,7,8,9])\n",
        ")\n",
        "\n",
        "X = preprocess.fit_transform(X)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_V7uylCl43VT"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Splitting the dataset into the Training set and Test set\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZC-jcbxGDsg0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vH7w4qLR43Vs",
        "outputId": "5c9b3b97-4b44-49a0-a892-f0d94aab5cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print( X_train.shape )\n",
        "print( X_test.shape )\n",
        "print( y_train.shape )\n",
        "print( y_test.shape )"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 13)\n",
            "(2000, 13)\n",
            "(8000, 1)\n",
            "(2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kFQoOEm5O6Df"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Build a Neural Network from scratch\n",
        "\n",
        "![Neural Network Model](https://cdn-images-1.medium.com/max/800/1*l78dvvJFf0cOJnXTJglR7A.png)"
      ]
    },
    {
      "metadata": {
        "id": "ZyhY0YbXFH0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Neural Network Cost Function\n",
        "\n",
        "> ## $ \\min_\\Theta J(\\Theta)=-\\frac{\\mathrm{1} }{m} \\sum_{i=1}^{m}  \\sum_{k=1}^{K}\\left[ y_k^{(i)} log((h_\\Theta(x^{(i)}))_k) + (1 - y_k^{(i)}) log (1 - (h_\\Theta(x^{(i)}))_k) \\right]  + \\frac{\\mathrm{\\lambda}}{2m}  \\sum_{l=1}^{L-1} \\sum_{i=1}^{S_l}\\sum_{j=1}^{S_l+1}( \\Theta_{ji}^{(l)})^2$\n",
        "\n",
        "> Where $ h_\\Theta(x)  \\in  \\mathbb{R}^K, (h_\\Theta(x))_i = i^{th} output  $\n",
        "\n",
        "> $ L = $ total no. of layers in neural network\n",
        "\n",
        "> $ S_l = $ no. of units (not couning bias unit ) in layer $ l $\n",
        "\n",
        "> ### Think of $ J(\\Theta) \\approx ( h_\\Theta(x^{(i)}) - y^{(i)} ) ^2 $"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Bk-Na74oPVeu"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Sigmoid Gredient\n",
        "\n",
        "> ## $\\frac{\\mathrm{d} }{\\mathrm{d} z}g(z) = g(z)(1-g(z)) $\n",
        "\n",
        "> where\n",
        "\n",
        "> ## $ g(z) = sigmoid(z) = \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6ATuXdxt43Wd"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 Backpropagation\n",
        "\n",
        "> ## $  \\delta^{(3)}_j = a_j^{(3)} - y_j $,  ( total number of layers $ L = 3 $ )\n",
        "\n",
        "> ## $  \\delta^{(2)} = ( \\Theta^{(2)} )^T  \\delta^{(3)} .* g'(z^{(2)}) $\n",
        "\n",
        "> ## $  \\delta^{(1)} = ( \\Theta^{(1)} )^T  \\delta^{(2)} .* g'(z^{(1)}) $"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N1Re9DpR1Qk_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self, inputSize, hiddenSize, outputSize, lmbda):\n",
        "  #parameters\n",
        "    self.inputSize = inputSize\n",
        "    self.outputSize = outputSize\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.lmbda = lmbda\n",
        "    \n",
        "  #weights\n",
        "    epsilon = 0.2\n",
        "    self.theta1 = np.random.randn(self.inputSize, self.hiddenSize)  * 2 * epsilon - epsilon\n",
        "    self.theta2 = np.random.randn(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    #self.theta1 = np.random.rand(self.inputSize, self.hiddenSize) * 2 * epsilon - epsilon\n",
        "    #self.theta2 = np.random.rand(self.hiddenSize, self.outputSize) * 2 * epsilon - epsilon\n",
        "    \n",
        "  #history\n",
        "    self.loss_history =  [] \n",
        "\n",
        "  def forward(self, X):\n",
        "    #forward propagation through our network\n",
        "    self.z = np.dot(X, self.theta1) # dot product of X (input) and first set of 3x2 weights\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = np.dot(self.z2, self.theta2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "    o = self.sigmoid(self.z3) # final activation function\n",
        "    return o\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    # activation function\n",
        "    return 1/(1+np.exp(-s))\n",
        "\n",
        "  def sigmoidPrime(self, s):\n",
        "    #derivative of sigmoid\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    # backward propagate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.theta2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.theta1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.theta2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "    \n",
        "  def cost(self, X, y ):\n",
        "    m = len(y)\n",
        "    y_output = self.forward(X)\n",
        "    \n",
        "    c1 = np.multiply(y, np.log(y_output))\n",
        "    c2 = np.multiply(1-y, np.log(1-y_output))\n",
        "    c = np.sum(c1 + c2)\n",
        "    \n",
        "    r1 = np.sum(np.sum(np.power(self.theta1,2), axis = 1))\n",
        "    r2 = np.sum(np.sum(np.power(self.theta2,2), axis = 1))\n",
        "    \n",
        "    return np.sum(c / (-m)) + (r1 + r2) * self.lmbda / (2*m)\n",
        "\n",
        "  \n",
        "  def loss(self, X, y):\n",
        "    return np.mean(np.square(y - self.forward(X)))\n",
        "\n",
        "  def train(self, X, y, epoch):\n",
        "    for i in range(epoch):\n",
        "      o = self.forward(X)\n",
        "      self.backward(X, y, o)\n",
        "      self.loss_history.append(self.loss(X,y))\n",
        "      print(\"epoch:[\", i, \"], loss: \", str(self.loss(X,y))  )\n",
        "\n",
        "  def predict(self, X):\n",
        "    return self.forward(X)\n",
        "  \n",
        "  \n",
        "  def get_loss_histroy(self):\n",
        "    return self.loss_history\n",
        "\n",
        "  def get_weight(self):\n",
        "    return self.theta1, self.theta2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1MjhrdvPyAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot the convergence of the cost function\n",
        "def plotConvergence(cost_history, iterations):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(len(cost_history)),cost_history,'bo')\n",
        "    plt.grid(True)\n",
        "    plt.title(\"Convergence of Cost Function\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
        "    dummy = plt.ylim([min(cost_history)-0.2*(max(cost_history)-min(cost_history)), max(cost_history)+0.2*(max(cost_history)-min(cost_history))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4suFNvfWj2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        },
        "outputId": "b25f5fb7-9f7b-4ccc-975f-4e349299a8fa"
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network( 13, 6, 1, 1 )\n",
        "\n",
        "iterations = 500\n",
        "NN.train(X_train, y_train, iterations)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:[ 0 ], loss:  0.19393804705517334\n",
            "epoch:[ 1 ], loss:  0.17040409381314822\n",
            "epoch:[ 2 ], loss:  0.16263781019539028\n",
            "epoch:[ 3 ], loss:  0.16054973583407803\n",
            "epoch:[ 4 ], loss:  0.15895774825241096\n",
            "epoch:[ 5 ], loss:  0.15767415161312726\n",
            "epoch:[ 6 ], loss:  0.15712740250799975\n",
            "epoch:[ 7 ], loss:  0.15601389061756696\n",
            "epoch:[ 8 ], loss:  0.15560548376923425\n",
            "epoch:[ 9 ], loss:  0.1570396919000555\n",
            "epoch:[ 10 ], loss:  0.15584396339518808\n",
            "epoch:[ 11 ], loss:  0.15588230140866638\n",
            "epoch:[ 12 ], loss:  0.15639366855634984\n",
            "epoch:[ 13 ], loss:  0.15562957789551324\n",
            "epoch:[ 14 ], loss:  0.15510141760899343\n",
            "epoch:[ 15 ], loss:  0.154957766868112\n",
            "epoch:[ 16 ], loss:  0.15469267948250642\n",
            "epoch:[ 17 ], loss:  0.1537166286298873\n",
            "epoch:[ 18 ], loss:  0.15316697224569845\n",
            "epoch:[ 19 ], loss:  0.15254090780268265\n",
            "epoch:[ 20 ], loss:  0.15338778699394634\n",
            "epoch:[ 21 ], loss:  0.1538249280615641\n",
            "epoch:[ 22 ], loss:  0.15367780320574365\n",
            "epoch:[ 23 ], loss:  0.15265696358435912\n",
            "epoch:[ 24 ], loss:  0.15189454819724085\n",
            "epoch:[ 25 ], loss:  0.15164907308262668\n",
            "epoch:[ 26 ], loss:  0.15216395133018526\n",
            "epoch:[ 27 ], loss:  0.15172234809850169\n",
            "epoch:[ 28 ], loss:  0.15055915058953379\n",
            "epoch:[ 29 ], loss:  0.15059955366573552\n",
            "epoch:[ 30 ], loss:  0.1506223243837501\n",
            "epoch:[ 31 ], loss:  0.15086487456686848\n",
            "epoch:[ 32 ], loss:  0.15048961744895228\n",
            "epoch:[ 33 ], loss:  0.14963263032516216\n",
            "epoch:[ 34 ], loss:  0.14932196889273597\n",
            "epoch:[ 35 ], loss:  0.14897085606329608\n",
            "epoch:[ 36 ], loss:  0.15017042935437389\n",
            "epoch:[ 37 ], loss:  0.15125949172294748\n",
            "epoch:[ 38 ], loss:  0.15115531469719695\n",
            "epoch:[ 39 ], loss:  0.15010123781526277\n",
            "epoch:[ 40 ], loss:  0.14931656790415365\n",
            "epoch:[ 41 ], loss:  0.14834183757000663\n",
            "epoch:[ 42 ], loss:  0.14878544294589707\n",
            "epoch:[ 43 ], loss:  0.14942164703908853\n",
            "epoch:[ 44 ], loss:  0.1480138227071577\n",
            "epoch:[ 45 ], loss:  0.14898325495430254\n",
            "epoch:[ 46 ], loss:  0.1482364330673709\n",
            "epoch:[ 47 ], loss:  0.14787384594996814\n",
            "epoch:[ 48 ], loss:  0.1475605072956442\n",
            "epoch:[ 49 ], loss:  0.1477418810263997\n",
            "epoch:[ 50 ], loss:  0.14770796600739508\n",
            "epoch:[ 51 ], loss:  0.14699815332894628\n",
            "epoch:[ 52 ], loss:  0.14674207617377308\n",
            "epoch:[ 53 ], loss:  0.14711626184742213\n",
            "epoch:[ 54 ], loss:  0.14822215377845138\n",
            "epoch:[ 55 ], loss:  0.14703334245777222\n",
            "epoch:[ 56 ], loss:  0.14630775152971404\n",
            "epoch:[ 57 ], loss:  0.14642127792096737\n",
            "epoch:[ 58 ], loss:  0.14555929614935886\n",
            "epoch:[ 59 ], loss:  0.1454426195096344\n",
            "epoch:[ 60 ], loss:  0.14534090600454078\n",
            "epoch:[ 61 ], loss:  0.1458449629702545\n",
            "epoch:[ 62 ], loss:  0.1461443305918307\n",
            "epoch:[ 63 ], loss:  0.14660093120424103\n",
            "epoch:[ 64 ], loss:  0.14724822949761354\n",
            "epoch:[ 65 ], loss:  0.1459400515530736\n",
            "epoch:[ 66 ], loss:  0.1451725933578063\n",
            "epoch:[ 67 ], loss:  0.14514504331091646\n",
            "epoch:[ 68 ], loss:  0.14483524201605383\n",
            "epoch:[ 69 ], loss:  0.14506689871863407\n",
            "epoch:[ 70 ], loss:  0.14816755289960384\n",
            "epoch:[ 71 ], loss:  0.1454096668025594\n",
            "epoch:[ 72 ], loss:  0.1446546348578061\n",
            "epoch:[ 73 ], loss:  0.14492910578234328\n",
            "epoch:[ 74 ], loss:  0.14515464746937773\n",
            "epoch:[ 75 ], loss:  0.14444486754608332\n",
            "epoch:[ 76 ], loss:  0.14471975205643117\n",
            "epoch:[ 77 ], loss:  0.1448173045623516\n",
            "epoch:[ 78 ], loss:  0.14428875104835512\n",
            "epoch:[ 79 ], loss:  0.14398463348877089\n",
            "epoch:[ 80 ], loss:  0.1437840590144834\n",
            "epoch:[ 81 ], loss:  0.1450403910803603\n",
            "epoch:[ 82 ], loss:  0.14540948879038193\n",
            "epoch:[ 83 ], loss:  0.14574737682391867\n",
            "epoch:[ 84 ], loss:  0.14584549369028815\n",
            "epoch:[ 85 ], loss:  0.146884587480915\n",
            "epoch:[ 86 ], loss:  0.14575480874996843\n",
            "epoch:[ 87 ], loss:  0.1437623892454883\n",
            "epoch:[ 88 ], loss:  0.1436711042585043\n",
            "epoch:[ 89 ], loss:  0.14322660461821157\n",
            "epoch:[ 90 ], loss:  0.14354016204246295\n",
            "epoch:[ 91 ], loss:  0.1439209348771398\n",
            "epoch:[ 92 ], loss:  0.14404671858312842\n",
            "epoch:[ 93 ], loss:  0.1456738418952352\n",
            "epoch:[ 94 ], loss:  0.1448221257378273\n",
            "epoch:[ 95 ], loss:  0.14623399324201017\n",
            "epoch:[ 96 ], loss:  0.14463701862972586\n",
            "epoch:[ 97 ], loss:  0.14384642740251458\n",
            "epoch:[ 98 ], loss:  0.14348523257322426\n",
            "epoch:[ 99 ], loss:  0.14347612100904375\n",
            "epoch:[ 100 ], loss:  0.14387476397291402\n",
            "epoch:[ 101 ], loss:  0.1442940602786822\n",
            "epoch:[ 102 ], loss:  0.1449080675099117\n",
            "epoch:[ 103 ], loss:  0.1453571339156843\n",
            "epoch:[ 104 ], loss:  0.14417032051242115\n",
            "epoch:[ 105 ], loss:  0.14397342229166987\n",
            "epoch:[ 106 ], loss:  0.14358024293265756\n",
            "epoch:[ 107 ], loss:  0.14354201613089565\n",
            "epoch:[ 108 ], loss:  0.14375818698490986\n",
            "epoch:[ 109 ], loss:  0.14424308528084726\n",
            "epoch:[ 110 ], loss:  0.14424824366270506\n",
            "epoch:[ 111 ], loss:  0.14486977064087891\n",
            "epoch:[ 112 ], loss:  0.1437688852843628\n",
            "epoch:[ 113 ], loss:  0.14347284987069314\n",
            "epoch:[ 114 ], loss:  0.14313461882720424\n",
            "epoch:[ 115 ], loss:  0.14304862338731195\n",
            "epoch:[ 116 ], loss:  0.1428303558318458\n",
            "epoch:[ 117 ], loss:  0.14315256057942874\n",
            "epoch:[ 118 ], loss:  0.1432927590200477\n",
            "epoch:[ 119 ], loss:  0.1422009122628694\n",
            "epoch:[ 120 ], loss:  0.1418209392773378\n",
            "epoch:[ 121 ], loss:  0.1419124892059427\n",
            "epoch:[ 122 ], loss:  0.14335957636447774\n",
            "epoch:[ 123 ], loss:  0.1438704298287728\n",
            "epoch:[ 124 ], loss:  0.14377003391851645\n",
            "epoch:[ 125 ], loss:  0.14288691846965862\n",
            "epoch:[ 126 ], loss:  0.1424985416462374\n",
            "epoch:[ 127 ], loss:  0.14238920463450075\n",
            "epoch:[ 128 ], loss:  0.14370301402943605\n",
            "epoch:[ 129 ], loss:  0.14250788704122164\n",
            "epoch:[ 130 ], loss:  0.14357155309105865\n",
            "epoch:[ 131 ], loss:  0.14253858467540118\n",
            "epoch:[ 132 ], loss:  0.14398069480800482\n",
            "epoch:[ 133 ], loss:  0.14354011984885257\n",
            "epoch:[ 134 ], loss:  0.14253391200898693\n",
            "epoch:[ 135 ], loss:  0.14324940453810356\n",
            "epoch:[ 136 ], loss:  0.14275071717839907\n",
            "epoch:[ 137 ], loss:  0.14191145911111108\n",
            "epoch:[ 138 ], loss:  0.14260508567781893\n",
            "epoch:[ 139 ], loss:  0.1431727467809468\n",
            "epoch:[ 140 ], loss:  0.14206475570738247\n",
            "epoch:[ 141 ], loss:  0.1421344079979247\n",
            "epoch:[ 142 ], loss:  0.1423109895284987\n",
            "epoch:[ 143 ], loss:  0.14166977529462957\n",
            "epoch:[ 144 ], loss:  0.14222525621098067\n",
            "epoch:[ 145 ], loss:  0.14314256341885487\n",
            "epoch:[ 146 ], loss:  0.14189768890632024\n",
            "epoch:[ 147 ], loss:  0.14253744517168393\n",
            "epoch:[ 148 ], loss:  0.1418460511583487\n",
            "epoch:[ 149 ], loss:  0.1419167414211157\n",
            "epoch:[ 150 ], loss:  0.14152668659090722\n",
            "epoch:[ 151 ], loss:  0.14244870143369967\n",
            "epoch:[ 152 ], loss:  0.14236535227052957\n",
            "epoch:[ 153 ], loss:  0.14133039551510831\n",
            "epoch:[ 154 ], loss:  0.14194416699911952\n",
            "epoch:[ 155 ], loss:  0.14291155308293138\n",
            "epoch:[ 156 ], loss:  0.14245530365295297\n",
            "epoch:[ 157 ], loss:  0.14143176816965025\n",
            "epoch:[ 158 ], loss:  0.14102288671177626\n",
            "epoch:[ 159 ], loss:  0.14119103659789273\n",
            "epoch:[ 160 ], loss:  0.14194852580041611\n",
            "epoch:[ 161 ], loss:  0.14195864592109683\n",
            "epoch:[ 162 ], loss:  0.14362570745961442\n",
            "epoch:[ 163 ], loss:  0.14184926572968215\n",
            "epoch:[ 164 ], loss:  0.1417074848700831\n",
            "epoch:[ 165 ], loss:  0.14084173989197546\n",
            "epoch:[ 166 ], loss:  0.14128118591399608\n",
            "epoch:[ 167 ], loss:  0.14160532641563414\n",
            "epoch:[ 168 ], loss:  0.141513864387201\n",
            "epoch:[ 169 ], loss:  0.14044705603600519\n",
            "epoch:[ 170 ], loss:  0.14132650651043327\n",
            "epoch:[ 171 ], loss:  0.1412073519037204\n",
            "epoch:[ 172 ], loss:  0.14099370262514588\n",
            "epoch:[ 173 ], loss:  0.14070600674583955\n",
            "epoch:[ 174 ], loss:  0.14110783340294064\n",
            "epoch:[ 175 ], loss:  0.1405988115924766\n",
            "epoch:[ 176 ], loss:  0.14097757485241771\n",
            "epoch:[ 177 ], loss:  0.14070489880073642\n",
            "epoch:[ 178 ], loss:  0.14138156189937656\n",
            "epoch:[ 179 ], loss:  0.14194116380353744\n",
            "epoch:[ 180 ], loss:  0.14136345485507565\n",
            "epoch:[ 181 ], loss:  0.14082229958993475\n",
            "epoch:[ 182 ], loss:  0.14096724665295735\n",
            "epoch:[ 183 ], loss:  0.14065579803064968\n",
            "epoch:[ 184 ], loss:  0.14227952295405605\n",
            "epoch:[ 185 ], loss:  0.14105173116449118\n",
            "epoch:[ 186 ], loss:  0.14147730729476482\n",
            "epoch:[ 187 ], loss:  0.14087391425738888\n",
            "epoch:[ 188 ], loss:  0.14041644358538596\n",
            "epoch:[ 189 ], loss:  0.14056785486807588\n",
            "epoch:[ 190 ], loss:  0.14232400956015537\n",
            "epoch:[ 191 ], loss:  0.14030036413329408\n",
            "epoch:[ 192 ], loss:  0.14157663132173962\n",
            "epoch:[ 193 ], loss:  0.14131818288504303\n",
            "epoch:[ 194 ], loss:  0.14037817092863106\n",
            "epoch:[ 195 ], loss:  0.1403990031090381\n",
            "epoch:[ 196 ], loss:  0.1422425917539146\n",
            "epoch:[ 197 ], loss:  0.14010122328073682\n",
            "epoch:[ 198 ], loss:  0.1408831261517597\n",
            "epoch:[ 199 ], loss:  0.14053967484784682\n",
            "epoch:[ 200 ], loss:  0.14143770459424823\n",
            "epoch:[ 201 ], loss:  0.14037725280367322\n",
            "epoch:[ 202 ], loss:  0.14028079603204907\n",
            "epoch:[ 203 ], loss:  0.14069214543303496\n",
            "epoch:[ 204 ], loss:  0.14116079280323188\n",
            "epoch:[ 205 ], loss:  0.14136519597301267\n",
            "epoch:[ 206 ], loss:  0.14166328009059367\n",
            "epoch:[ 207 ], loss:  0.1410590568354853\n",
            "epoch:[ 208 ], loss:  0.14063656550345183\n",
            "epoch:[ 209 ], loss:  0.14042004095595775\n",
            "epoch:[ 210 ], loss:  0.14112042063683952\n",
            "epoch:[ 211 ], loss:  0.1409986246855163\n",
            "epoch:[ 212 ], loss:  0.14073463506714937\n",
            "epoch:[ 213 ], loss:  0.14073535210270935\n",
            "epoch:[ 214 ], loss:  0.14201738983748805\n",
            "epoch:[ 215 ], loss:  0.1413908999405501\n",
            "epoch:[ 216 ], loss:  0.139954709692946\n",
            "epoch:[ 217 ], loss:  0.1412693754225019\n",
            "epoch:[ 218 ], loss:  0.14097277258444907\n",
            "epoch:[ 219 ], loss:  0.1404297979091335\n",
            "epoch:[ 220 ], loss:  0.1405612603599775\n",
            "epoch:[ 221 ], loss:  0.14139256635443256\n",
            "epoch:[ 222 ], loss:  0.14032406827998958\n",
            "epoch:[ 223 ], loss:  0.14008433476755058\n",
            "epoch:[ 224 ], loss:  0.14060018102336913\n",
            "epoch:[ 225 ], loss:  0.14059636048208649\n",
            "epoch:[ 226 ], loss:  0.13974318219785664\n",
            "epoch:[ 227 ], loss:  0.13977989794564058\n",
            "epoch:[ 228 ], loss:  0.14055757444512967\n",
            "epoch:[ 229 ], loss:  0.1416077359801035\n",
            "epoch:[ 230 ], loss:  0.14081390711230074\n",
            "epoch:[ 231 ], loss:  0.14063130853447\n",
            "epoch:[ 232 ], loss:  0.13990141021128782\n",
            "epoch:[ 233 ], loss:  0.1406408055424094\n",
            "epoch:[ 234 ], loss:  0.14061467014178278\n",
            "epoch:[ 235 ], loss:  0.1414834374752906\n",
            "epoch:[ 236 ], loss:  0.1409131275859689\n",
            "epoch:[ 237 ], loss:  0.1407364050341102\n",
            "epoch:[ 238 ], loss:  0.14038943747584756\n",
            "epoch:[ 239 ], loss:  0.14030888041806186\n",
            "epoch:[ 240 ], loss:  0.13993183417748692\n",
            "epoch:[ 241 ], loss:  0.13991104487610753\n",
            "epoch:[ 242 ], loss:  0.1398267256371654\n",
            "epoch:[ 243 ], loss:  0.1396227100955464\n",
            "epoch:[ 244 ], loss:  0.14041901900051235\n",
            "epoch:[ 245 ], loss:  0.1422283896934075\n",
            "epoch:[ 246 ], loss:  0.14071559788007174\n",
            "epoch:[ 247 ], loss:  0.13993436136175041\n",
            "epoch:[ 248 ], loss:  0.14005736933264643\n",
            "epoch:[ 249 ], loss:  0.1396717621390028\n",
            "epoch:[ 250 ], loss:  0.13975227989932495\n",
            "epoch:[ 251 ], loss:  0.14008050583420512\n",
            "epoch:[ 252 ], loss:  0.1406389608222124\n",
            "epoch:[ 253 ], loss:  0.139825045065143\n",
            "epoch:[ 254 ], loss:  0.1405660858330836\n",
            "epoch:[ 255 ], loss:  0.14022889188510013\n",
            "epoch:[ 256 ], loss:  0.14022062855304\n",
            "epoch:[ 257 ], loss:  0.14017404795883026\n",
            "epoch:[ 258 ], loss:  0.1400293496081216\n",
            "epoch:[ 259 ], loss:  0.14042332707330074\n",
            "epoch:[ 260 ], loss:  0.140108528119762\n",
            "epoch:[ 261 ], loss:  0.1407094463150117\n",
            "epoch:[ 262 ], loss:  0.1400908836023323\n",
            "epoch:[ 263 ], loss:  0.14120540166537268\n",
            "epoch:[ 264 ], loss:  0.14107924759569823\n",
            "epoch:[ 265 ], loss:  0.13970197279805105\n",
            "epoch:[ 266 ], loss:  0.13983931351386628\n",
            "epoch:[ 267 ], loss:  0.13984231201866876\n",
            "epoch:[ 268 ], loss:  0.14148969871108583\n",
            "epoch:[ 269 ], loss:  0.14079709396979623\n",
            "epoch:[ 270 ], loss:  0.13993710674563886\n",
            "epoch:[ 271 ], loss:  0.14006595378677691\n",
            "epoch:[ 272 ], loss:  0.13988703155076254\n",
            "epoch:[ 273 ], loss:  0.14039770323892956\n",
            "epoch:[ 274 ], loss:  0.14005828385677302\n",
            "epoch:[ 275 ], loss:  0.14134579729995903\n",
            "epoch:[ 276 ], loss:  0.13999896928417746\n",
            "epoch:[ 277 ], loss:  0.14016051924098458\n",
            "epoch:[ 278 ], loss:  0.1406399565603914\n",
            "epoch:[ 279 ], loss:  0.14081043404927482\n",
            "epoch:[ 280 ], loss:  0.13996441179123692\n",
            "epoch:[ 281 ], loss:  0.13984423950391772\n",
            "epoch:[ 282 ], loss:  0.1412922455674735\n",
            "epoch:[ 283 ], loss:  0.13994283706897517\n",
            "epoch:[ 284 ], loss:  0.13988193731627324\n",
            "epoch:[ 285 ], loss:  0.13995776109486893\n",
            "epoch:[ 286 ], loss:  0.13961791871322837\n",
            "epoch:[ 287 ], loss:  0.14030843253089495\n",
            "epoch:[ 288 ], loss:  0.14028117899222514\n",
            "epoch:[ 289 ], loss:  0.1403726085310177\n",
            "epoch:[ 290 ], loss:  0.14084848052578025\n",
            "epoch:[ 291 ], loss:  0.13995610226435246\n",
            "epoch:[ 292 ], loss:  0.14049897576563442\n",
            "epoch:[ 293 ], loss:  0.14010980695003183\n",
            "epoch:[ 294 ], loss:  0.14018162577663687\n",
            "epoch:[ 295 ], loss:  0.14068360810728664\n",
            "epoch:[ 296 ], loss:  0.13956235493796026\n",
            "epoch:[ 297 ], loss:  0.1395549156900565\n",
            "epoch:[ 298 ], loss:  0.14014428555425507\n",
            "epoch:[ 299 ], loss:  0.1397889696044961\n",
            "epoch:[ 300 ], loss:  0.1400601265624143\n",
            "epoch:[ 301 ], loss:  0.14028209574761843\n",
            "epoch:[ 302 ], loss:  0.13950925919339294\n",
            "epoch:[ 303 ], loss:  0.13983259273364684\n",
            "epoch:[ 304 ], loss:  0.13982920985031574\n",
            "epoch:[ 305 ], loss:  0.14020700761677954\n",
            "epoch:[ 306 ], loss:  0.13992630427619035\n",
            "epoch:[ 307 ], loss:  0.13982235775063998\n",
            "epoch:[ 308 ], loss:  0.13953607963272796\n",
            "epoch:[ 309 ], loss:  0.13908262831683804\n",
            "epoch:[ 310 ], loss:  0.13995816060264768\n",
            "epoch:[ 311 ], loss:  0.13966371789463017\n",
            "epoch:[ 312 ], loss:  0.1410830461878849\n",
            "epoch:[ 313 ], loss:  0.14004891408795517\n",
            "epoch:[ 314 ], loss:  0.1392923926279107\n",
            "epoch:[ 315 ], loss:  0.13946250305174593\n",
            "epoch:[ 316 ], loss:  0.1405969045575042\n",
            "epoch:[ 317 ], loss:  0.1412536028676158\n",
            "epoch:[ 318 ], loss:  0.14082608412072734\n",
            "epoch:[ 319 ], loss:  0.1395706006553876\n",
            "epoch:[ 320 ], loss:  0.13978509387164934\n",
            "epoch:[ 321 ], loss:  0.13977317538816927\n",
            "epoch:[ 322 ], loss:  0.14014097824944985\n",
            "epoch:[ 323 ], loss:  0.13984322018057105\n",
            "epoch:[ 324 ], loss:  0.13980837781137728\n",
            "epoch:[ 325 ], loss:  0.13985828852342902\n",
            "epoch:[ 326 ], loss:  0.14084251007615495\n",
            "epoch:[ 327 ], loss:  0.1404549115002441\n",
            "epoch:[ 328 ], loss:  0.1395591836740116\n",
            "epoch:[ 329 ], loss:  0.1392150000750628\n",
            "epoch:[ 330 ], loss:  0.13946433201703531\n",
            "epoch:[ 331 ], loss:  0.14015725673013008\n",
            "epoch:[ 332 ], loss:  0.1403178583062367\n",
            "epoch:[ 333 ], loss:  0.13977977164264987\n",
            "epoch:[ 334 ], loss:  0.13951723613748937\n",
            "epoch:[ 335 ], loss:  0.1392776947184555\n",
            "epoch:[ 336 ], loss:  0.13974904599415425\n",
            "epoch:[ 337 ], loss:  0.13956438470909177\n",
            "epoch:[ 338 ], loss:  0.1396178121218739\n",
            "epoch:[ 339 ], loss:  0.13987757591589098\n",
            "epoch:[ 340 ], loss:  0.14025024318644866\n",
            "epoch:[ 341 ], loss:  0.14008401110599936\n",
            "epoch:[ 342 ], loss:  0.14011832846099426\n",
            "epoch:[ 343 ], loss:  0.1393613364196489\n",
            "epoch:[ 344 ], loss:  0.13938402717025397\n",
            "epoch:[ 345 ], loss:  0.13934150888434738\n",
            "epoch:[ 346 ], loss:  0.14000558545792233\n",
            "epoch:[ 347 ], loss:  0.13968548017960075\n",
            "epoch:[ 348 ], loss:  0.13981422679445002\n",
            "epoch:[ 349 ], loss:  0.13964593452233207\n",
            "epoch:[ 350 ], loss:  0.1397244814848237\n",
            "epoch:[ 351 ], loss:  0.13927360201447034\n",
            "epoch:[ 352 ], loss:  0.13973325289705152\n",
            "epoch:[ 353 ], loss:  0.1397695860578715\n",
            "epoch:[ 354 ], loss:  0.13989963743373965\n",
            "epoch:[ 355 ], loss:  0.1397297460224452\n",
            "epoch:[ 356 ], loss:  0.1400806238519981\n",
            "epoch:[ 357 ], loss:  0.14115840897300172\n",
            "epoch:[ 358 ], loss:  0.14023717231778757\n",
            "epoch:[ 359 ], loss:  0.13977498444641845\n",
            "epoch:[ 360 ], loss:  0.1398122723643613\n",
            "epoch:[ 361 ], loss:  0.13969355154578053\n",
            "epoch:[ 362 ], loss:  0.1398621739044035\n",
            "epoch:[ 363 ], loss:  0.14057157024022998\n",
            "epoch:[ 364 ], loss:  0.13997148381278235\n",
            "epoch:[ 365 ], loss:  0.13998053200254765\n",
            "epoch:[ 366 ], loss:  0.13939137071301252\n",
            "epoch:[ 367 ], loss:  0.13932441137860477\n",
            "epoch:[ 368 ], loss:  0.1390606769358773\n",
            "epoch:[ 369 ], loss:  0.14008017983314744\n",
            "epoch:[ 370 ], loss:  0.1405763151049317\n",
            "epoch:[ 371 ], loss:  0.14083944630523018\n",
            "epoch:[ 372 ], loss:  0.1399332131554838\n",
            "epoch:[ 373 ], loss:  0.1400123914326092\n",
            "epoch:[ 374 ], loss:  0.13908077254600848\n",
            "epoch:[ 375 ], loss:  0.1392575926073779\n",
            "epoch:[ 376 ], loss:  0.13993869681031618\n",
            "epoch:[ 377 ], loss:  0.13984212064007642\n",
            "epoch:[ 378 ], loss:  0.1399613830925415\n",
            "epoch:[ 379 ], loss:  0.14037091244410807\n",
            "epoch:[ 380 ], loss:  0.14063642037958443\n",
            "epoch:[ 381 ], loss:  0.13953930062380004\n",
            "epoch:[ 382 ], loss:  0.1391746409415054\n",
            "epoch:[ 383 ], loss:  0.1392922144554222\n",
            "epoch:[ 384 ], loss:  0.13899128534848404\n",
            "epoch:[ 385 ], loss:  0.13898797516896633\n",
            "epoch:[ 386 ], loss:  0.13923632451550486\n",
            "epoch:[ 387 ], loss:  0.13940286208302413\n",
            "epoch:[ 388 ], loss:  0.14002716763889253\n",
            "epoch:[ 389 ], loss:  0.13992101693238862\n",
            "epoch:[ 390 ], loss:  0.14054619268182314\n",
            "epoch:[ 391 ], loss:  0.14020108376106755\n",
            "epoch:[ 392 ], loss:  0.13928131961613674\n",
            "epoch:[ 393 ], loss:  0.13980621699056678\n",
            "epoch:[ 394 ], loss:  0.1405551102291605\n",
            "epoch:[ 395 ], loss:  0.14023843516686277\n",
            "epoch:[ 396 ], loss:  0.13951105803404124\n",
            "epoch:[ 397 ], loss:  0.1397141174185948\n",
            "epoch:[ 398 ], loss:  0.13936943698218437\n",
            "epoch:[ 399 ], loss:  0.1403278870766272\n",
            "epoch:[ 400 ], loss:  0.14048573289535563\n",
            "epoch:[ 401 ], loss:  0.1397089467761582\n",
            "epoch:[ 402 ], loss:  0.13995818199842092\n",
            "epoch:[ 403 ], loss:  0.1399766292177893\n",
            "epoch:[ 404 ], loss:  0.14060096707353079\n",
            "epoch:[ 405 ], loss:  0.13907338236030595\n",
            "epoch:[ 406 ], loss:  0.13931092402766007\n",
            "epoch:[ 407 ], loss:  0.13964699430118124\n",
            "epoch:[ 408 ], loss:  0.13978032221444528\n",
            "epoch:[ 409 ], loss:  0.1390631244391734\n",
            "epoch:[ 410 ], loss:  0.13946412940924421\n",
            "epoch:[ 411 ], loss:  0.14024155632767343\n",
            "epoch:[ 412 ], loss:  0.14014103555388965\n",
            "epoch:[ 413 ], loss:  0.13996135207681118\n",
            "epoch:[ 414 ], loss:  0.13916854967963752\n",
            "epoch:[ 415 ], loss:  0.13913537053834127\n",
            "epoch:[ 416 ], loss:  0.13919197361212082\n",
            "epoch:[ 417 ], loss:  0.1405065505463085\n",
            "epoch:[ 418 ], loss:  0.1409882725306902\n",
            "epoch:[ 419 ], loss:  0.14010805785146563\n",
            "epoch:[ 420 ], loss:  0.13969392059587513\n",
            "epoch:[ 421 ], loss:  0.13911546542775272\n",
            "epoch:[ 422 ], loss:  0.1389874048523203\n",
            "epoch:[ 423 ], loss:  0.1394841257614581\n",
            "epoch:[ 424 ], loss:  0.14000400661727178\n",
            "epoch:[ 425 ], loss:  0.14025038834013412\n",
            "epoch:[ 426 ], loss:  0.1398022914040367\n",
            "epoch:[ 427 ], loss:  0.14049338771208955\n",
            "epoch:[ 428 ], loss:  0.14060535461701212\n",
            "epoch:[ 429 ], loss:  0.13957022146188894\n",
            "epoch:[ 430 ], loss:  0.13923419738751186\n",
            "epoch:[ 431 ], loss:  0.13907811200644477\n",
            "epoch:[ 432 ], loss:  0.13910862526147652\n",
            "epoch:[ 433 ], loss:  0.1395512863256638\n",
            "epoch:[ 434 ], loss:  0.13998648934308114\n",
            "epoch:[ 435 ], loss:  0.13936702141344515\n",
            "epoch:[ 436 ], loss:  0.13951662697188152\n",
            "epoch:[ 437 ], loss:  0.1392432285613349\n",
            "epoch:[ 438 ], loss:  0.1401284790982549\n",
            "epoch:[ 439 ], loss:  0.1396753850198611\n",
            "epoch:[ 440 ], loss:  0.1398244070670099\n",
            "epoch:[ 441 ], loss:  0.13925543666815504\n",
            "epoch:[ 442 ], loss:  0.1395913330544489\n",
            "epoch:[ 443 ], loss:  0.13951568623586824\n",
            "epoch:[ 444 ], loss:  0.13985039364721047\n",
            "epoch:[ 445 ], loss:  0.14037623342662015\n",
            "epoch:[ 446 ], loss:  0.14086588684037687\n",
            "epoch:[ 447 ], loss:  0.13949277062397353\n",
            "epoch:[ 448 ], loss:  0.13941438809056986\n",
            "epoch:[ 449 ], loss:  0.13934785167379765\n",
            "epoch:[ 450 ], loss:  0.1400042416046341\n",
            "epoch:[ 451 ], loss:  0.14136021848381877\n",
            "epoch:[ 452 ], loss:  0.1400104730531914\n",
            "epoch:[ 453 ], loss:  0.13955795249899608\n",
            "epoch:[ 454 ], loss:  0.1402270853361885\n",
            "epoch:[ 455 ], loss:  0.1395207901146506\n",
            "epoch:[ 456 ], loss:  0.13992166255847735\n",
            "epoch:[ 457 ], loss:  0.1398298635875243\n",
            "epoch:[ 458 ], loss:  0.1399180201568301\n",
            "epoch:[ 459 ], loss:  0.14091373362526063\n",
            "epoch:[ 460 ], loss:  0.13960399318697583\n",
            "epoch:[ 461 ], loss:  0.13950129729348726\n",
            "epoch:[ 462 ], loss:  0.13965068082460963\n",
            "epoch:[ 463 ], loss:  0.13936324161944583\n",
            "epoch:[ 464 ], loss:  0.13933530515394704\n",
            "epoch:[ 465 ], loss:  0.13973517727880894\n",
            "epoch:[ 466 ], loss:  0.13933522454004824\n",
            "epoch:[ 467 ], loss:  0.13962834553031803\n",
            "epoch:[ 468 ], loss:  0.1388771876688986\n",
            "epoch:[ 469 ], loss:  0.1390377335625415\n",
            "epoch:[ 470 ], loss:  0.13941462466520396\n",
            "epoch:[ 471 ], loss:  0.13936097762706492\n",
            "epoch:[ 472 ], loss:  0.14134169905993862\n",
            "epoch:[ 473 ], loss:  0.1393150131182223\n",
            "epoch:[ 474 ], loss:  0.1392227481976079\n",
            "epoch:[ 475 ], loss:  0.1404136476605792\n",
            "epoch:[ 476 ], loss:  0.13924618836619984\n",
            "epoch:[ 477 ], loss:  0.13880800274053334\n",
            "epoch:[ 478 ], loss:  0.1397312680848544\n",
            "epoch:[ 479 ], loss:  0.14053836040032308\n",
            "epoch:[ 480 ], loss:  0.13991679187648232\n",
            "epoch:[ 481 ], loss:  0.13958762649169404\n",
            "epoch:[ 482 ], loss:  0.14020335147118784\n",
            "epoch:[ 483 ], loss:  0.1398446824263359\n",
            "epoch:[ 484 ], loss:  0.14007678956593844\n",
            "epoch:[ 485 ], loss:  0.1393647662535129\n",
            "epoch:[ 486 ], loss:  0.13938953088484668\n",
            "epoch:[ 487 ], loss:  0.1396264994589042\n",
            "epoch:[ 488 ], loss:  0.13911906351550313\n",
            "epoch:[ 489 ], loss:  0.13890859164678251\n",
            "epoch:[ 490 ], loss:  0.13935327267501468\n",
            "epoch:[ 491 ], loss:  0.13966626944687763\n",
            "epoch:[ 492 ], loss:  0.13976346652322474\n",
            "epoch:[ 493 ], loss:  0.13930604698514965\n",
            "epoch:[ 494 ], loss:  0.1389786971099444\n",
            "epoch:[ 495 ], loss:  0.1393723521713451\n",
            "epoch:[ 496 ], loss:  0.13885341546906446\n",
            "epoch:[ 497 ], loss:  0.13859142848896283\n",
            "epoch:[ 498 ], loss:  0.1391317996178818\n",
            "epoch:[ 499 ], loss:  0.13971554576604703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-8hMDyzQZqm",
        "colab_type": "code",
        "outputId": "41428de0-91ed-4228-cbf6-da9c668044d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "plotConvergence(NN.get_loss_histroy(),iterations )"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAGCCAYAAAChJrSAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XtclGX+//H3wAgewBaMQbNs/bqh\ngKfssOUhykNqZhuahiXt1pqamYe20mUVO2llbWnqtyjzW2tt0pqou+XPtoN2Is3V1UJNc11XURQU\nUQQ5zNy/P1gmkDkiA7fwej4ePh7OPXPfczEflDfXdd3XZTEMwxAAAAAaVFBDNwAAAACEMgAAAFMg\nlAEAAJgAoQwAAMAECGUAAAAmQCgDAAAwAWtDNwBAYBiGoTfffFPvv/++ysrKZLfb1bdvX/3ud79T\neHh4QzfvgvDxxx/r8ccf14ABA/TEE0/UeH716tV68803dfbsWZWVlalnz5567LHHFB0dXav32759\nu0JDQ9WlS5caz/Xv31+GYSg0NNR5zGq16m9/+1ut3suT9957T6NHj5YkDRkyRG+//bYuvvjiOn8f\nANVZWKcMaJyef/55bd68WYsXL1Z0dLSKioo0d+5c7d+/X++8844sFktDN9H0UlJSZLPZNG3atBrP\n/fnPf9abb76pV155RZ06dVJZWZleeeUV/fWvf9Xf/va3auHJV6mpqbrqqqv0q1/9qsZz/fv31/z5\n83X11VfX6mvxVW5uru6++2599NFHAX0fADUxfAk0QidPntTy5cv17LPPOnttWrZsqdTUVI0bN06G\nYaikpESpqakaPHiwhg4dqmeffVZ2u11SRQBYsWKF7rjjDvXt21fPPvusJOmOO+7Q+vXrne/z8ccf\nO3tUPv74Yw0fPlwDBgzQfffdpxMnTkiSFi1apFmzZumOO+7Qm2++qZKSEk2dOlX9+vXTfffdpxde\neEEzZ86UJOXk5GjixIkaPHiwBg8erI0bN0qSDh06pL59++pPf/qThg8frn79+unDDz+UVNEj+Mwz\nz6h///4aPHiwli5d6jy+ePFiDR48WDfddJOefvpp59dXlcPh0EsvvaQhQ4ZoyJAhmjlzpoqKivTW\nW29p/fr1WrFihWbNmlXjnCVLlig1NVWdOnWSJDVr1kxTpkzRjBkzZLFY3F5XktatW6dbb71VQ4cO\n1fDhw7Vp0ya9++67WrNmjZ5//nn93//9n1/1Tk5O1po1a1w+7ty5s1avXq3bb79dffv21Ztvvul8\n3WuvvaYBAwZo8ODBeuaZZ2QYhpKSknT48GENGTJEpaWl6ty5s3JyciRJf/rTn3TLLbdoyJAheuCB\nB5w1njlzpl5++WXde++9uummm3TvvfequLjYr68BgCQDQKOzYcMGY9CgQR5fk5aWZtx///1GWVmZ\nUVxcbIwcOdJYvXq1YRiGcdNNNxkPP/ywUV5ebuTk5Bjx8fHGkSNHjNdee8147LHHnNd47LHHjGXL\nlhn/+c9/jCuvvNL44YcfDMMwjFdffdV46KGHDMMwjJdfftno27evcfz4ccMwDGP58uVGUlKSUVZW\nZhw6dMi4/vrrjRkzZhiGYRj33HOP8dJLLxmGYRj//ve/jWuvvdY4ceKEcfDgQSMuLs5Yvny5YRiG\n8eGHHzq/vtWrVxtJSUlGaWmpcfr0aSMhIcHYvn27kZGRYQwbNsw4deqUUVZWZowfP955flV/+9vf\njNtvv904c+aMUV5ebjzwwAPGkiVLDMMwjBkzZjj/XtXevXuN+Ph4w+FwuP18PV33l7/8pXHo0CHD\nMAzj22+/NebNm2cYhmGMHTvWWYNz3XTTTca3337r8rlzz6v6OCYmxnj++ecNwzCM7du3G926dTPK\ny8uNb7/91hg0aJBx+vRpo6SkxBg5cqTx4YcfGt98840xcOBA57ViYmKMI0eOGNu2bTNuuOEGIy8v\nzzAMw3jyySeNlJQU5+c0dOhQIz8/3ygrKzNuu+02Y82aNW4/GwCu0VMGNEInT55UmzZtPL5mw4YN\nGj16tKxWq5o3b67hw4frq6++cj4/fPhwBQcHKzo6Wm3atNGRI0c0ZMgQbdy4UXa7XeXl5dqwYYOG\nDBmizz//XNdee61iYmIkSUlJSfr000+dPVM9evRQZGSkJGnLli0aPHiwrFar2rdvr4SEBElSUVGR\nNm3apN/85jeSpMsvv1xXXXWVs7esvLxcI0aMkCTFx8fr8OHDkqTPP/9cgwcPVrNmzRQWFqYPP/xQ\n3bp102effaaRI0cqPDxcVqtVo0aNcjkkt2HDBt1+++1q2bKlgoODNWLEiGqfg7vPNzIy0uMQsKfr\ntmnTRitWrFB2drauvvpq/f73v/f4fpUeffRRZ8/bkCFDdP/99/t0XuVwaHx8vEpKSnT8+HF9/vnn\nSkhIUFhYmEJCQrR8+XLdfPPNHr+ewYMHO7+vRo0aVe1zSkhI0M9+9jNZrVbFxMToyJEjPrUNwE+Y\n6A80QhERETp69KjH15w4cUIXXXSR8/FFF12k48ePOx+HhYU5/x4cHCy73a7LLrtM7dq107Zt21RW\nVqaOHTuqXbt2On36tLZs2aIhQ4ZUO//kyZPOa1c6deqUfvaznzkfR0dHKycnR6dPn3YOn1UqKirS\ndddd52xDy5YtJUlBQUFyOBySpPz8fLVu3dp5TuVrTp8+rTfeeEPp6emSJLvd7gyG/nwOrkREROj4\n8eMqLy+X1er6v1FP133llVf0yiuvaMSIEWrXrp1SUlJ07bXXenxPqWKeYG3mlFXe2BEcHCypYvg1\nPz9fNpvN+ZoWLVp4vMaJEyeqvb5169bVPqeqN49Ufr8A8A+hDGiEevbsqePHjysrK0vx8fHO42Vl\nZVq8eLEmTpyoiy++2BmapIreH1/usBs8eLA++eQTlZWVaejQoZIkm82m3r176+WXX/Z6flhYmM6c\nOeN8nJubK6mi9yg4OFjvv/++WrVqVe2cQ4cOub1eRESE8vPznY/z8vLUvHlz2Ww29e/fX2PHjvXY\nntp8Dh07dlRkZKQ+/fTTGr1Lixcv1l133eXxuh06dNAzzzwjh8Oh1atX63e/+52++OILj+/pSdWQ\nKkkFBQVezzn3c6v6d1dq+/0CwHcMXwKNUOvWrTVu3DjNmDFDBw4ckCQVFxcrNTVVO3fuVIsWLXTj\njTdq5cqVstvtKioq0po1a5xDiZ4MHjxYmZmZ+uyzz5w9Y3379tWWLVt08OBBSdKOHTv09NNPuzy/\nW7du+uijj+RwOHTkyBF9/vnnkiqWd0hISNCKFSuc7f3973/vdRisf//++uCDD1RaWqqioiLddddd\n2rNnjwYMGKA1a9Y4J5yvWLFCGRkZNc6/8cYbtXbtWhUXF6u8vFwrV670+jkEBQVp2rRpevrpp7Vj\nxw5JFYH3pZde0scff6ywsDC31z1x4oTuvfdeFRYWKigoSD169HAOg1qtVp0+fdrje7sSFRWl3bt3\nS5K2bdumf//7317P6d+/vz799FMVFBSovLxcDz74oL788ktZrVYVFRWpvLy8xuf097//3RneVqxY\n4dP3CwDf0VMGNFIPPfSQLrroIj3wwAOy2+0KCgrSgAED9Pjjj0uquEPv4MGDGjZsmCwWi4YMGeLs\n+fKkY8eOcjgcio6Odt7ZabPZ9NRTT+nBBx9UWVmZWrVqpZSUFJfnjxkzRt9++60GDhyomJgYDRs2\nzNmz8/jjj2vOnDn6y1/+Ikm67bbb1K5dO489Zbfccot++OEH3XzzzQoNDdUdd9yhXr16yTAM7d27\nV4mJiZIqeqfmzp1b4/whQ4bohx9+0IgRI2QYhn75y1/qnnvu8fo5jBw5UqGhoZo9e7bOnj0ri8Wi\na6+9Vm+99ZZCQkLcXjc0NFT9+vXTyJEjFRwcrGbNmjnbNXDgQD3//PM6ePCgz/PMJOnee+/Vww8/\n7Jzb16dPH6/n9OzZU7/97W91++23KyQkRP369dOtt96qM2fO6KKLLlKfPn2qhdju3btr/Pjxuvvu\nu+VwOBQbG+v8XgJQN1inDEC9MwzD2Tv03HPPyW63uw1xANBUMHwJoF598sknGjlypEpLS3XmzBlt\n3LhRPXv2bOhmAUCDY/gSQL268cYbtXHjRg0dOlRBQUG68cYbq921CQBNFcOXAAAAJsDwJQAAgAkQ\nygAAAEzggp9Tlpvr/5o+/oqIaKn8/KKAvw/8Q13Mh5qYE3UxH2piPvVVk6iocLfP0VPmA6s1uKGb\nABeoi/lQE3OiLuZDTczHDDUhlAEAAJgAoQwAAMAECGUAAAAmQCgDAAAwAUIZAACACRDKAAAATIBQ\nBgAAYAKEMgAAABMglAEAAJgAoQwAAMAECGUAAAAmQCgDAAAwAUIZAACACRDKAAAATIBQBgAAYALW\nQF583rx52r59uywWi1JSUtS9e3fnc998841efPFFBQUFqWPHjpo7d66CgoI8ngMAANBYBSyUbd68\nWQcOHFB6err27dunlJQUpaenO59PTU3Vn/70J7Vt21ZTpkzRF198oRYtWng8BwAAoLEK2PBlZmam\nBg4cKEnq1KmTCgoKVFhY6Hx+1apVatu2rSQpMjJS+fn5Xs8BAABorAIWyvLy8hQREeF8HBkZqdzc\nXOfjsLAwSdKxY8f01VdfKSEhwes5AAAAjVVA55RVZRhGjWPHjx/XxIkTNWfOnGphzNM554qIaCmr\nNbhO2uhJVFR4wN8D/qMu5kNNzIm6mA81MZ+GrknAQpnNZlNeXp7z8bFjxxQVFeV8XFhYqPvvv1/T\npk1T3759fTrHlfz8ojpueU1RUeHKzT0d8PeBf6iL+VATc6Iu5kNNzKe+auIp+AVs+LJPnz5av369\nJCkrK0s2m805ZClJzz77rH7961/rhhtu8PkcAACAxipgPWW9evVSfHy8kpKSZLFYNGfOHK1atUrh\n4eHq27evVq9erQMHDmjlypWSpFtvvVV33nlnjXMAAACaAovhy8QtE6uvrka6mc2HupgPNTEn6mI+\n1MR8GvXwJQAAAHxHKAMAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAAEyCUAQAAmAChDAAAwAQI\nZQAAACZAKAMAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAAEyCUAQAAmAChDAAAwAQIZQAAACZA\nKPMgI8OqhISWslqlhISWysiwNnSTAABAI0XKcCMjw6oJE1o4H+/aFfzfx8VKTCxvuIYBAIBGiZ4y\nNxYsCHF5fOFC18cBAADOB6HMjT17XH807o4DAACcDxKGGzExDr+OAwAAnA9CmRvTppW6PD51quvj\nAAAA54NQ5kZiYrnS0ooVF2eX1SrFxdmVlsYkfwAAEBjcfelBYmK5EhPLFRUVrtzcooZuDgAAaMTo\nKQMAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAAEyCUAQAAmAChDAAAwAQIZQAAACYQ0FA2b948\n3XnnnUpKStKOHTuqPVdSUqIZM2ZoxIgRzmMOh0OzZ89WUlKSkpOTtW/fvkA2DwAAwDQCFso2b96s\nAwcOKD09XXPnztXcuXOrPT9//nzFxsZWO/bJJ5/o9OnTWrFihebOnav58+cHqnkAAACmErBQlpmZ\nqYEDB0qSOnXqpIKCAhUWFjqfnz59uvP5Sv/+97/VvXt3SVKHDh10+PBh2e32QDURAADANAIWyvLy\n8hQREeF8HBkZqdzcXOfjsLCwGufExMToyy+/lN1u17/+9S8dPHhQ+fn5gWoiAACAadTbhuSGYXh9\nTUJCgrZu3aq7775bnTt31v/8z/94PS8ioqWs1uC6aqZbUVHhAX8P+I+6mA81MSfqYj7UxHwauiYB\nC2U2m015eXnOx8eOHVNUVJTX86ZPn+78+8CBA9WmTRuPr8/PL6p9I30UFRWu3NzTAX8f+Ie6mA81\nMSfqYj7UxHzqqyaegl/Ahi/79Omj9evXS5KysrJks9lcDllWtXv3bv3+97+XJH3++eeKi4tTUBCr\ndgAAgMYvYD1lvXr1Unx8vJKSkmSxWDRnzhytWrVK4eHhGjRokKZMmaKcnBzt379fycnJGj16tIYN\nGybDMHTHHXcoNDRUL7zwQqCaBwAAYCoWw5fJXiZWX12NdDObD3UxH2piTtTFfKiJ+TTq4UsAAAD4\njlAGAABgAoQyAAAAEyCUAQAAmAChDAAAwAQIZQAAACZAKAMAADABQhkAAIAJEMoAAABMgFAGAABg\nAoQyAAAAEyCUAQAAmAChDAAAwAQIZQAAACZAKAMAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAA\nEyCUAQAAmAChDAAAwAQIZQAAACZAKAMAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAAEyCUAQAA\nmAChDAAAwAQIZQAAACZAKAMAADABQhkAAIAJEMoAAABMwBrIi8+bN0/bt2+XxWJRSkqKunfv7nyu\npKREqamp2rt3r1atWiVJOnPmjGbMmKGCggKVlZXpwQcfVL9+/QLZRAAAAFMIWE/Z5s2bdeDAAaWn\np2vu3LmaO3dutefnz5+v2NjYascyMjLUsWNHLV++XAsXLqxxDgAAQGMVsFCWmZmpgQMHSpI6deqk\ngoICFRYWOp+fPn268/lKEREROnnypCTp1KlTioiICFTzAAAATCVgoSwvL69aqIqMjFRubq7zcVhY\nWI1zhg0bpsOHD2vQoEEaO3asZsyYEajmAQAAmEpA55RVZRiG19esWbNGl1xyid544w3t3r1bKSkp\nzvlm7kREtJTVGlxXzXQrKio84O8B/1EX86Em5kRdzIeamE9D1yRgocxmsykvL8/5+NixY4qKivJ4\nztatW9W3b19JUpcuXXTs2DHZ7XYFB7sPXfn5RXXTYA+iosKVm3s64O8D/1AX86Em5kRdzIeamE99\n1cRT8AvY8GWfPn20fv16SVJWVpZsNpvLIcuqLr/8cm3fvl2SlJ2drVatWnkMZAAAAI1FwHrKevXq\npfj4eCUlJclisWjOnDlatWqVwsPDNWjQIE2ZMkU5OTnav3+/kpOTNXr0aN15551KSUnR2LFjVV5e\nrscffzxQzQMAADAVi+HLZC8Tq6+uRrqZzYe6mA81MSfqYj7UxHwa9fAlAAAAfEcoAwAAMAFCGQAA\ngAkQygAAAEyAUAYAAGAChDIAAAATIJQBAACYAKEMAADABAhlAAAAJkAoAwAAMAFCGQAAgAkQygAA\nAEyAUAYAAGAChDIAAAATIJQBAACYAKEMAADABAhlAAAAJkAoAwAAMAFCGQAAgAkQygAAAEyAUAYA\nAGAChDIAAAATsHp7wd69e/WXv/xFBQUFMgzDeXz+/PkBbRgAAEBT4jWUTZs2TUOHDlVsbGx9tAcA\nAKBJ8hrKLr74Yk2ePLk+2gIAANBkeZ1TdsMNN+jLL79UaWmpHA6H8w8AAADqjteesldeeUWFhYXV\njlksFu3atStgjQIAAGhqvIayLVu21Ec7AAAAmjSvoezMmTN688039d1338lisejKK6/UPffco+bN\nm9dH+wAAAJoEr3PKZs+ercLCQiUlJWn06NHKzc3VrFmz6qNtAAAATYbXnrK8vDy9+OKLzsc33XST\nkpOTA9ooAACApsZrT1lxcbGKi4udj4uKilRSUhLQRgEAADQ1XnvK7rzzTg0dOlRdu3aVYRjauXOn\npk6dWh9tM4WMDKsWL5Z27gxTTIxD06aVKjGxvKGbBQAAGhmLUXXvJDeOHDmirKwsWSwWde3aVdHR\n0fXRNp/k5p4O2LUzMqyaMKFFjeNpacUEMxOIigoPaP3hP2piTtTFfKiJ+dRXTaKiwt0+57anbOPG\njUpISNDKlSurHf/iiy8kSXfccYfXN543b562b98ui8WilJQUde/e3flcSUmJUlNTtXfvXq1atUqS\n9Je//EVr1651vub777/Xtm3bvL5PoCxYEOLy+MKFIYQyAABQp9yGsh9++EEJCQn6xz/+4fJ5b6Fs\n8+bNOnDggNLT07Vv3z6lpKQoPT3d+fz8+fMVGxurvXv3Oo+NGjVKo0aNcp6/bt06v76YurZnj+sp\nd+6OAwAA1JbbUDZ+/HhJUt++fTVs2LBqz7377rteL5yZmamBAwdKkjp16qSCggIVFhYqLCxMkjR9\n+nSdPHmyWs9YVUuWLNELL7zg21cRIDExDu3aFezyOAAAQF1yG8p27dql77//XsuWLat292V5ebmW\nLFmiMWPGeLxwXl6e4uPjnY8jIyOVm5vrDGVhYWE6efKky3N37Nihdu3aKSoqyusXEBHRUlZrzeBU\nF1JTJVdf5uzZwR7HhFF/qIP5UBNzoi7mQ03Mp6Fr4jaUhYSE6Pjx4zp9+nS1IUyLxaLHHnvM7zfy\n4X4Cp5UrVyoxMdGn1+bnF/ndFl8NGCClpVm1ZEkL7dxpKCbGoalTSzVgQLlycwP2tvARE2XNh5qY\nE3UxH2piPqae6N+pUyd16tRJ1113nX7xi184e7jy8vJ08cUXe31Tm82mvLw85+Njx4751PMlSZs2\nbTLNrgGJieUaP17KzS30/mIAAIBa8jpjPSsrq1rP2MMPP6y3337b64X79Omj9evXO69hs9mcwc6T\no0ePqlWrVgoJcX3nIwAAQGPkdfHYtWvX6p133nE+XrZsmcaOHauxY8d6PK9Xr16Kj49XUlKSLBaL\n5syZo1WrVik8PFyDBg3SlClTlJOTo/379ys5OVmjR4/W8OHDlZubq8jIyPP/ygAAAC4gXkOZ3W6X\n1frTyywWi8/zwx555JFqj7t06eL8+8svv+zynK5du2rp0qU+XR8AAKCx8BrK+vfvr6SkJF111VVy\nOBz65ptvdPPNN9dH2wAAAJoMr6Fs0qRJuvbaa7Vjxw7nMGTPnj3ro20AAABNhk9L04eFhSkuLk5d\nunRRcXGxMjMzA90uAACAJsVrT9lDDz2k3bt3q23bts5jFotF119/fUAbBgAA0JR4DWXZ2dn6+9//\nXh9tAQAAaLK8Dl927NhRpaWl9dEWAACAJstrT1lQUJCGDRum7t27Kzj4pz0m58+fH9CGAQAANCVe\nQ1nv3r3Vu3fv+mgLAABAk+U1lF199dX10Q4AAIAmzWso+/Wvf+1cxb+srEz5+fn6xS9+odWrV9dH\n+wAAAJoEr6Hs008/rfZ47969WrlyZcAaBAAA0BT5tHhsVVdccYWysrIC0RYAAIAmy2tP2YIFC2Sx\nWJyPc3JydOrUqYA2CgAAoKnx2lNmtVoVHBzs/NO5c2e9/vrr9dE2AACAJsNtT9mTTz6p1NRUnThx\nQqmpqfXZJgAAgCbHbSj76quv9PDDD2vz5s0qLCys8TyLxwIAANQdt6Hs9ddf19atW7Vr1y42HwcA\nAAgwt6GsQ4cO6tChg3r16qUOHTrUZ5sAAACaHK8T/QlkAAAAgef3OmUAAACoe15D2QcffFDj2Lvv\nvhuQxgAAADRVbueU7dy5U1lZWVq2bJmKi4udx8vKyrRkyRKNGTOmXhoIAADQFLgNZaGhoTp+/LhO\nnz6tf/zjH87jFotFjz32WL00zixWrJCefLKl9uwJUkyMQ9OmlSoxsbyhmwUAABoRt6GsU6dO6tSp\nk6677jr17NnTedzhcCgoqOlMRcvIsGrCBEkKliTt2hWsCRNaSCommAEAgDrjNV3961//0jvvvCO7\n3a4xY8ZowIAB+vOf/1wfbTOFBQtCXB5fuND1cQAAgNrwGsrS09M1atQo/f3vf9cVV1yhTz75ROvW\nrauPtpnCnj2uPyJ3xwEAAGrDa7IIDQ1VSEiINm7cqKFDhzapoUtJiolx+HUcAACgNnxKWE888YS2\nbt2qa6+9Vtu2bVNpaWmg22Ua06a5/lqnTm06nwEAAAg8r6HshRde0OWXX65XX31VwcHBys7O1hNP\nPFEfbTOFxMRyvfuuFBdnl9VqKC7OrrQ0JvkDAIC65fbuy0o2m01du3bVhg0btHHjRvXo0UNdunSp\nj7aZRlKSNGBAUUM3AwAANGJee8oWLlyo+fPn69ixYzp69KiefvpppaWl1UfbAAAAmgyvPWWbNm3S\nihUrnBP8y8vLNXbsWE2oWLyrycjIsGrBghAWkAUAAAHhNZSdu1is1WqVxWIJaKPMZsUK/XfB2Aos\nIAsAAOqa11DWtWtXTZw4Ub1795Ykff311+rWrZtPF583b562b98ui8WilJQUde/e3flcSUmJUlNT\ntXfvXq1atcp5fO3atVq6dKmsVqumTJmiG2+80c8vqe7Nm+f6+MKFIYQyAABQJ7yGspSUFK1bt84Z\nrm677TYNHTrU64U3b96sAwcOKD09Xfv27VNKSorS09Odz8+fP1+xsbHau3ev81h+fr6WLFmi999/\nX0VFRVq0aJEpQtnOna6Ps4AsAACoKx5D2cGDB3XZZZdp2LBhGjZsmIqLi3X06FGfhi8zMzM1cOBA\nSRX7aBYUFKiwsFBhYWGSpOnTp+vkyZNau3ZttXOuv/56hYWFKSwsTE899dT5fG11Ji5O+u67msdZ\nQBYAANQVt6EsMzNTjz76qNatW6fw8HBJFSFt0qRJWrBggbp27erxwnl5eYqPj3c+joyMVG5urjOU\nhYWF6eTJk9XOOXTokM6ePauJEyfq1KlTeuihh3T99dd7fJ+IiJayWoM9f5XnKSVFGjOm5vHZs4MV\nFRUe0PeGZ3z+5kNNzIm6mA81MZ+GronbULZ48WItW7bMGcgkKSYmRq+88oqee+45LV261K83MgzD\np9edPHlSixcv1uHDh3XPPffos88+89gzl58f+PXDkpLCdepUsZ58MlTZ2RVtad/e0KlTJcrNZU5Z\nQ4mKCldu7umGbgaqoCbmRF3Mh5qYT33VxFPwczspyjAMxcTE1Dh+xRVXqKSkxOub2mw25eXlOR8f\nO3ZMUVFRHs9p06aNrrzySlmtVnXo0EGtWrXSiRMnvL5XfcnODpJkkWRRdnaQJkxooYwMr9PyAAAA\nvHIbyoqK3PdAnTvs6EqfPn20fv16SVJWVpZsNptz6NKdvn376ptvvpHD4VB+fr6KiooUERHh9b3q\nw4IFIS6PL1zo+jgAAIA/3HbzXHHFFXr33Xc15pzJVK+//rp69Ojh9cK9evVSfHy8kpKSZLFYNGfO\nHK1atUrh4eEaNGiQpkyZopz3VJeGAAAgAElEQVScHO3fv1/JyckaPXq0hg8frsGDB2v06NGSpFmz\nZlVbI60hubvTkjswAQBAXbAYbiZ75ebm6sEHH1RQUJC6du0qh8OhrVu3KiwsTGlpaWrVqlV9t9Wl\n+hr/jYuza9eumjcUxMXZtWED+2I2BOZkmA81MSfqYj7UxHzMMKfMbU9ZVFSU3nvvPWVmZmrv3r0K\nDg7W0KFDdc011wSkkWY3bVpptVX9K02dWtoArQEAAI2N11nq119/vddlKZqCipX7a96BCQAAUBeY\nEOUn7sAEAACBQCjzg7s7MJ98MrSeWwIAABobQpkf3N1pmZ0dpLZtw5SQ0JJeMwAAUCuEMj942uvS\n4bBo165ghjMBAECtEMr8MG2ab3daTpjQnF4zAADgF0KZHxITy9W+vfvesp/QawYAAPxDKPNTaqr3\nfT+rYhsmAADgC0KZnxITyzVunO8Lxu7ezUcMAAC8IzHUwldf1dxuyR2Hw8IQJgAA8IpQVgv+bkLO\nECYAAPCGUFYLnpbGcGXnziB6ywAAgEeEslrwdWmMn1g0YUILpaSw8j8AAHCNUFYLvi+NUd3SpSH0\nmAEAAJcIZbXkfmkMw+N5zC8DAACuEMpqKTGxXGlpxYqLs8tqNRQXZ1daWrFiYz33oPl7kwAAAGga\nGEs7D4mJ5UpMLK9xfMKEFm7PiY42lJDQUnv2BCkmxqFp00pdXgMAADQtdNvUscoetIgI1z1m2dlB\n2rUrWHb7T1sxXXllK+aaAQDQxBHKAiAxsVw//HDG55X/s7OD2CcTAIAmjlAWQP6s/C9xEwAAAE0Z\noSyA/J3Uz00AAAA0XaSAAPJ35f/oaM/LaQAAgMaLUBZA/q78n53NdkwAADRVhLIAqs3K/xMmNFdC\nQkvCGQAATQyhLMDcr/zvzk9LZRDMAABoOghlAVbbfTIl6ckn2cAcAICmglBWD/zvLavAHDMAAJoO\nQlk9cLdPZlCQ97stXa1dlpFhVc+erWSzhclmC2NHAAAAGgF+ktcTV/tkLljg0K5dnheYPXftsowM\na429NbOzLf89Vsw+mgAAXKDoKWtAviyZERwstWsX5rwjc8EC96v+syMAAAAXLkJZA0pMLPe6P2ZJ\niaXa5uW7d7svGTsCAABw4eKneAObN6/EOd/MYvE+x8wwLG6f83cHAQAAYB4BDWXz5s3TnXfeqaSk\nJO3YsaPacyUlJZoxY4ZGjBjhPLZp0yZdd911Sk5OVnJysp566qlANs80EhPLtWFDkbp0Ob9Q1bu3\nvY5aBAAA6lvAJvpv3rxZBw4cUHp6uvbt26eUlBSlp6c7n58/f75iY2O1d+/eaudde+21evnllwPV\nLFM73+HHpUtDdM01dib7AwBwAQpYT1lmZqYGDhwoSerUqZMKCgpUWFjofH769OnO51GhLoYfmewP\nAMCFKWA9ZXl5eYqPj3c+joyMVG5ursLCwiRJYWFhOnnyZI3zfvzxR02cOFEFBQWaPHmy+vTp4/F9\nIiJaymr1vKxEXYiKCg/4e6SmSmPGnN81du4MVrt24YqLk1JSpKSkummbWdVHXeAfamJO1MV8qIn5\nNHRN6m2dMsPwPon95z//uSZPnqyhQ4fq4MGDuueee/TRRx8pJMR9709+flFdNtOlqKhw5eaeDvj7\nDBggpaVZ9eSTocrOrjqh3/3kflfsdum77yoC3iOPOJSaWtIohzTrqy7wHTUxJ+piPtTEfOqrJp6C\nX8CGL202m/Ly8pyPjx07pqioKI/nREdH65ZbbpHFYlGHDh108cUX6+jRo4FqoiklJpZr27YzOnas\nUMeOFSr4PDsBs7OD2NwcAIALQMBCWZ8+fbR+/XpJUlZWlmw2m3Po0p21a9fqjTfekCTl5ubq+PHj\nio6ODlQTLwh1tcwFc80AADC3gHWf9OrVS/Hx8UpKSpLFYtGcOXO0atUqhYeHa9CgQZoyZYpycnK0\nf/9+JScna/To0erfv78eeeQRffLJJyorK9Pjjz/uceiyKZg2rbTGtkq1wcKyAACYm8XwZbKXidXX\n+G9Djv2npIRq6dLzC6ft2zu0bdsZ5+PKLZv27AlSTIxD06aV+jXv7HzPrwsNXRfURE3MibqYDzUx\nn0Y9pwx1Z968ErVv73oY02r1LVNnZwcpJSVU0k+bmu/aFVxtCydf552d7/kAAKAmQtkFIjW1xOVx\nhx9TzpYuDZHNFqYHH2zu8nlf55252xSdeWsAANQeoewCkZhY7twj02o1FBdnV1pasTp39vdGAIvK\ny10vseHrvDN3r2PeGgAAtcd40wUkMbHc5byturgRQHJ9p6eruWMxMQ7t2lVzrQ42RAcAoPbo2rjA\nVe1Bk87vno1zNzR3N3esTx/XG59PnVrq0/tkZFiVkNBS7dqFKSGhJXPRAAAQoaxRSEws14YNRUpL\nO3te11m6NKRaQHryyVCXr/v662CXQ6m+3H3JTQIAALjGT8JGpCIUFWvhworhxvBwQ/n5/uXuKVOa\nS6oId9nZ7ueOuRtK9cbTTQKNcSsoAAB8RU9ZI1PZa3b4cKF++OGM0tKK/7uchm9DmyUlFk2Y0EIz\nZ7ruJZPOb+6YvzcJMNQJAGgqCGWNXOVemrGx/gUpTz1s584984e7QOfuJgOGOgEATQWhrImYNs23\nSfi+OHfuma8yMqzKyXG9HIermwwqhlJrYj00AEBjRChrIhITyzVuXN0FM3+DUWWvl7seuKpBr/K1\nJSXnt54aAAAXEn66NSHz5pVUu2syNLT2S2js2RPk13wvd3dyVlUZ9NzdDFCp6lAnc84AAI0FoayJ\nqXojwMsv134JjfJyuZzv1bZtzXCUkhLq9k7Oqip7wLz1hFWuh7Zihes2EMwAABciQlkTVrnwbO0W\nnXU9tOhw/BSObLYwde7cSkuX+jbUWdkD5u5mAKvVqLYe2mOPub5O1aFVetIAABcKQlkTl5hY7ved\nmb6z+LVOWmUPmLubEsrLLXryyVBlZFiVkWHVwYOur1PZ08bdmwCACwmhDD7cmXl+2zd5um5QUM0d\nARITy/+7tlpN2dlBPq2jlpFh1YMPcvcmAODCQZcBnDsBzJwZ6rJnq317Q9nZrocrz0da2lmXq/hn\nZFi9vp+nHrjoaMPjJu3cvQkAMCN+OkFSRTCr3AHg3D0tU1NL6vz9KpfnOHe+V0pK6H8DVe1D4Gef\nef5dIzrafc9fRoZVPXu2ks0WJpstTFde2arGcCfz1AAAgWAxDCNQY1P1Ijf3dMDfIyoqvF7ex8wy\nMqxue9L8NW5cqa65xu6xNyvQXG2gXjkHzZX27R06fNii4OCKuW3nGjeuVPPm1X14PVdGhlULFlTs\nbRoT49C0aaWm2jOUfyvmRF3Mh5qYT33VJCoq3O1z/IoPn1RuQJ6RYXVueB4dbfi01MW5vv46WF99\nFRyAVvquct20qgHH3W4D0k+bs5e7yT9Ll4bommvsAQ1I54bGyhsXpJoBEwBw4aGnzAf8RuNeZUjb\nvTtIDodvQ45WqyHDkOz2up+n1pBCQw0lJ5fpq6+Ca/Rk1UUPV0JCS+3aVTPMxsXZtWFDUV19GeeF\nfyvmRF3Mh5qYjxl6yghlPuAfj2+q9qJJrof5pIoQYRhyGTAao3HjSl2u1eZuCNVdeGvXLsxlkLVa\nDR0+XBiYxvuJfyvmRF3Mh5qYjxlCGRP9UWeq7hawZIn73QKmTi31a4P0m266sIfmli9v5vL4uUtz\neFtXzd2iuu6OAwAuLIQyBETlbgEV640ZkgxdeqnD2Tvkywbpla9PTy92u26ZVDEJ39O1WrVq2M7g\nEjfz/3fuDFJKSqjzbs8JE1yvq1Y5/81dkK1cdDcQuNMUAOoPw5c+oJs5cK68spXLmwXat3do27Yz\nzsee7oysDHoZGVY9+WSoc42zSy81NHt2RSJqyDs9K0Lp+c2fq/o1Vg4Rx8Q4NHVq3dx96WrYVHL9\nuVVszSWXw6wX2r8Vs9/NWlcutLo0BdTEfMwwfEko8wH/eALHXdhyN9+qaujq0MGiP/zBtzsPU1JC\nfd6Dsy5CVF1r394hw5AOH65ol9UqORxS587+B4lzg0ifPnaXn0379g63gdnV8bS0Yo0f38KvfysN\nGYr8+d670PF/mPlQE/MhlNUBQtmFr7a9P/7WpS7XWjMbX9dJ89TjWJO7cOr6eFycXVlZwdVq4il0\nNXQouhDuZq0r/B9mPtTEfMwQyhrfTydccKreILBhQ1HAfiCfu2tB4Pb0rH9Ll4Z43IWgUuX8tEDY\nuTNIVqucc8+83biwYIHrnsvKNvoyn83da3w51912W3W1DRfz8QD4i1CGJqcyBMbGNra7Fi2SLM5N\n288NaBV7ivrzT97dEK7743b7T4vaTprkeUN4d+EnOztInTu38hjoJPd3q1Zu1eXpXKlu72Y9N4B5\na4OnwGa2MGe29pzL7O1r7ALx+TflmjJ86QO6mc3pfOvibSiv8o7PyjlszZpVrOgfEiKVlUnt2hmy\nWKRDhyrCkLlVtNUw/G+nxWLU6jxPbalydb/OrDq06G74MTTUUElJzev6evOIv8On/gwJx8XZNXVq\nqccbKAIxpOvrvxVf5xuaZd5dQw+Bn4+6+P+roW9S8eXz97edDVlThi+BBpSYWO5xqY3U1BJt23ZG\nx44V6tixQmVnF+ro0UIdPFionJxCbdt2Rlu3nlFamvs12czDUutg5fnXNkP+DwNbqvzxT9XeNXc9\nbe6WIMnOrliCpJKrZVs8fT+4kpFh1ZQprnsEXdm9O8jtsO2UKc3dLotSdU07V70IddGz4Krn0d3N\nMecOgzdUz4a7z/LcNQDduVB7ZLxNDQjk+1b9vNxNh6j8/GvTzvOt6YWOUIYmLTXV9U/wceN8/62z\n8od7XJxdVmvFD/ZLL3UoKOiC7oSuwlN4qt9ewvJyOYdlW7d29/m6b8/SpSE1fiBUDOlWH/qtGnSi\no8N02WVhatu2egjq2bNiiNVVr5w7DodFP/zgLky6/ywrA6i7H3J18QPa3Q9DV7Kzg6oNxbobRg40\nT/MCvQUuX4e5fQ1uroawPc13rJx/6e51nngLQ1XbVLkOorf5pt64qrO76RCVdalNwPI21/NCDdK+\nYvjSBwxfmlNd1SVQa3+5u7ak/y7tUfvfiaxWw+02VvDMajWcO05MmdLc7VDn+dTHk9oMB1dfEsW3\ncyuHejMyrFq8uIV27jScQ5Ku9md1t42Xt+u7G0aWKn65cfVelc53CM7de0dEOFzeZV11vT93Q85V\nh8jdLaVz7lCar0PYcXF27dzpfXu5qu30df1A6act1zIyrHriiVAdPuz6e7g2Q4Hu1pR0zVBsrEM/\n/OB6T2RPW8O5/34ydMklhsuvKSjIqNXyQOcyw/BlQEPZvHnztH37dlksFqWkpKh79+7O50pKSpSa\nmqq9e/dq1apV1c47e/asbr31Vk2aNEkjRozw+B6EsqbrQq9LbTZzrxQUZPh9DvxhvrXq/Ff5X7v3\nr2PcuFItX97Mr14/yVBwsGS3+/YelbwFo4gIh0aOLPcY5ir5t/7gT4Hc8y9FhtLSzurbb90P31YN\nbpVD2P59dp55mnsYHGy4Dc/t2zs0dGi518+kamA/dw6hq8/dv6V0vPO07Mz5vlf79g6lppbUKpw1\n6lC2efNmvfHGG0pLS9O+ffuUkpKi9PR05/NPPfWULrvsMq1du7ZGKHvppZf05Zdf6u677yaUwa3G\nVJdze9R693Y9wfrSSx2aPbtECxaENJkN3dG4WK2GDEOyWORXb6/Vashul4KDK4axKzRMcK7aIxWY\n3UJ8D9S1URlOfWl7+/YOFRWpTtd39LQriPTT/4c7d1ZOLajde/hys0HV5+LiLJo8uX5uKHAnYKFs\n4cKFuuSSSzRq1ChJ0pAhQ7Ry5UqFhYVJkgoLC3Xy5ElNmTKlWijbt2+fXnzxRXXp0kXt27cnlMGt\nxl4XT8OqgfthAMCbyjt5PQ3dmllEhENFRZY67d3znfvAeW4vl79D6ue+T2ysQ23bGvrss5rzzgJ5\nt7M3nkJZwGbI5eXlKT4+3vk4MjJSubm5zlAWFhamkydP1jjvueee0+zZs7V69Wqf3icioqWs1sD/\no/D0IaLhNOa6jB9f8adCsKQW1Z5r3Vp65hlp507pkkuk//yn5jXatJGOH695vEMH6Ve/ktascX1e\nXXLXBuBClZ0dpNjY8Av2+7phdzVxH7Iqb7SZMKFu3mfXrmDt2uX62QkTWqi5mxunlyxpUeX/3vpV\nb7ct+NIht3r1avXs2VOXXXaZz9fNzw/8diiNvUfmQtXU6zJgQMWfSu561jz1uM2e7fvQaW3Nm1fs\ndn5OWlqxh6FYf+Z1+bclFHC+Gj6Q8b19vs66Wc1o505Dubmub0SoCw3SU2az2ZSXl+d8fOzYMUVF\nRXk8Z8OGDTp48KA2bNignJwchYSEqG3bturdu3egmgk0GomJ5S673N0d9/T8NdfYnUEtOrpi4dmc\nHIuiow0VF0snTri+C7CiV6ziF7BLLzU0e3aJ8/pVr3luOHQ9hOB5snVVoaGu1yeLi6t4nwcfbM7d\nqmhk+H4OlNrs6lFXAhbK+vTpo0WLFikpKUlZWVmy2WzOoUt3FixY4Pz7okWL1L59ewIZ0AC8BTnJ\ndc/c+PEt3P6G6Sk0SsUuA1vVMOfpLtXk5DKX4e2n4OfbpGZfREY6/htKq+/yUBEK/ftBWXEnXcXf\nW7UydOaMb8NK7ds7dNFFhnbvDlJQUOXEd35IA3WhcumihhDQJTFeeOEFbdmyRRaLRXPmzNHOnTsV\nHh6uQYMGacqUKcrJydHevXvVtWtXjR49WsOHD3eeWxnKmOgPd6iL+QS6JhkZ1v8uZ1ARQKr2xnlb\nb676uTUDTEXYqhmKIiMdOnXK4nUNO3c3X7Rq5VBERMUaY82aVWzR1aWL62tVXSbFU9iq7TpZ1Z3/\n8FfFmmvVjpzX9fz3U6/skCHlev99q8v5Uu7WLat+Hddtv9CXn6n7bdIat6AgQzk5gRu6lBpwnbL6\nQChruqiL+VwoNanN/LvzuW4g2urudbt3B9UIf61bt9BTT9lrXMO/BUErlmTJyXEfUP29nmQ4A5Wn\nYWp3a3O5ukvO3YLNnkKru/1SK9/jfBd7dhf4g4IMNWumgN0FWRkw/K+La5GRDvXo4XB5N6NU8f3h\n6pcPSR5/Iao9Q1arFB1t1Nliz57WUKsrhLLzdKH8oGlqqIv5UBNzclcXdz1s48aVat06q8seSU88\nXc/TnMLKc6v2gp77vnURmGfODHXZYzZuXKnLUDhuXKnmzSvxqSey6k4RvgT+2bODNWDA6YAub1N1\nkVhP7+EqTPXubdfXXwe7XZLHXY+1N67qXNugdu4ivnUR/Opr43N3CGU+4AeNOVEX86Em5uSpLnXd\nuxfIbcvqQm17SX/6oV83WxdVrYmrO6DPDUSS3IZWT3c3uwtSVc9vyPr4EuTdzSd195n7u9NDpQ4d\npC1bGumK/vWFUNZ0URfzoSbmRF3qzvn0ElVV1zUxexj2xJe2+/v1uRvGXrgwRLt2BbmcZ/fuu9KA\nAYSy80Ioa7qoi/lQE3OiLuZDTRqW+7vHGzaU1dvisQAAAGbgy7I/DaEh91oAAADAfxHKAAAATIBQ\nBgAAYAKEMgAAABMglAEAAJgAoQwAAMAECGUAAAAmQCgDAAAwAUIZAACACRDKAAAATIBQBgAAYAKE\nMgAAABMglAEAAJgAoQwAAMAECGUAAAAmQCgDAAAwAUIZAACACRDKAAAATIBQBgAAYAKEMgAAABMg\nlAEAAJgAoQwAAMAECGUAAAAmQCgDAAAwAUIZAACACRDKAAAATIBQBgAAYAKEMgAAABMIaCibN2+e\n7rzzTiUlJWnHjh3VnispKdGMGTM0YsQI57Hi4mJNnTpVY8eO1ahRo/TZZ58FsnkAAACmEbBQtnnz\nZh04cEDp6emaO3eu5s6dW+35+fPnKzY2ttqxzz77TF27dtXbb7+tBQsW6Nlnnw1U8wAAAEzFGqgL\nZ2ZmauDAgZKkTp06qaCgQIWFhQoLC5MkTZ8+XSdPntTatWud59xyyy3Ovx85ckTR0dGBah4AAICp\nBCyU5eXlKT4+3vk4MjJSubm5zlAWFhamkydPujw3KSlJOTk5evXVV72+T0RES1mtwXXTaA+iosID\n/h7wH3UxH2piTtTFfKiJ+TR0TQIWys5lGIbPr12xYoV27dqlRx99VGvXrpXFYnH72vz8orponkdR\nUeHKzT0d8PeBf6iL+VATc6Iu5kNNzKe+auIp+AVsTpnNZlNeXp7z8bFjxxQVFeXxnO+//15HjhyR\nJMXGxsput+vEiROBaiIAAIBpBCyU9enTR+vXr5ckZWVlyWazOYcu3dmyZYuWLVsmqWL4s6ioSBER\nEYFqIgAAgGkEbPiyV69eio+PV1JSkiwWi+bMmaNVq1YpPDxcgwYN0pQpU5STk6P9+/crOTlZo0eP\nVlJSkv7whz/orrvu0tmzZ5WamqqgIJZSAwAAjZ/F8GeylwnV1/gvY//mQ13Mh5qYE3UxH2piPo16\nThkAAAB8RygDAAAwAUIZAACACRDKAAAATIBQBgAAYAIX/N2XAAAAjQE9ZQAAACZAKAMAADABQhkA\nAIAJEMoAAABMgFAGAABgAoQyAAAAE7A2dAPMbt68edq+fbssFotSUlLUvXv3hm5Sk7Jnzx5NmjRJ\nv/nNbzR27FgdOXJEjz32mOx2u6KiovT8888rJCREa9eu1VtvvaWgoCCNHj1ao0aNauimN1rz58/X\nP/7xD5WXl2vChAnq1q0bNWlAxcXFmjlzpo4fP66SkhJNmjRJXbp0oSYmcfbsWd16662aNGmSrr/+\neurSgDZt2qSpU6fqiiuukCTFxMRo3Lhx5qqJAbc2bdpkjB8/3jAMw/jxxx+N0aNHN3CLmpYzZ84Y\nY8eONWbNmmUsX77cMAzDmDlzpvHhhx8ahmEYf/zjH4133nnHOHPmjHHzzTcbp06dMoqLi41hw4YZ\n+fn5Ddn0RiszM9MYN26cYRiGceLECSMhIYGaNLAPPvjAeO211wzDMIxDhw4ZN998MzUxkRdffNEY\nMWKE8f7771OXBvbNN98YDz30ULVjZqsJw5ceZGZmauDAgZKkTp06qaCgQIWFhQ3cqqYjJCREr7/+\numw2m/PYpk2bNGDAAEnSTTfdpMzMTG3fvl3dunVTeHi4mjdvrl69emnr1q0N1exG7ZprrtHChQsl\nSa1bt1ZxcTE1aWC33HKL7r//fknSkSNHFB0dTU1MYt++ffrxxx914403SuL/LzMyW00IZR7k5eUp\nIiLC+TgyMlK5ubkN2KKmxWq1qnnz5tWOFRcXKyQkRJLUpk0b5ebmKi8vT5GRkc7XUKfACQ4OVsuW\nLSVJK1eu1A033EBNTCIpKUmPPPKIUlJSqIlJPPfcc5o5c6bzMXVpeD/++KMmTpyoMWPG6KuvvjJd\nTZhT5geDHalMxV09qFPgffzxx1q5cqWWLVumm2++2XmcmjScFStWaNeuXXr00Uerfd7UpGGsXr1a\nPXv21GWXXebyeepS/37+859r8uTJGjp0qA4ePKh77rlHdrvd+bwZakIo88BmsykvL8/5+NixY4qK\nimrAFqFly5Y6e/asmjdvrqNHj8pms7msU8+ePRuwlY3bF198oVdffVVLly5VeHg4NWlg33//vdq0\naaN27dopNjZWdrtdrVq1oiYNbMOGDTp48KA2bNignJwchYSE8G+lgUVHR+uWW26RJHXo0EEXX3yx\nvvvuO1PVhOFLD/r06aP169dLkrKysmSz2RQWFtbArWraevfu7azJRx99pH79+qlHjx767rvvdOrU\nKZ05c0Zbt27V1Vdf3cAtbZxOnz6t+fPnKy0tTT/72c8kUZOGtmXLFi1btkxSxZSLoqIiamICCxYs\n0Pvvv6/33ntPo0aN0qRJk6hLA1u7dq3eeOMNSVJubq6OHz+uESNGmKomFoO+Uo9eeOEFbdmyRRaL\nRXPmzFGXLl0auklNxvfff6/nnntO2dnZslqtio6O1gsvvKCZM2eqpKREl1xyiZ555hk1a9ZM/+//\n/T+98cYbslgsGjt2rG677baGbn6jlJ6erkWLFqljx47OY88++6xmzZpFTRrI2bNn9Yc//EFHjhzR\n2bNnNXnyZHXt2lUzZsygJiaxaNEitW/fXn379qUuDaiwsFCPPPKITp06pbKyMk2ePFmxsbGmqgmh\nDAAAwAQYvgQAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAAEyCUAWgwnTt3Vnl5uSRpzZo1dXbd\nv/71r3I4HJKk5OTkaqt2m0lycrK+/vrrhm4GAJMglAFocHa7Xf/7v/9bZ9dbtGiRM5QtX75cwcHB\ndXZtAAgUtlkC0OBSUlKUnZ2t++67T8uWLdOHH36ot99+W4ZhKDIyUk8//bQiIiLUq1cv3XHHHXI4\nHEpJSdGcOXP0r3/9S6WlperRo4dmzZqll19+WQcOHNBvfvMbLV68WL/85S+VlZWl0tJSzZ49Wzk5\nOSovL9evfvUr3XXXXVq1apW+/vprORwO7d+/X+3bt9eiRYtksVic7du0aZNee+01tW3bVj/++KOs\nVquWLl2q48eP66677tLnn38uqSIMlpeXa/r06bryyiv1wAMP6NNPP1VZWZkmTpyo9957T/v379fj\njz+uvn37SpI+/fRTLV26VEePHtWkSZM0bNgwFRQUaM6cOTpx4oQKCwt17733avjw4Vq0aJEOHTqk\nw4cPa8aMGeratWuD1AtAYBDKADS4hx56SJmZmVq2bJmOHDmiV199VStXrlRISIjeeustpaWlaebM\nmSoqKlJCQoL69Omj/Px8de7cWU899ZQkaciQIdqzZ4+mTJmiJUuW6M0335TV+tN/ccuXL1fr1q31\nxz/+UWfPntUtt9yifv36SZK2bdumDz74QKGhoRo0aJB27dqluLi4am385z//qY8++kht2rRRcnKy\nvvzyS8XGxrr9moqKil7bv7IAAALXSURBVNS1a1eNHz9eycnJ+vTTT/X6669r1apV+vOf/+wMZXa7\nXcuWLdOBAwc0ZswYDR06VAsWLFC/fv00cuRIFRUV6Ve/+pX69OkjSTp06JDefvvtaqERQONAKANg\nKtu2bVNubq5++9vfSpJKS0t16aWXSpIMw1CvXr0kSa1bt9aRI0d05513KiQkRLm5ucrPz3d73e3b\nt2vEiBGSpObNm6tr167KysqSJHXv3l3NmzeXJLVr104FBQU1zu/UqZPatGkjSWrfvr1Onjzp9Wu5\n6qqrJFVshFzZ7rZt2+r06dPO11SGrcsvv1ySdOLECW3atEnfffedVq9eLUmyWq06dOiQJKlHjx4E\nMqCRIpQBMJWQkBB1795daWlpLp9v1qyZJOmDDz7Qd999p3feeUdWq9UZuNw5N8gYhuE8du6cM1e7\nz7mal3buNcvKyqodq3qOu3ltVV9f2aaQkBDNmTNH3bp1q/bajRs3Or9+AI0PE/0BNLigoCDnXZjd\nunXTjh07lJubK0lat26dPv744xrnHD9+XB07dpTVatX333+v//znPyotLZVUEXQqr1epR48e+uKL\nLyRVDC1mZWUpPj7+vNodFhamgoICFRcXy26369tvv/X7GpmZmZKk/fv3Kzg4WJGRkbrqqqu0bt06\nSRUbjj/++OM1vh4AjQ+hDECDs9lsuvjiizVixAiFh4frD3/4gyZMmKC7775bK1euVM+ePWucM2TI\nEP3zn//U2LFj9dFHH+m+++7T008/rYKCAud8rP/85z/O1ycnJ+vMmTO6++679etf/1qTJk1yDovW\n1kUXXaTExESNHDlSDz74YI15aL6wWq164IEHNHnyZM2aNUsWi0WTJ092zjG7++67FRcXV21+HIDG\nyWK46qcHAABAvaKnDAAAwAQIZQAAACZAKAMAADABQhkAAIAJEMoAAABMgFAGAABgAoQyAAAAEyCU\nAQAAmMD/B5pwU3hwdd51AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f6ROChOed9pC",
        "colab_type": "code",
        "outputId": "58d54b69-c939-4b38-b02d-1229c9a71bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = NN.predict(X_test)\n",
        "y_pred = (y_pred >= 0.5)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1590    5]\n",
            " [ 359   46]]\n",
            "\n",
            "Accuracy Score: 0.818\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      1.00      0.90      1595\n",
            "           1       0.90      0.11      0.20       405\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      2000\n",
            "   macro avg       0.86      0.56      0.55      2000\n",
            "weighted avg       0.83      0.82      0.76      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}